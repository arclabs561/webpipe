#![recursion_limit = "256"]

use anyhow::Result;
#[cfg(feature = "vlm")]
use base64::Engine;
use clap::{Parser, Subcommand};
#[cfg(feature = "eval")]
mod eval;

#[cfg(feature = "eval")]
fn best_effort_git_sha() -> Option<String> {
    let out = std::process::Command::new("git")
        .args(["rev-parse", "HEAD"])
        .output()
        .ok()?;
    if !out.status.success() {
        return None;
    }
    let s = String::from_utf8_lossy(&out.stdout).trim().to_string();
    if s.is_empty() {
        None
    } else {
        Some(s)
    }
}

#[cfg(feature = "eval")]
fn blake3_hex_bytes(bytes: &[u8]) -> String {
    blake3::hash(bytes).to_hex().to_string()
}

// Controlled vocabulary for deterministic "critic loop" findings.
// Keep this small, stable, and aggregatable (no free-form drift).
#[cfg(feature = "eval")]
const CRITIC_ISSUE_VOCAB: &[&str] = &[
    "missing_structured_content",
    "missing_or_bad_kind",
    "unexpected_kind",
    "tool_failed",
    "missing_urls",
    "empty_top_chunks",
    "low_signal",
    "warnings_present",
    "missing_markdown",
    "markdown_is_json",
    "markdown_missing_request_section",
    "markdown_missing_summary_section",
];

#[derive(Parser, Debug)]
#[command(name = "webpipe")]
#[command(about = "Local fetch/search plumbing (MCP stdio server)", long_about = None)]
struct Cli {
    #[command(subcommand)]
    command: Commands,
}

#[derive(Subcommand, Debug)]
#[allow(clippy::large_enum_variant)]
enum Commands {
    /// Run as an MCP stdio server (for Cursor / MCP clients).
    #[cfg(feature = "stdio")]
    McpStdio,
    /// Run a local search evaluation harness (writes a JSON artifact).
    #[cfg(feature = "eval")]
    EvalSearch(EvalSearchCmd),
    /// Run a local fetch evaluation harness (writes a JSON artifact).
    #[cfg(feature = "eval")]
    EvalFetch(EvalFetchCmd),
    /// Run a local search->fetch->extract evaluation harness (writes a JSON artifact).
    #[cfg(all(feature = "eval", feature = "stdio"))]
    EvalSearchExtract(EvalSearchExtractCmd),
    /// Run a small E2E matrix over a versioned query set (writes a JSONL artifact).
    #[cfg(all(feature = "eval", feature = "stdio"))]
    EvalMatrix(EvalMatrixCmd),
    /// Score an eval-matrix JSONL artifact against an E2E qrels file (json).
    #[cfg(feature = "eval")]
    EvalMatrixScore(EvalMatrixScoreCmd),
    /// Export eval-matrix JSONL rows into a judge-ready JSONL dataset (optionally joined with qrels).
    #[cfg(feature = "eval")]
    EvalMatrixExport(EvalMatrixExportCmd),
    /// Deterministic baseline judge over exported eval-matrix examples (json).
    #[cfg(feature = "eval")]
    EvalMatrixJudge(EvalMatrixJudgeCmd),
    /// LLM-based semantic judge over exported eval-matrix examples (json).
    ///
    /// This is opt-in and requires a configured local LLM backend:
    /// - llm_backend=openai_compat: set WEBPIPE_OPENAI_COMPAT_BASE_URL and choose a model
    /// - llm_backend=ollama: set WEBPIPE_OLLAMA_ENABLE=true (and optionally WEBPIPE_OLLAMA_* vars)
    ///
    /// The judge consumes the exported `judge_text` field (already bounded), and returns
    /// a strict JSON scorecard per example plus totals.
    #[cfg(feature = "eval")]
    EvalMatrixLlmJudge(EvalMatrixLlmJudgeCmd),
    /// Run eval-matrix -> score -> export -> judge (one-shot).
    ///
    /// This is a convenience wrapper that invokes the existing subcommands with explicit
    /// artifact paths, so you can run a full end-to-end evaluation with one call.
    #[cfg(feature = "eval")]
    EvalMatrixRun(EvalMatrixRunCmd),
    /// Multi-judge, multi-trial agentic evaluation harness (json).
    ///
    /// Runs several tool trials per query, asks multiple LLM judges (domain-seeded) to score and
    /// take notes over the trials, then runs an optional meta-judge to aggregate reports.
    #[cfg(all(feature = "eval", feature = "stdio"))]
    EvalJudgeSwarm(EvalJudgeSwarmCmd),
    /// Offline-first adversarial “critic loop” over MCP tools (json + transcript).
    ///
    /// This does NOT try to be “a better general agent than Cursor”.
    /// It is a bounded harness that uses our own tools, then emits critique notes about
    /// tool-result quality and failure modes in a stable, aggregatable schema.
    #[cfg(all(feature = "eval", feature = "stdio"))]
    EvalCriticLoop(EvalCriticLoopCmd),
    /// Offline-first adversarial critic over an E2E query pack (json + transcript).
    ///
    /// This is the “what we test and why” harness:
    /// - validates MCP tool output shape (Markdown-first + structured_content)
    /// - checks tool-result quality signals (urls/top_chunks/low_signal/warnings)
    /// - aggregates controlled-vocab issues across a dataset so we can set priorities
    #[cfg(all(feature = "eval", feature = "stdio"))]
    EvalCriticRun(EvalCriticRunCmd),
    /// Generate a versioned E2E query pack for domain-seeded live runs (json).
    ///
    /// Emits the same schema as `fixtures/e2e_queries_v1.json`, but with "live" tags and optional
    /// seed URLs per domain (arxiv/news/code) to help warm caches.
    #[cfg(feature = "eval")]
    EvalGenerateDomainPack(EvalGenerateDomainPackCmd),
    /// Diagnose configuration/launch issues (json; no secrets).
    Doctor(DoctorCmd),
    /// List tools exposed by the MCP stdio server (for auditing what Cursor sees).
    #[cfg(feature = "stdio")]
    McpListTools(McpListToolsCmd),
    /// Call an MCP tool via a spawned `mcp-stdio` child and print its Markdown.
    ///
    /// This is a local debugging bridge for “what does Cursor see?” without needing the
    /// Cursor UI. It spawns `webpipe mcp-stdio`, calls a tool, and prints the returned
    /// Markdown content to stdout.
    #[cfg(feature = "stdio")]
    McpCall(McpCallCmd),
    /// Run a small, tool-agnostic MCP smoke sweep (single child server).
    ///
    /// This is a debugging helper for validating that all MCP-visible tools work end-to-end
    /// in one process, so `webpipe_usage` reflects the full run.
    #[cfg(feature = "stdio")]
    McpSweep(McpSweepCmd),
    /// Summarize a transcript JSONL into a compact JSON report.
    ///
    /// This is the “index” for later audits: counts by stage/run_kind, parse-fail rates,
    /// and the slowest calls, without re-running anything.
    #[cfg(feature = "eval")]
    EvalTranscriptSummarize(EvalTranscriptSummarizeCmd),
    /// Summarize multiple VLM critique artifacts into a compact meta report (json).
    ///
    /// Inputs are JSON artifacts produced by `webpipe vlm-openrouter`.
    #[cfg(all(feature = "eval", feature = "vlm"))]
    EvalVlmSummarize(EvalVlmSummarizeCmd),
    /// Run a VLM over multiple images (and optionally summarize) (json).
    ///
    /// This is the “one-button” E2E loop for rendered-page critique:
    /// - takes `--image ...` (repeatable) and/or `--images-dir ...`
    /// - runs `vlm-openrouter` over each image
    /// - writes per-image artifacts under `--out-dir`
    /// - optionally appends transcript events
    /// - writes a meta summary JSON like `eval-vlm-summarize`
    #[cfg(all(feature = "eval", feature = "vlm"))]
    EvalVlmRun(EvalVlmRunCmd),
    /// Bundle page + plots VLM summaries into one artifact (json).
    ///
    /// This is a convenience “one file to hand to a meta-judge” wrapper over:
    /// - `eval-vlm-run` (full-page screenshots)
    /// - `eval-vlm-run` (plot images)
    #[cfg(all(feature = "eval", feature = "vlm"))]
    EvalVlmBundle(EvalVlmBundleCmd),
    /// Print version info.
    Version(VersionCmd),
    /// Run a VLM (Gemini vision) over a local image file (e.g. a browser screenshot).
    ///
    /// This is a tiny, explicit bridge for E2E: render -> screenshot -> VLM.
    #[cfg(feature = "vlm")]
    VlmImageToText(VlmImageToTextCmd),
    /// Run an OpenRouter multimodal chat completion over an image.
    ///
    /// This is a pragmatic “VLM critique” path that uses OpenRouter’s OpenAI-compatible
    /// `/v1/chat/completions` endpoint with an image-capable model.
    ///
    /// Env:
    /// - `WEBPIPE_OPENROUTER_API_KEY` or `OPENROUTER_API_KEY`
    /// - `WEBPIPE_OPENROUTER_MODEL` (optional) to avoid passing `--model`
    /// - `WEBPIPE_OPENROUTER_BASE_URL` (optional; default: https://openrouter.ai/api)
    #[cfg(feature = "vlm")]
    VlmOpenrouter(VlmOpenrouterCmd),
    /// Score an eval_search_extract artifact against a qrels file (json).
    #[cfg(feature = "eval")]
    EvalQrels(EvalQrelsCmd),
}

#[cfg(feature = "eval")]
#[derive(clap::Args, Debug)]
struct EvalQrelsCmd {
    /// Eval artifact path produced by `eval-search-extract`.
    #[arg(long)]
    eval_artifact: std::path::PathBuf,
    /// Qrels file (json), see `crates/webpipe-mcp/fixtures/qrels_seed.json`.
    #[arg(long)]
    qrels: std::path::PathBuf,
    /// Output JSON path (default: .generated/webpipe-eval-qrels-<epoch>.json)
    #[arg(long)]
    out: Option<std::path::PathBuf>,
    /// Override "now" for deterministic outputs.
    #[arg(long)]
    now_epoch_s: Option<u64>,
}

#[cfg(feature = "eval")]
#[derive(clap::Args, Debug)]
struct EvalSearchExtractCmd {
    /// Provider to use. Allowed: auto, brave, tavily, searxng
    #[arg(long, default_value = "auto")]
    provider: String,
    /// When provider="auto", choose routing mode. Allowed: fallback, merge, mab
    #[arg(long, default_value = "fallback")]
    auto_mode: String,
    /// How to select `top_chunks` across URLs. Allowed: score, pareto
    #[arg(long, default_value = "score")]
    selection_mode: String,
    /// Optional: run multiple selection modes and include a summary comparison.
    ///
    /// Example: "score,pareto"
    #[arg(long)]
    compare_selection_modes: Option<String>,
    /// Which fetch backend to use. Allowed: local, firecrawl
    #[arg(long, default_value = "local")]
    fetch_backend: String,
    /// If true, and fetch_backend="local", retry a single URL with Firecrawl when local extraction yields empty text.
    #[arg(long, action = clap::ArgAction::Set, default_value_t = false)]
    firecrawl_fallback_on_empty_extraction: bool,

    /// Query (repeatable). If you pass urls, query is optional.
    #[arg(long)]
    query: Vec<String>,
    /// File containing queries (one per line; blank lines and #comments ignored).
    #[arg(long)]
    queries_file: Vec<std::path::PathBuf>,
    /// Seed query dataset (json), e.g. `crates/webpipe-mcp/fixtures/queries_seed.json`.
    ///
    /// When provided, query runs will include `query_id` so they can be scored against qrels.
    #[arg(long)]
    queries_json: Vec<std::path::PathBuf>,

    /// URL (repeatable). If provided, skip search and hydrate these URLs directly.
    #[arg(long)]
    url: Vec<String>,
    /// File containing URLs (one per line; blank lines and #comments ignored).
    #[arg(long)]
    urls_file: Vec<std::path::PathBuf>,
    /// When urls=[...] is provided, how to pick URLs under max_urls. Allowed: auto, preserve, query_rank
    #[arg(long, default_value = "auto")]
    url_selection_mode: String,

    /// Enable agentic URL discovery loop (bounded by max_urls).
    ///
    /// This allows the evaluator to follow links and replace weak initial picks,
    /// rather than assuming the seed list ordering or URL-string heuristics are sufficient.
    #[arg(long, action = clap::ArgAction::Set, default_value_t = false)]
    agentic: bool,

    /// Agentic selector strategy. Allowed: auto, lexical, llm
    ///
    /// - auto: use OpenRouter if configured, else lexical
    /// - lexical: deterministic URL/link scoring only
    /// - llm: force OpenRouter planner (falls back to lexical if not configured)
    #[arg(long, default_value = "auto")]
    agentic_selector: String,

    /// High-level width vs depth preset. Allowed: balanced, wide, deep
    ///
    /// Note: this only fills defaults for optional knobs like --max-results/--max-urls when
    /// those flags are omitted.
    #[arg(long, default_value = "balanced")]
    exploration: String,

    /// When agentic=true, allow up to this many search rounds total.
    #[arg(long)]
    agentic_max_search_rounds: Option<usize>,

    /// When agentic=true, maximum frontier size.
    #[arg(long)]
    agentic_frontier_max: Option<usize>,

    /// Max planner (LLM) calls for a single request.
    #[arg(long)]
    planner_max_calls: Option<usize>,

    #[arg(long)]
    max_results: Option<usize>,
    #[arg(long)]
    max_urls: Option<usize>,
    #[arg(long, default_value_t = 20_000)]
    timeout_ms: u64,
    #[arg(long, default_value_t = 5_000_000)]
    max_bytes: u64,
    /// If true, and fetch_backend="local", retry a URL with a higher max_bytes when the body was truncated.
    #[arg(long, action = clap::ArgAction::Set, default_value_t = false)]
    retry_on_truncation: bool,
    /// When retry_on_truncation=true, max_bytes to use for the retry.
    #[arg(long)]
    truncation_retry_max_bytes: Option<u64>,
    /// Width used for HTML->text extraction (default: 100).
    #[arg(long, default_value_t = 100)]
    width: usize,
    /// Max chars of extracted text per URL.
    #[arg(long)]
    max_chars: Option<usize>,
    /// Max chunks per URL (also used as the across-URL `top_chunks` cap).
    #[arg(long)]
    top_chunks: Option<usize>,
    /// Max chars per chunk.
    #[arg(long)]
    max_chunk_chars: Option<usize>,
    /// Include extracted links.
    #[arg(long, action = clap::ArgAction::Set, default_value_t = true)]
    include_links: bool,
    #[arg(long, default_value_t = 25)]
    max_links: usize,
    /// Include full extracted text in per-URL results.
    #[arg(long, action = clap::ArgAction::Set, default_value_t = false)]
    include_text: bool,

    /// Output JSON path (default: .generated/webpipe-eval-search-extract-<epoch>.json)
    #[arg(long)]
    out: Option<std::path::PathBuf>,
    /// Override "now" for deterministic outputs.
    #[arg(long)]
    now_epoch_s: Option<u64>,
}

#[cfg(feature = "eval")]
#[derive(clap::Args, Debug)]
struct EvalMatrixCmd {
    /// E2E query dataset (json), e.g. `crates/webpipe-mcp/fixtures/e2e_queries_v1.json`.
    #[arg(long)]
    queries_json: std::path::PathBuf,
    /// Base URL used to expand `url_paths` entries in the dataset.
    ///
    /// Example: http://127.0.0.1:8080
    #[arg(long)]
    base_url: String,
    /// Provider to use for the "search" leg. Allowed: auto, brave, tavily, searxng
    #[arg(long, default_value = "searxng")]
    provider: String,
    /// When provider="auto", choose routing mode. Allowed: fallback, merge, mab
    #[arg(long, default_value = "fallback")]
    auto_mode: String,
    /// How to select `top_chunks` across URLs. Allowed: score, pareto
    #[arg(long, default_value = "score")]
    selection_mode: String,
    /// Which fetch backend to use. Allowed: local, firecrawl
    #[arg(long, default_value = "local")]
    fetch_backend: String,
    /// Max search results to request (default: 1; max: 20).
    #[arg(long, default_value_t = 1)]
    max_results: usize,
    /// Max URLs to process (default: 1; max: 10).
    #[arg(long, default_value_t = 1)]
    max_urls: usize,
    /// Fetch timeout per URL (ms).
    #[arg(long, default_value_t = 20_000)]
    timeout_ms: u64,
    /// Max bytes per URL.
    #[arg(long, default_value_t = 2_000_000)]
    max_bytes: u64,
    /// Across-URL top chunks cap.
    #[arg(long, default_value_t = 2)]
    top_chunks: usize,
    /// Max chars per chunk.
    #[arg(long, default_value_t = 200)]
    max_chunk_chars: usize,
    /// Output JSONL path (default: .generated/webpipe-eval-matrix-<epoch>.jsonl)
    #[arg(long)]
    out: Option<std::path::PathBuf>,
    /// Override "now" for deterministic outputs.
    #[arg(long)]
    now_epoch_s: Option<u64>,
}

#[cfg(feature = "eval")]
#[derive(clap::Args, Debug)]
struct EvalMatrixScoreCmd {
    /// Matrix artifact path produced by `eval-matrix` (jsonl).
    #[arg(long)]
    matrix_artifact: std::path::PathBuf,
    /// E2E qrels file (json), see `crates/webpipe-mcp/fixtures/e2e_qrels_v1.json`.
    #[arg(long)]
    qrels: std::path::PathBuf,
    /// Output JSON path (default: .generated/webpipe-eval-matrix-score-<epoch>.json)
    #[arg(long)]
    out: Option<std::path::PathBuf>,
    /// Override "now" for deterministic outputs.
    #[arg(long)]
    now_epoch_s: Option<u64>,
}

#[cfg(feature = "eval")]
#[derive(clap::Args, Debug)]
struct EvalMatrixExportCmd {
    /// Matrix artifact path produced by `eval-matrix` (jsonl).
    #[arg(long)]
    matrix_artifact: std::path::PathBuf,
    /// Optional E2E qrels file (json) to attach expected URL needles and a hit label.
    #[arg(long)]
    qrels: Option<std::path::PathBuf>,
    /// Max characters per exported text field (keeps artifacts bounded).
    #[arg(long, default_value_t = 4000)]
    max_text_chars: usize,
    /// Output JSONL path (default: .generated/webpipe-eval-matrix-export-<epoch>.jsonl)
    #[arg(long)]
    out: Option<std::path::PathBuf>,
    /// Override "now" for deterministic outputs.
    #[arg(long)]
    now_epoch_s: Option<u64>,
}

#[cfg(feature = "eval")]
#[derive(clap::Args, Debug)]
struct EvalMatrixJudgeCmd {
    /// Exported examples path produced by `eval-matrix-export` (jsonl).
    #[arg(long)]
    examples_artifact: std::path::PathBuf,
    /// Output JSON path (default: .generated/webpipe-eval-matrix-judge-<epoch>.json)
    #[arg(long)]
    out: Option<std::path::PathBuf>,
    /// Override "now" for deterministic outputs.
    #[arg(long)]
    now_epoch_s: Option<u64>,
}

#[cfg(feature = "eval")]
#[derive(clap::Args, Debug)]
struct EvalMatrixLlmJudgeCmd {
    /// Exported examples path produced by `eval-matrix-export` (jsonl).
    #[arg(long)]
    examples_artifact: std::path::PathBuf,
    /// LLM backend to use. Allowed: openai, openai_compat, openrouter, groq, ollama
    #[arg(long, default_value = "openai_compat")]
    llm_backend: String,
    /// Optional model override (used by llm_backend="openai_compat"/"openrouter"/"groq").
    #[arg(long)]
    llm_model: Option<String>,
    /// Prompt preset to use (default: v2).
    #[arg(long, default_value = "v2")]
    prompt_preset: String,
    /// Optional file containing a custom system prompt (overrides prompt_preset).
    #[arg(long)]
    system_prompt_file: Option<std::path::PathBuf>,
    /// Timeout per example (ms).
    #[arg(long, default_value_t = 12_000)]
    timeout_ms: u64,
    /// Max examples to judge (bounded).
    #[arg(long, default_value_t = 50)]
    max_examples: usize,
    /// Max characters from judge_text to include in the prompt (safety bound).
    #[arg(long, default_value_t = 4000)]
    max_prompt_chars: usize,
    /// Max tokens for the judge response (openai_compat only).
    #[arg(long)]
    max_tokens: Option<u64>,
    /// Temperature for the judge response (openai_compat only; default 0.0).
    #[arg(long)]
    temperature: Option<f64>,
    /// top_p for the judge response (openai_compat only; default 1.0).
    #[arg(long)]
    top_p: Option<f64>,
    /// If true, try to enforce JSON output using OpenAI-compatible response_format (best-effort).
    #[arg(long, default_value_t = true)]
    json_mode: bool,
    /// Retry once if the LLM output is not parseable JSON (best-effort).
    #[arg(long, default_value_t = true)]
    retry_on_parse_fail: bool,

    /// If true, write a JSONL transcript for LLM judge calls (auditing).
    ///
    /// This does NOT change the judge schema; it only records prompts/responses.
    #[arg(long, action = clap::ArgAction::Set, default_value_t = true)]
    transcript: bool,
    /// Transcript output path (JSONL). If omitted, defaults to `<out>.transcript.jsonl`.
    #[arg(long)]
    transcript_jsonl: Option<std::path::PathBuf>,
    /// Max characters stored per transcript string field (bounded, Unicode-safe).
    #[arg(long, default_value_t = 4000)]
    transcript_max_chars: usize,

    /// Output JSON path (default: .generated/webpipe-eval-matrix-llm-judge-<epoch>.json)
    #[arg(long)]
    out: Option<std::path::PathBuf>,
    /// Override "now" for deterministic outputs.
    #[arg(long)]
    now_epoch_s: Option<u64>,
}

#[cfg(feature = "eval")]
#[derive(clap::Args, Debug)]
struct EvalMatrixRunCmd {
    /// E2E query dataset (json), e.g. `crates/webpipe-mcp/fixtures/e2e_queries_v1.json`.
    #[arg(long)]
    queries_json: std::path::PathBuf,
    /// E2E qrels file (json), e.g. `crates/webpipe-mcp/fixtures/e2e_qrels_v1.json`.
    #[arg(long)]
    qrels: std::path::PathBuf,
    /// Base URL used to expand `url_paths` entries in the dataset.
    #[arg(long)]
    base_url: String,
    /// Provider to use for the "search" leg. Allowed: auto, brave, tavily, searxng
    #[arg(long, default_value = "searxng")]
    provider: String,
    /// When provider="auto", choose routing mode. Allowed: fallback, merge, mab
    #[arg(long, default_value = "fallback")]
    auto_mode: String,
    /// How to select `top_chunks` across URLs. Allowed: score, pareto
    #[arg(long, default_value = "score")]
    selection_mode: String,
    /// Which fetch backend to use. Allowed: local, firecrawl
    #[arg(long, default_value = "local")]
    fetch_backend: String,
    /// Output directory for all artifacts (default: .generated).
    #[arg(long)]
    out_dir: Option<std::path::PathBuf>,
    /// Max characters per exported judge_text (keeps artifacts bounded).
    #[arg(long, default_value_t = 4000)]
    max_text_chars: usize,
    /// Override "now" for deterministic outputs.
    #[arg(long)]
    now_epoch_s: Option<u64>,
    /// Optional git SHA to include in the run manifest (best-effort if omitted).
    #[arg(long)]
    git_sha: Option<String>,
    /// Output format: json|text
    #[arg(long = "output", alias = "format", default_value = "text")]
    output: String,
}

#[cfg(feature = "eval")]
#[derive(clap::Args, Debug)]
struct EvalJudgeSwarmCmd {
    /// E2E query dataset (json), e.g. `crates/webpipe-mcp/fixtures/e2e_queries_v1.json`.
    #[arg(long)]
    queries_json: std::path::PathBuf,
    /// Optional E2E qrels file (json), e.g. `crates/webpipe-mcp/fixtures/e2e_qrels_v1.json`.
    #[arg(long)]
    qrels: Option<std::path::PathBuf>,
    /// Base URL used to expand `url_paths` entries in the dataset.
    #[arg(long)]
    base_url: String,

    /// Provider to use for the "search" leg. Allowed: auto, brave, tavily, searxng
    #[arg(long, default_value = "searxng")]
    provider: String,
    /// When provider="auto", choose routing mode. Allowed: fallback, merge, mab
    #[arg(long, default_value = "fallback")]
    auto_mode: String,
    /// How to select `top_chunks` across URLs. Allowed: score, pareto
    #[arg(long, default_value = "score")]
    selection_mode: String,
    /// Which fetch backend to use. Allowed: local, firecrawl
    #[arg(long, default_value = "local")]
    fetch_backend: String,

    /// Max search results to request (default: 2; max: 20).
    #[arg(long, default_value_t = 2)]
    max_results: usize,
    /// Max URLs to process (default: 2; max: 10).
    #[arg(long, default_value_t = 2)]
    max_urls: usize,
    /// Fetch timeout per URL (ms).
    #[arg(long, default_value_t = 20_000)]
    timeout_ms: u64,
    /// Max bytes per URL.
    #[arg(long, default_value_t = 2_000_000)]
    max_bytes: u64,
    /// Across-URL top chunks cap.
    #[arg(long, default_value_t = 3)]
    top_chunks: usize,
    /// Max chars per chunk.
    #[arg(long, default_value_t = 300)]
    max_chunk_chars: usize,

    /// Trial set to run per query. Allowed: basic, wide
    #[arg(long, default_value = "basic")]
    trial_set: String,

    /// Comma-separated judge preset ids to run.
    ///
    /// Default is a “poor man's adversarial panel” (no free-form notes):
    /// - ux: UX/layout failures
    /// - writing: coherence/readability failures
    /// - math: notation fidelity failures
    /// - redteam: skeptical catch-all
    #[arg(long, default_value = "ux,writing,math,redteam")]
    judges: String,
    /// Optional JSON file defining judge presets (ids, domain tags, system prompts).
    ///
    /// Shape:
    /// { "schema_version": 1, "kind": "webpipe_judge_presets", "presets": [{ "id": "...", "domain_tags": ["..."], "system": "..." }] }
    #[arg(long)]
    judge_presets_json: Option<std::path::PathBuf>,
    /// Only include queries that have any of these tags (comma-separated). If none match, falls back to all.
    #[arg(long)]
    domain_tags: Option<String>,
    /// Max queries per judge (bounded).
    #[arg(long, default_value_t = 20)]
    max_queries: usize,
    /// Seed for deterministic query ordering / selection.
    #[arg(long, default_value_t = 0)]
    seed: u64,

    /// LLM backend to use. Allowed: openai, openai_compat, openrouter, groq, ollama
    #[arg(long, default_value = "openai_compat")]
    llm_backend: String,
    /// Optional model override (used by llm_backend="openai_compat"/"openrouter"/"groq").
    #[arg(long)]
    llm_model: Option<String>,
    /// Timeout per LLM call (ms).
    #[arg(long, default_value_t = 12_000)]
    llm_timeout_ms: u64,
    /// Temperature for judge responses (openai_compat/openrouter/groq only; default 0.0).
    #[arg(long)]
    temperature: Option<f64>,
    /// top_p for judge responses (openai_compat/openrouter/groq only; default 1.0).
    #[arg(long)]
    top_p: Option<f64>,
    /// If true, try to enforce JSON output using OpenAI-compatible response_format (best-effort).
    #[arg(long, default_value_t = true)]
    json_mode: bool,
    /// Retry once if the LLM output is not parseable JSON (best-effort).
    #[arg(long, default_value_t = true)]
    retry_on_parse_fail: bool,

    /// If true, run an evidence-only task solver per query (picks best trial, answers or abstains).
    #[arg(long, default_value_t = true)]
    task_solve: bool,
    /// How many trials to include in the task solver prompt (bounded).
    #[arg(long, default_value_t = 2)]
    task_solve_max_trials: usize,
    /// Max characters of evidence text per included trial (bounded).
    #[arg(long, default_value_t = 1600)]
    task_solve_max_evidence_chars: usize,

    /// Optional: path to a previous `eval-judge-swarm` output JSON.
    ///
    /// Used to do “adversarial over time” query selection (curriculum): focus on unseen/failed,
    /// or sort by prior difficulty.
    #[arg(long)]
    curriculum_from: Option<std::path::PathBuf>,
    /// Curriculum mode when `--curriculum-from` is provided.
    ///
    /// Allowed:
    /// - none: ignore prior run
    /// - unseen: only queries not present in the prior run
    /// - failed: only queries that previously were not solved / did not hit expected URLs
    /// - harder: sort ascending by prior best_score (lower score first)
    /// - easier: sort descending by prior best_score (higher score first)
    #[arg(long, default_value = "none")]
    curriculum: String,

    /// If true, write a JSONL transcript for tool + LLM calls (auditing).
    ///
    /// This is intentionally separate from the judge scorecard schema.
    #[arg(long, action = clap::ArgAction::Set, default_value_t = true)]
    transcript: bool,
    /// Transcript output path (JSONL). If omitted, defaults to `<out>.transcript.jsonl`.
    #[arg(long)]
    transcript_jsonl: Option<std::path::PathBuf>,
    /// Max characters stored per transcript string field (bounded, Unicode-safe).
    #[arg(long, default_value_t = 4000)]
    transcript_max_chars: usize,

    /// Output JSON path (default: .generated/webpipe-eval-judge-swarm-<epoch>.json)
    #[arg(long)]
    out: Option<std::path::PathBuf>,
    /// Override "now" for deterministic outputs.
    #[arg(long)]
    now_epoch_s: Option<u64>,
}

#[cfg(all(feature = "eval", feature = "stdio"))]
#[derive(clap::Args, Debug)]
struct EvalCriticLoopCmd {
    /// Query to pass to `web_search_extract`.
    #[arg(long)]
    query: String,
    /// URL(s) to pass directly (repeatable).
    #[arg(long = "url")]
    urls: Vec<String>,
    /// Which fetch backend to use. Allowed: local, firecrawl
    #[arg(long, default_value = "local")]
    fetch_backend: String,
    /// If true, disallow non-localhost networking (safe default for offline fixtures).
    #[arg(long, action = clap::ArgAction::Set, default_value_t = false)]
    no_network: bool,
    /// Output JSON report path (default: .generated/webpipe-eval-critic-loop-<epoch>.json).
    #[arg(long)]
    out: Option<std::path::PathBuf>,
    /// Transcript output path (JSONL). If omitted, defaults to `<out>.transcript.jsonl`.
    #[arg(long)]
    transcript_jsonl: Option<std::path::PathBuf>,
    /// Max characters stored per transcript string field (bounded, Unicode-safe).
    #[arg(long, default_value_t = 4000)]
    transcript_max_chars: usize,
    /// Override "now" for deterministic outputs.
    #[arg(long)]
    now_epoch_s: Option<u64>,
}

#[cfg(all(feature = "eval", feature = "stdio"))]
#[derive(clap::Args, Debug)]
struct EvalCriticRunCmd {
    /// E2E query dataset (json), e.g. `crates/webpipe-mcp/fixtures/e2e_queries_v1.json`.
    #[arg(long)]
    queries_json: std::path::PathBuf,
    /// Base URL used to expand `url_paths` entries in the dataset (for local fixture servers).
    ///
    /// Example: http://127.0.0.1:8080
    #[arg(long)]
    base_url: String,
    /// Which fetch backend to use. Allowed: local, firecrawl
    #[arg(long, default_value = "local")]
    fetch_backend: String,
    /// If true, disallow non-localhost networking (safe default for offline fixtures).
    #[arg(long, action = clap::ArgAction::Set, default_value_t = false)]
    no_network: bool,
    /// Max queries (bounded).
    #[arg(long, default_value_t = 20)]
    max_queries: usize,
    /// Output JSON report path (default: .generated/webpipe-eval-critic-run-<epoch>.json).
    #[arg(long)]
    out: Option<std::path::PathBuf>,
    /// Transcript output path (JSONL). If omitted, defaults to `<out>.transcript.jsonl`.
    #[arg(long)]
    transcript_jsonl: Option<std::path::PathBuf>,
    /// Max characters stored per transcript string field (bounded, Unicode-safe).
    #[arg(long, default_value_t = 4000)]
    transcript_max_chars: usize,
    /// Override "now" for deterministic outputs.
    #[arg(long)]
    now_epoch_s: Option<u64>,
}

#[cfg(feature = "eval")]
#[derive(clap::Args, Debug)]
struct EvalGenerateDomainPackCmd {
    /// Comma-separated domain ids. Allowed: arxiv, news, code
    #[arg(long, default_value = "arxiv,news,code")]
    domains: String,
    /// Max queries per domain (bounded; default: 8).
    #[arg(long, default_value_t = 8)]
    per_domain: usize,
    /// Seed for deterministic ordering/selection.
    #[arg(long, default_value_t = 0)]
    seed: u64,
    /// Optional notes to embed in the output JSON.
    #[arg(long)]
    notes: Option<String>,
    /// Output JSON path (default: .generated/webpipe-e2e-queries-domain-pack-<epoch>.json)
    #[arg(long)]
    out: Option<std::path::PathBuf>,
    /// Override "now" for deterministic outputs.
    #[arg(long)]
    now_epoch_s: Option<u64>,
}

#[cfg(feature = "eval")]
#[derive(clap::Args, Debug)]
struct EvalSearchCmd {
    /// Provider list (comma-separated). Allowed: brave,tavily
    #[arg(long, default_value = "brave,tavily")]
    providers: String,
    /// Query (repeatable).
    #[arg(long)]
    query: Vec<String>,
    /// File containing queries (one per line; blank lines and #comments ignored).
    #[arg(long)]
    queries_file: Vec<std::path::PathBuf>,
    #[arg(long, default_value_t = 10)]
    max_results: usize,
    #[arg(long)]
    language: Option<String>,
    #[arg(long)]
    country: Option<String>,
    /// Output JSON path (default: .generated/webpipe-eval-search-<epoch>.json)
    #[arg(long)]
    out: Option<std::path::PathBuf>,
    /// Override "now" for deterministic outputs.
    #[arg(long)]
    now_epoch_s: Option<u64>,
}

#[cfg(feature = "eval")]
#[derive(clap::Args, Debug)]
struct EvalFetchCmd {
    /// Fetcher list (comma-separated). Allowed: local,firecrawl
    #[arg(long, default_value = "local,firecrawl")]
    fetchers: String,
    /// URL (repeatable).
    #[arg(long)]
    url: Vec<String>,
    /// File containing URLs (one per line; blank lines and #comments ignored).
    #[arg(long)]
    urls_file: Vec<std::path::PathBuf>,
    #[arg(long, default_value_t = 30_000)]
    timeout_ms: u64,
    #[arg(long, default_value_t = 5_000_000)]
    max_bytes: u64,
    /// Max chars stored per output text (keeps artifacts bounded).
    #[arg(long, default_value_t = 10_000)]
    max_text_chars: usize,
    /// Also run a local HTML->text extraction pass and include it in the artifact.
    #[arg(long, default_value_t = false)]
    extract: bool,
    /// Width used for HTML->text extraction (default: 100).
    #[arg(long, default_value_t = 100)]
    extract_width: usize,
    /// Output JSON path (default: .generated/webpipe-eval-fetch-<epoch>.json)
    #[arg(long)]
    out: Option<std::path::PathBuf>,
    /// Override "now" for deterministic outputs.
    #[arg(long)]
    now_epoch_s: Option<u64>,
}

#[derive(clap::Args, Debug)]
struct DoctorCmd {
    /// Output format: pretty|json|text
    #[arg(long = "output", alias = "format", default_value = "pretty")]
    output: String,
    /// Attempt a local stdio MCP handshake (list_tools) to prove Cursor can start the server.
    ///
    /// This is a self-check: it spawns a child `webpipe mcp-stdio` process and calls `list_tools`.
    /// It does not perform any network fetch/search, and it does not print any secret values.
    #[arg(long, action = clap::ArgAction::Set, default_value_t = true)]
    check_stdio: bool,
    /// Timeout for the stdio handshake (ms).
    #[arg(long, default_value_t = 3000)]
    timeout_ms: u64,
}

#[cfg(feature = "stdio")]
#[derive(clap::Args, Debug)]
struct McpListToolsCmd {
    /// Output format: pretty|json
    #[arg(long = "output", alias = "format", default_value = "pretty")]
    output: String,
    /// Timeout for the MCP call (ms).
    #[arg(long, default_value_t = 3000)]
    timeout_ms: u64,
    /// Include the full JSON Schema for each tool (can be large).
    #[arg(long, action = clap::ArgAction::Set, default_value_t = false)]
    include_schema: bool,
}

#[cfg(feature = "stdio")]
#[derive(clap::Args, Debug)]
struct McpCallCmd {
    /// Tool name to call (e.g. "search_evidence", "web_extract", "web_search_extract").
    #[arg(long)]
    tool: String,
    /// JSON object string for tool arguments (e.g. '{"query":"...","max_urls":3}').
    ///
    /// Note: must be a JSON object (top-level `{...}`), not an array.
    #[arg(long, conflicts_with = "args_json_file")]
    args_json: Option<String>,
    /// Path to a JSON file containing either:
    /// - a single JSON object `{...}` (one call), or
    /// - a JSON array of objects `[{...},{...}]` (multiple calls)
    ///
    /// This is the easiest way to compare parameter sets in one spawned MCP server,
    /// so cache/process overhead doesn't dominate (e.g. sequential vs parallel knobs).
    #[arg(long, conflicts_with = "args_json")]
    args_json_file: Option<std::path::PathBuf>,
    /// Repeat the call(s) this many times (default: 1).
    ///
    /// When multiple args objects are provided (via --args-json-file array),
    /// the entire sequence is repeated.
    #[arg(long, default_value_t = 1)]
    repeat: usize,
    /// Timeout for the MCP call (ms).
    #[arg(long, default_value_t = 25_000)]
    timeout_ms: u64,
    /// If true, also print the structured JSON payload (pretty) after the Markdown.
    #[arg(long, action = clap::ArgAction::Set, default_value_t = false)]
    print_structured: bool,
    /// If true, print Markdown for every call (default: only print the first call's Markdown).
    #[arg(long, action = clap::ArgAction::Set, default_value_t = false)]
    print_each: bool,
}

#[cfg(feature = "stdio")]
#[derive(clap::Args, Debug)]
struct McpSweepCmd {
    /// Output format: pretty|json
    #[arg(long = "output", alias = "format", default_value = "pretty")]
    output: String,
    /// Timeout for each MCP call (ms).
    #[arg(long, default_value_t = 25_000)]
    timeout_ms: u64,
    /// Max number of URLs for the search_evidence call (bounded).
    #[arg(long, default_value_t = 4)]
    max_urls: usize,
    /// Max chars for extraction-heavy calls (bounded).
    #[arg(long, default_value_t = 20_000)]
    max_chars: usize,
}

#[cfg(feature = "eval")]
#[derive(clap::Args, Debug)]
struct EvalTranscriptSummarizeCmd {
    /// Transcript JSONL path produced by eval runs (e.g. `<out>.transcript.jsonl`).
    #[arg(long)]
    transcript_jsonl: std::path::PathBuf,
    /// Only include events whose `run_kind` matches this string (optional).
    #[arg(long)]
    run_kind: Option<String>,
    /// Only include events whose `stage` matches this string (optional).
    #[arg(long)]
    stage: Option<String>,
    /// Keep only the top-K slowest events (default: 20).
    #[arg(long, default_value_t = 20)]
    top_k: usize,
    /// Output JSON path (default: .generated/webpipe-eval-transcript-summary-<epoch>.json)
    #[arg(long)]
    out: Option<std::path::PathBuf>,
    /// Override "now" for deterministic outputs.
    #[arg(long)]
    now_epoch_s: Option<u64>,
}

#[cfg(all(feature = "eval", feature = "vlm"))]
#[derive(clap::Args, Debug)]
struct EvalVlmSummarizeCmd {
    /// Input JSON artifact(s) produced by `webpipe vlm-openrouter`.
    #[arg(long = "input", num_args = 1..)]
    inputs: Vec<std::path::PathBuf>,
    /// Output JSON path (default: .generated/webpipe-eval-vlm-summary-<epoch>.json)
    #[arg(long)]
    out: Option<std::path::PathBuf>,
    /// Override "now" for deterministic outputs.
    #[arg(long)]
    now_epoch_s: Option<u64>,
}

#[cfg(all(feature = "eval", feature = "vlm"))]
#[derive(clap::Args, Debug)]
struct EvalVlmRunCmd {
    /// One or more image paths to critique (repeatable).
    #[arg(long = "image")]
    images: Vec<std::path::PathBuf>,
    /// Optional directory of images. If set, we include image-like files in this directory.
    #[arg(long)]
    images_dir: Option<std::path::PathBuf>,
    /// Max number of images taken from `--images-dir` (default: 50).
    #[arg(long, default_value_t = 50)]
    max_images: usize,
    /// Output directory for per-image artifacts (default: .generated/webpipe-eval-vlm-run-<epoch>/).
    #[arg(long)]
    out_dir: Option<std::path::PathBuf>,
    /// Output JSON path for meta summary (default: <out_dir>/summary.json).
    #[arg(long)]
    out_summary: Option<std::path::PathBuf>,
    /// The OpenRouter model id (optional; otherwise env/default like `vlm-openrouter`).
    #[arg(long)]
    model: Option<String>,
    /// The critique prompt/instructions (optional).
    #[arg(long)]
    prompt: Option<String>,
    /// Judge goal(s), in priority order (repeatable).
    ///
    /// Use goals to steer what "good" means for this run. Earlier goals have higher priority.
    #[arg(long = "goal")]
    goals: Vec<String>,
    /// Built-in goal bundle(s) (repeatable).
    ///
    /// Use this to select a curated goal set by id (e.g. `sinprimes_story_v1`). You can combine
    /// multiple bundles; their goals are appended in the order specified.
    #[arg(long = "goal-profile")]
    goal_profiles: Vec<String>,
    /// Print available goal profiles (and their goal lists) then exit 0.
    #[arg(long)]
    list_goal_profiles: bool,
    /// Optional goals file (one goal per line; lines starting with `#` are ignored).
    ///
    /// Goal ordering rule:
    /// - goals are concatenated in this order: `--goal` (argv order), then `--goals-file`, then `--goal-profile`
    /// - earlier goals have higher priority
    #[arg(long)]
    goals_file: Option<std::path::PathBuf>,
    /// Judge profile(s) to run (repeatable).
    ///
    /// This expands the run into a fan-out DAG:
    /// - tasks = images × profiles × trials
    /// - results are collected per image and also summarized per profile
    ///
    /// Built-ins: general, ux_layout, tables, plots, math, writing, runtime
    #[arg(long = "profile")]
    profiles: Vec<String>,
    /// How many VLM trials to run per image (default: 1).
    ///
    /// Use this to reduce variance: aggregate by median score and consensus fixes.
    #[arg(long, default_value_t = 1)]
    trials: u64,
    /// OpenRouter temperature (default: 0.2). Use 0.0 for maximum stability.
    #[arg(long, default_value_t = 0.2)]
    temperature: f64,
    /// Max in-flight VLM calls (bounded parallelism).
    #[arg(long, default_value_t = 6)]
    max_inflight: usize,
    /// Cache mode for per-task artifacts.
    ///
    /// - off: always call the model
    /// - reuse_match: reuse existing artifact only if (model, temperature, prompt_hash, context_hash) match
    /// - reuse_any: reuse any existing artifact file if present
    #[arg(long, default_value = "reuse_match")]
    cache: String,
    /// Optional directory of render-context JSON files (one per image).
    ///
    /// If set, we look for:
    /// - `<context_dir>/<image_filename>.context.json`
    /// - `<context_dir>/<image_stem>.context.json`
    #[arg(long)]
    context_dir: Option<std::path::PathBuf>,
    /// Max chars of context text appended to the prompt (default: 12_000).
    #[arg(long, default_value_t = 12_000)]
    context_max_chars: usize,
    /// Max chars stored per VLM output `text` field.
    #[arg(long, default_value_t = 50_000)]
    max_chars: usize,
    /// Optional transcript JSONL path (appends one compact event per call).
    #[arg(long)]
    transcript_jsonl: Option<std::path::PathBuf>,
    /// Max chars stored per transcript string field.
    #[arg(long, default_value_t = 8_000)]
    transcript_max_chars: usize,
    /// Override "now" for deterministic outputs.
    #[arg(long)]
    now_epoch_s: Option<u64>,
}

#[cfg(all(feature = "eval", feature = "vlm"))]
#[derive(clap::Args, Debug)]
struct EvalVlmBundleCmd {
    /// Page-level VLM summary JSON (typically produced by `eval-vlm-run` on *_full.png).
    #[arg(long)]
    page_summary: std::path::PathBuf,
    /// Plot-level VLM summary JSON (typically produced by `eval-vlm-run` on story_assets/).
    #[arg(long)]
    plots_summary: std::path::PathBuf,
    /// Output JSON path (default: .generated/webpipe-eval-vlm-bundle-<epoch>.json).
    #[arg(long)]
    out: Option<std::path::PathBuf>,
    /// Override "now" for deterministic outputs.
    #[arg(long)]
    now_epoch_s: Option<u64>,
}

#[derive(clap::Args, Debug)]
struct VersionCmd {
    /// Output format: json|text
    #[arg(long = "output", alias = "format", default_value = "json")]
    output: String,
}

#[cfg(feature = "vlm")]
#[derive(clap::Args, Debug)]
struct VlmImageToTextCmd {
    /// Path to an image file (png/jpg/webp) to send to Gemini.
    #[arg(long)]
    image: std::path::PathBuf,
    /// Optional MIME type override (e.g. image/png). If omitted, inferred from file extension.
    #[arg(long)]
    mime_type: Option<String>,
    /// Output JSON path (default: .generated/webpipe-vlm-image-to-text-<epoch>.json)
    #[arg(long)]
    out: Option<std::path::PathBuf>,
    /// Override "now" for deterministic outputs.
    #[arg(long)]
    now_epoch_s: Option<u64>,
}

#[cfg(feature = "vlm")]
#[derive(clap::Args, Debug)]
struct VlmOpenrouterCmd {
    /// Path to an image file (png/jpg/webp) to send as an inline data URL.
    #[arg(long)]
    image: std::path::PathBuf,
    /// The OpenRouter model id (e.g. "google/gemini-2.0-flash-001").
    ///
    /// If omitted, uses `WEBPIPE_OPENROUTER_MODEL` or falls back to a conservative default.
    #[arg(long)]
    model: Option<String>,
    /// The critique prompt/instructions.
    #[arg(long)]
    prompt: Option<String>,
    /// Output JSON path (default: .generated/webpipe-vlm-openrouter-<epoch>.json)
    #[arg(long)]
    out: Option<std::path::PathBuf>,
    /// Override "now" for deterministic outputs.
    #[arg(long)]
    now_epoch_s: Option<u64>,
    /// Max chars stored in the output `text` field (default: 50_000).
    #[arg(long, default_value_t = 50_000)]
    max_chars: usize,
    /// OpenRouter temperature (default: 0.2). Use 0.0 for maximum stability.
    #[arg(long, default_value_t = 0.2)]
    temperature: f64,
    /// Optional render-context JSON file to append to the prompt.
    #[arg(long)]
    context_json: Option<std::path::PathBuf>,
    /// Max chars of context text appended to the prompt (default: 12_000).
    #[arg(long, default_value_t = 12_000)]
    context_max_chars: usize,
    /// Optional transcript JSONL path (appends one compact event per call).
    #[arg(long)]
    transcript_jsonl: Option<std::path::PathBuf>,
    /// Max chars stored per transcript string field (keeps transcript bounded).
    #[arg(long, default_value_t = 8_000)]
    transcript_max_chars: usize,
}

#[cfg(feature = "vlm")]
fn vlm_truncate_chars(s: &str, max_chars: usize) -> (String, bool) {
    if max_chars == 0 {
        return (String::new(), !s.is_empty());
    }
    let mut out = String::new();
    for (n, ch) in s.chars().enumerate() {
        if n >= max_chars {
            return (out, true);
        }
        out.push(ch);
    }
    (out, false)
}

#[cfg(feature = "vlm")]
fn vlm_strip_json_fence(s: &str) -> &str {
    // Common model failure: wrap JSON in ```json ... ```.
    // Keep this tiny and deterministic: strip one leading/trailing fence pair if present.
    let t = s.trim();
    if let Some(rest) = t.strip_prefix("```") {
        // drop optional language tag on the first line
        let rest = rest.trim_start();
        let rest = rest.strip_prefix("json").unwrap_or(rest);
        let rest = rest.strip_prefix("JSON").unwrap_or(rest);
        let rest = rest.trim_start_matches(['\n', '\r']);
        if let Some(end) = rest.rfind("```") {
            return rest[..end].trim();
        }
    }
    t
}

#[cfg(feature = "vlm")]
fn vlm_extract_json_object(s: &str) -> Option<serde_json::Value> {
    // Try strict parse first.
    let t = vlm_strip_json_fence(s);
    if let Ok(v) = serde_json::from_str::<serde_json::Value>(t) {
        if v.is_object() {
            return Some(v);
        }
    }
    // Fallback: search for the first {...} JSON object in the string.
    let mut depth: i64 = 0;
    let mut start: Option<usize> = None;
    for (i, ch) in t.char_indices() {
        match ch {
            '{' => {
                if depth == 0 {
                    start = Some(i);
                }
                depth += 1;
            }
            '}' => {
                if depth > 0 {
                    depth -= 1;
                    if depth == 0 {
                        if let Some(s0) = start {
                            let slice = &t[s0..=i];
                            if let Ok(v) = serde_json::from_str::<serde_json::Value>(slice) {
                                if v.is_object() {
                                    return Some(v);
                                }
                            }
                        }
                        start = None;
                    }
                }
            }
            _ => {}
        }
    }
    None
}

#[cfg(feature = "vlm")]
fn vlm_infer_mime_from_path(p: &std::path::Path) -> Option<&'static str> {
    let ext = p
        .extension()
        .and_then(|s| s.to_str())
        .unwrap_or("")
        .trim()
        .to_ascii_lowercase();
    match ext.as_str() {
        "png" => Some("image/png"),
        "jpg" | "jpeg" => Some("image/jpeg"),
        "webp" => Some("image/webp"),
        "gif" => Some("image/gif"),
        _ => None,
    }
}

#[cfg(feature = "vlm")]
fn vlm_append_jsonl(path: &std::path::Path, line: &serde_json::Value) -> anyhow::Result<()> {
    use std::io::Write;
    let mut f = std::fs::OpenOptions::new()
        .create(true)
        .append(true)
        .open(path)?;
    writeln!(f, "{}", serde_json::to_string(line)?)?;
    Ok(())
}

#[cfg(feature = "vlm")]
fn vlm_format_context_block(ctx: &serde_json::Value, max_chars: usize) -> String {
    // Keep this stable and bounded so prompts remain deterministic-ish.
    let mut lines: Vec<String> = Vec::new();
    if let Some(k) = ctx.get("kind").and_then(|x| x.as_str()) {
        lines.push(format!("kind: {k}"));
    }
    if let Some(p) = ctx.get("page") {
        if let Some(url) = p.get("url").and_then(|x| x.as_str()) {
            lines.push(format!("page.url: {url}"));
        }
        if let Some(t) = p.get("title").and_then(|x| x.as_str()) {
            if !t.trim().is_empty() {
                lines.push(format!("page.title: {t}"));
            }
        }
        if let Some(scroll_h) = p
            .get("scroll")
            .and_then(|x| x.get("scrollHeight"))
            .and_then(|x| x.as_u64())
        {
            lines.push(format!("page.scrollHeight: {scroll_h}"));
        }
        if let Some(y) = p
            .get("scroll_pos")
            .and_then(|x| x.get("y"))
            .and_then(|x| x.as_i64())
        {
            lines.push(format!("page.scrollY: {y}"));
        }
        if let Some(h) = p.get("hash").and_then(|x| x.as_str()) {
            let hh = h.trim();
            if !hh.is_empty() {
                lines.push(format!("page.hash: {hh}"));
            }
        }
        if let Some(toc_n) = p
            .get("toc")
            .and_then(|x| x.get("link_count"))
            .and_then(|x| x.as_u64())
        {
            lines.push(format!("page.toc.link_count: {toc_n}"));
        }
        if let Some(active0) = p
            .get("toc")
            .and_then(|x| x.get("active"))
            .and_then(|x| x.as_array())
            .and_then(|a| a.first())
            .and_then(|x| x.as_str())
        {
            let s = active0.trim();
            if !s.is_empty() {
                lines.push(format!("page.toc.active: {s}"));
            }
        }
    }
    let warn_n = ctx
        .get("console")
        .and_then(|c| c.get("warnings"))
        .and_then(|x| x.as_array())
        .map(|a| a.len())
        .unwrap_or(0);
    let err_n = ctx
        .get("console")
        .and_then(|c| c.get("errors"))
        .and_then(|x| x.as_array())
        .map(|a| a.len())
        .unwrap_or(0);
    let reqfail_n = ctx
        .get("network")
        .and_then(|n| n.get("request_failures"))
        .and_then(|x| x.as_array())
        .map(|a| a.len())
        .unwrap_or(0);
    lines.push(format!(
        "console: warnings={warn_n} errors={err_n} request_failures={reqfail_n}"
    ));

    // Include bounded excerpts (head HTML + a few console errors + request failures).
    if let Some(head) = ctx
        .get("page")
        .and_then(|p| p.get("head_html_excerpt"))
        .and_then(|x| x.as_str())
    {
        let (h, _) = vlm_truncate_chars(head, 2000);
        if !h.trim().is_empty() {
            lines.push("head_html_excerpt:".to_string());
            lines.push(h);
        }
    }

    // Include a tiny computed-style snapshot so "contrast" critiques can be grounded.
    if let Some(cs) = ctx
        .get("page")
        .and_then(|p| p.get("computed_styles"))
        .and_then(|x| x.as_object())
    {
        let keys = [
            "body",
            "layout",
            "meta",
            "caption",
            "progress",
            "progress_bar",
            "toc_aside",
            "toc",
            "toc_card",
            "table",
            "table_row_odd",
            "table_row_even",
            "callout",
            "callout_title",
        ];
        let mut any = false;
        for k in keys {
            if let Some(v) = cs.get(k) {
                if v.is_null() {
                    continue;
                }
                if !any {
                    lines.push("computed_styles:".to_string());
                    any = true;
                }
                let compact = serde_json::to_string(v).unwrap_or_else(|_| "{}".to_string());
                let (s, _) = vlm_truncate_chars(&compact, 800);
                lines.push(format!("- {k}: {s}"));
            }
        }
    }
    if let Some(arr) = ctx
        .get("console")
        .and_then(|c| c.get("errors"))
        .and_then(|x| x.as_array())
    {
        for (i, it) in arr.iter().take(6).enumerate() {
            let t = it.get("text").and_then(|x| x.as_str()).unwrap_or("").trim();
            if !t.is_empty() {
                lines.push(format!("console.error[{i}]: {t}"));
            }
        }
    }
    if let Some(arr) = ctx
        .get("network")
        .and_then(|n| n.get("request_failures"))
        .and_then(|x| x.as_array())
    {
        for (i, it) in arr.iter().take(6).enumerate() {
            let u = it.get("url").and_then(|x| x.as_str()).unwrap_or("").trim();
            let f = it
                .get("failure")
                .and_then(|x| x.as_str())
                .unwrap_or("")
                .trim();
            if !u.is_empty() && !f.is_empty() {
                lines.push(format!("requestfailed[{i}]: {f} {u}"));
            }
        }
    }

    let joined = lines.join("\n");
    let (out, clipped) = vlm_truncate_chars(&joined, max_chars);
    if clipped {
        format!("{out}\n<context_truncated:true>")
    } else {
        out
    }
}

#[cfg(feature = "vlm")]
fn vlm_load_context_for_image(
    context_dir: &std::path::Path,
    img_path: &std::path::Path,
    max_chars: usize,
) -> Option<String> {
    let fname = img_path.file_name().and_then(|s| s.to_str())?;
    let stem = img_path.file_stem().and_then(|s| s.to_str()).unwrap_or("");
    let candidates = [
        context_dir.join(format!("{fname}.context.json")),
        context_dir.join(format!("{stem}.context.json")),
    ];
    for p in candidates {
        if !p.exists() {
            continue;
        }
        let raw = std::fs::read_to_string(&p).ok()?;
        let v: serde_json::Value = serde_json::from_str(&raw).ok()?;
        return Some(vlm_format_context_block(&v, max_chars));
    }
    None
}

#[cfg(feature = "vlm")]
fn vlm_validate_critique_json(v: &serde_json::Value) -> bool {
    let Some(o) = v.as_object() else { return false };

    // Required scalar fields.
    let overall_ok = o.get("overall").and_then(|x| x.as_str()).is_some_and(|s| {
        let s = s.trim();
        !s.is_empty() && s.len() <= 600
    });
    let score_ok = o
        .get("score_0_10")
        .and_then(|x| x.as_f64())
        .is_some_and(|s| (0.0..=10.0).contains(&s));
    let verdict_ok = o
        .get("verdict")
        .and_then(|x| x.as_str())
        .is_some_and(|s| matches!(s.trim(), "good" | "mixed" | "bad"));

    let strengths_ok = match o.get("strengths").and_then(|x| x.as_array()) {
        Some(xs) => {
            let n = xs.len();
            (3..=6).contains(&n)
                && xs.iter().all(|x| {
                    x.as_str().is_some_and(|s| {
                        let s = s.trim();
                        !s.is_empty() && s.len() <= 160
                    })
                })
        }
        None => false,
    };

    // issues[] object schema + size bounds.
    let issues_ok = match o.get("issues").and_then(|x| x.as_array()) {
        Some(xs) => {
            if xs.len() > 40 {
                return false;
            }
            xs.iter().all(|it| {
                let Some(m) = it.as_object() else {
                    return false;
                };
                let area_ok = m.get("area").and_then(|x| x.as_str()).is_some_and(|s| {
                    matches!(
                        s.trim(),
                        "navigation"
                            | "layout_spacing"
                            | "typography"
                            | "visual_hierarchy"
                            | "plots_figures"
                            | "tables_data"
                            | "math_typesetting"
                            | "writing_clarity"
                            | "accessibility"
                            | "technical_runtime"
                    )
                });
                let sev_ok = m
                    .get("severity")
                    .and_then(|x| x.as_str())
                    .is_some_and(|s| matches!(s.trim(), "P0" | "P1" | "P2"));
                let region_ok = m
                    .get("region")
                    .and_then(|x| x.as_str())
                    .is_some_and(|s| matches!(s.trim(), "top" | "middle" | "bottom" | "global"));
                let problem_ok = m
                    .get("problem")
                    .and_then(|x| x.as_str())
                    .is_some_and(|s| !s.trim().is_empty() && s.len() <= 200);
                let evidence_ok = m
                    .get("evidence")
                    .and_then(|x| x.as_str())
                    .is_some_and(|s| !s.trim().is_empty() && s.len() <= 200);
                let fix_ok = m
                    .get("fix")
                    .and_then(|x| x.as_str())
                    .is_some_and(|s| !s.trim().is_empty() && s.len() <= 200);

                area_ok && sev_ok && region_ok && problem_ok && evidence_ok && fix_ok
            })
        }
        None => false,
    };

    let top_3_ok = match o.get("top_3_fixes").and_then(|x| x.as_array()) {
        Some(xs) => {
            if xs.is_empty() || xs.len() > 3 {
                return false;
            }
            let mut seen = std::collections::BTreeSet::new();
            xs.iter().all(|x| {
                x.as_str().is_some_and(|s| {
                    let s = s.trim();
                    !s.is_empty() && s.len() <= 96 && seen.insert(s.to_string())
                })
            })
        }
        None => false,
    };

    overall_ok && score_ok && verdict_ok && strengths_ok && issues_ok && top_3_ok
}

#[cfg(feature = "vlm")]
#[allow(clippy::too_many_arguments)]
async fn vlm_openrouter_call(
    http: &reqwest::Client,
    url: &str,
    api_key: &str,
    model: &str,
    prompt: &str,
    img_path: &std::path::Path,
    max_chars: usize,
    temperature: f64,
    now: u64,
) -> serde_json::Value {
    let image_path_s = img_path.display().to_string();
    let bytes = match std::fs::read(img_path) {
        Ok(b) => b,
        Err(_) => {
            return serde_json::json!({
                "schema_version": 1,
                "kind": "webpipe_vlm_openrouter",
                "generated_at_epoch_s": now,
                "ok": false,
                "inputs": {
                    "image_path": image_path_s,
                    "bytes_len": 0u64,
                    "mime_type": "application/octet-stream",
                    "model": model,
                    "temperature": temperature
                },
                "error": { "code": "image_read_failed" }
            });
        }
    };
    let mime = vlm_infer_mime_from_path(img_path).unwrap_or("application/octet-stream");
    let data_url = format!(
        "data:{};base64,{}",
        mime,
        base64::engine::general_purpose::STANDARD.encode(&bytes)
    );

    // OpenAI-compatible multimodal message: content as an array of parts.
    let req = serde_json::json!({
        "model": model,
        "messages": [{
            "role": "user",
            "content": [
                {"type": "text", "text": prompt},
                {"type": "image_url", "image_url": {"url": data_url}}
            ]
        }],
        "temperature": temperature
    });

    let resp = http
        .post(url)
        .header("Authorization", format!("Bearer {}", api_key))
        .header("Content-Type", "application/json")
        .json(&req)
        .timeout(std::time::Duration::from_millis(60_000))
        .send()
        .await;

    match resp {
        Ok(r) => {
            let status = r.status().as_u16();
            let v: serde_json::Value = match r.json().await {
                Ok(v) => v,
                Err(_) => {
                    return serde_json::json!({
                        "schema_version": 1,
                        "kind": "webpipe_vlm_openrouter",
                        "generated_at_epoch_s": now,
                        "ok": false,
                        "inputs": {
                            "image_path": image_path_s,
                            "bytes_len": bytes.len(),
                            "mime_type": mime,
                            "model": model,
                            "temperature": temperature
                        },
                        "http": { "status": status },
                        "error": { "code": "openrouter_bad_json" }
                    });
                }
            };

            if v.get("ok").and_then(|x| x.as_bool()) == Some(false) || status >= 400 {
                return serde_json::json!({
                    "schema_version": 1,
                    "kind": "webpipe_vlm_openrouter",
                    "generated_at_epoch_s": now,
                    "ok": false,
                    "inputs": {
                        "image_path": image_path_s,
                        "bytes_len": bytes.len(),
                        "mime_type": mime,
                        "model": model,
                        "temperature": temperature
                    },
                    "http": { "status": status },
                    "provider_response": v,
                    "error": { "code": "openrouter_non_success" }
                });
            }

            // Extract choices[0].message.content (string or parts).
            let mut text = String::new();
            if let Some(c0) = v
                .get("choices")
                .and_then(|x| x.as_array())
                .and_then(|a| a.first())
            {
                let content = c0
                    .get("message")
                    .and_then(|m| m.get("content"))
                    .cloned()
                    .unwrap_or(serde_json::Value::Null);
                match content {
                    serde_json::Value::String(s) => text = s,
                    serde_json::Value::Array(parts) => {
                        for p in parts {
                            if let Some(t) = p.get("text").and_then(|x| x.as_str()) {
                                if !text.is_empty() {
                                    text.push('\n');
                                }
                                text.push_str(t);
                            }
                        }
                    }
                    _ => {}
                }
            }
            let (text_trunc, clipped) = vlm_truncate_chars(&text, max_chars);
            let parsed = vlm_extract_json_object(&text_trunc).filter(vlm_validate_critique_json);
            serde_json::json!({
                "schema_version": 1,
                "kind": "webpipe_vlm_openrouter",
                "generated_at_epoch_s": now,
                "ok": true,
                "inputs": {
                    "image_path": image_path_s,
                    "bytes_len": bytes.len(),
                    "mime_type": mime,
                    "model": model,
                    "temperature": temperature
                },
                "http": { "status": status },
                "text": text_trunc,
                "text_truncated": clipped,
                "parsed": parsed,
                "parsed_ok": parsed.is_some()
            })
        }
        Err(_) => serde_json::json!({
            "schema_version": 1,
            "kind": "webpipe_vlm_openrouter",
            "generated_at_epoch_s": now,
            "ok": false,
            "inputs": {
                "image_path": image_path_s,
                "bytes_len": 0u64,
                "mime_type": "application/octet-stream",
                "model": model,
                "temperature": temperature
            },
            "error": { "code": "openrouter_request_failed" }
        }),
    }
}

#[cfg(feature = "stdio")]
mod mcp {
    use super::*;
    use rmcp::{
        handler::server::router::prompt::PromptRouter as RmcpPromptRouter,
        handler::server::router::tool::ToolRouter as RmcpToolRouter,
        handler::server::wrapper::Parameters,
        model::{
            Annotated, CallToolResult, Content, GetPromptRequestParam, GetPromptResult,
            ListPromptsResult, ListResourcesResult, ListToolsResult, PaginatedRequestParam,
            PromptMessage, PromptMessageRole, RawResource, ReadResourceRequestParam,
            ReadResourceResult, Resource, ResourceContents, ServerCapabilities, ServerInfo,
        },
        prompt, prompt_router,
        service::RequestContext,
        tool, tool_router, ErrorData as McpError, RoleServer, ServiceExt,
    };
    use schemars::generate::SchemaSettings;
    use schemars::{JsonSchema, SchemaGenerator};
    use serde::Deserialize;
    use std::collections::BTreeMap;
    use std::path::PathBuf;
    use std::sync::Arc;
    use webpipe_core::{
        Error as WebpipeError, FetchBackend, FetchCachePolicy, FetchRequest, FetchSource,
        SearchProvider, SearchQuery,
    };
    use webpipe_local::LocalFetcher;

    // Breaking output shape changes (legacy field removal) are tracked via schema_version.
    const SCHEMA_VERSION: u64 = 2;

    fn p<T>(v: T) -> Parameters<Option<T>> {
        Parameters(Some(v))
    }

    #[derive(Clone, Copy, Debug, Eq, PartialEq)]
    enum McpToolset {
        /// Minimal, Cursor-first surface (“excellent”).
        Normal,
        /// Full surface for debugging and niche workflows.
        Debug,
    }

    fn mcp_toolset_from_env() -> McpToolset {
        match std::env::var("WEBPIPE_MCP_TOOLSET")
            .unwrap_or_else(|_| "normal".to_string())
            .trim()
            .to_ascii_lowercase()
            .as_str()
        {
            // Canonical modes:
            "normal" => McpToolset::Normal,
            "debug" => McpToolset::Debug,
            // Back-compat aliases:
            "slim" | "offline" | "research" => McpToolset::Normal,
            "full" => McpToolset::Debug,
            _ => McpToolset::Normal,
        }
    }

    fn mcp_toolset_name(t: McpToolset) -> &'static str {
        match t {
            McpToolset::Normal => "normal",
            McpToolset::Debug => "debug",
        }
    }

    /// Maps deprecated tool names to their current canonical names.
    ///
    /// When a tool is renamed, add an entry here to document the migration path.
    /// Deprecated tools remain callable (they delegate to the canonical handler)
    /// but are excluded from the default visible toolset.
    ///
    /// Inspired by github/github-mcp-server's `deprecated_tool_aliases.go`.
    /// Used in `webpipe_meta` to populate `supported.deprecated_aliases`.
    #[allow(dead_code)]
    const DEPRECATED_TOOL_ALIASES: &[(&str, &str)] = &[
        // Fetch aliases (redundant names; web_fetch / web_extract are canonical)
        ("http_fetch", "web_fetch"),
        ("page_extract", "web_extract"),
        ("page_extract_text", "web_extract"),
        // arxiv_search and arxiv_enrich are now merged into the single `arxiv` tool.
        ("arxiv_search", "arxiv"),
        ("arxiv_enrich", "arxiv"),
        // Search-extract aliases (search_evidence is the Normal-toolset canonical name)
        ("_web_search_extract_debug", "search_evidence"),
    ];

    fn mcp_tool_allowed(tool_name: &str, toolset: McpToolset) -> bool {
        // Small, opinionated surfaces: the point is to reduce choice overload in Cursor.
        match toolset {
            McpToolset::Debug => true,
            // Normal toolset: one meta tool (webpipe_meta covers usage + usage_reset via method=),
            // core fetch/extract/search tools, and research/academic tools.
            //
            // Design invariant: every tool in this list must work with ZERO API keys configured.
            // API-key-gated tools (web_perplexity, etc.) are shown conditionally — only when
            // their key is actually set — so the default surface is fully functional out of the box.
            //
            // Deprecated aliases are excluded here; they remain callable but not default-visible.
            McpToolset::Normal => {
                // Always-on: work without any API keys.
                let keyless = matches!(
                    tool_name,
                    "webpipe_meta" | "web_extract" | "search_evidence" | "arxiv" | "paper_search"
                );
                if keyless {
                    return true;
                }
                // Conditionally visible: show only when the required key is configured.
                // This keeps the default surface fully functional while surfacing
                // optional premium integrations when they're actually usable.
                match tool_name {
                    "web_perplexity" => {
                        has_env("WEBPIPE_PERPLEXITY_API_KEY") || has_env("PERPLEXITY_API_KEY")
                    }
                    _ => false,
                }
            }
        }
    }

    /// Build a tool `inputSchema` that is compatible with stricter MCP clients.
    ///
    /// `rmcp` derives schemas from handler param types, and `Parameters<Option<T>>` tends to
    /// produce a nullable-root schema (e.g. `anyOf: [T, null]`). Some clients appear to require
    /// the tool `inputSchema` to be an object schema (not a union), and will ignore tools if the
    /// schema is not recognized.
    ///
    /// We keep `Option<T>` in the handler (so callers may omit args), but we *publish* the schema
    /// for `T` directly (non-null), using Draft-07 for broad compatibility.
    fn tool_input_schema_draft07<T: JsonSchema>() -> serde_json::Map<String, serde_json::Value> {
        let settings = SchemaSettings::draft07();
        let gen = SchemaGenerator::new(settings);
        let schema = gen.into_root_schema_for::<T>();
        match serde_json::to_value(&schema) {
            Ok(serde_json::Value::Object(m)) => m,
            _ => serde_json::Map::new(),
        }
    }

    #[path = "envelope.rs"]
    mod envelope;
    use envelope::*;

    // ---- Minimal self-contained helpers (public-repo friendly) ----
    //
    // Historically, this repo used workspace-local crates for text normalization, Pareto selection,
    // and routing windows. For a standalone public repository, we keep small, deterministic
    // implementations here instead.

    mod textprep {
        /// Deterministic, lightweight normalization used for stable keys.
        ///
        /// Goals:
        /// - never panic
        /// - ASCII-lowercase, whitespace-normalized
        /// - keep it small (not a full tokenizer)
        pub fn scrub(s: &str) -> String {
            let mut out = String::new();
            let mut last_space = true;
            for ch in s.chars() {
                // Fold a small set of common Greek letters into ASCII tokens.
                //
                // Rationale: academic text and PDFs often contain β/Ω/θ/... where users naturally
                // type "beta"/"omega"/"theta" in queries. This keeps keys and overlap scoring
                // stable and more intuitive without changing display text.
                match ch {
                    'α' | 'Α' => {
                        if !last_space {
                            out.push(' ');
                        }
                        out.push_str("alpha ");
                        last_space = true;
                        continue;
                    }
                    'β' | 'Β' => {
                        if !last_space {
                            out.push(' ');
                        }
                        out.push_str("beta ");
                        last_space = true;
                        continue;
                    }
                    'γ' | 'Γ' => {
                        if !last_space {
                            out.push(' ');
                        }
                        out.push_str("gamma ");
                        last_space = true;
                        continue;
                    }
                    'δ' | 'Δ' => {
                        if !last_space {
                            out.push(' ');
                        }
                        out.push_str("delta ");
                        last_space = true;
                        continue;
                    }
                    'ε' | 'ϵ' | 'Ε' => {
                        if !last_space {
                            out.push(' ');
                        }
                        out.push_str("epsilon ");
                        last_space = true;
                        continue;
                    }
                    'λ' | 'Λ' => {
                        if !last_space {
                            out.push(' ');
                        }
                        out.push_str("lambda ");
                        last_space = true;
                        continue;
                    }
                    'μ' | 'µ' | 'Μ' => {
                        if !last_space {
                            out.push(' ');
                        }
                        out.push_str("mu ");
                        last_space = true;
                        continue;
                    }
                    'π' | 'Π' => {
                        if !last_space {
                            out.push(' ');
                        }
                        out.push_str("pi ");
                        last_space = true;
                        continue;
                    }
                    'φ' | 'ϕ' | 'Φ' => {
                        if !last_space {
                            out.push(' ');
                        }
                        out.push_str("phi ");
                        last_space = true;
                        continue;
                    }
                    'ω' | 'Ω' | 'Ω' => {
                        if !last_space {
                            out.push(' ');
                        }
                        out.push_str("omega ");
                        last_space = true;
                        continue;
                    }
                    'ρ' | 'Ρ' => {
                        if !last_space {
                            out.push(' ');
                        }
                        out.push_str("rho ");
                        last_space = true;
                        continue;
                    }
                    'σ' | 'ς' | 'Σ' => {
                        if !last_space {
                            out.push(' ');
                        }
                        out.push_str("sigma ");
                        last_space = true;
                        continue;
                    }
                    'θ' | 'ϑ' => {
                        if !last_space {
                            out.push(' ');
                        }
                        out.push_str("theta ");
                        last_space = true;
                        continue;
                    }
                    _ => {}
                }
                let c = ch.to_ascii_lowercase();
                if c.is_ascii_alphanumeric() {
                    out.push(c);
                    last_space = false;
                } else if c.is_whitespace() || c == '-' || c == '_' || c == '/' {
                    if !last_space {
                        out.push(' ');
                        last_space = true;
                    }
                } else {
                    // drop other punctuation/control
                }
            }
            out.trim().to_string()
        }

        pub mod stopwords {
            // Small, stable list: enough to avoid obvious junk tokens.
            pub const ENGLISH: [&str; 47] = [
                "a", "an", "and", "are", "as", "at", "be", "by", "for", "from", "has", "have",
                "how", "i", "in", "is", "it", "its", "me", "my", "of", "on", "or", "our", "s",
                "she", "that", "the", "their", "them", "there", "they", "this", "to", "was", "we",
                "were", "what", "when", "where", "which", "who", "why", "will", "with", "you",
                "your",
            ];
        }
    }

    mod pare {
        /// Return indices of the Pareto frontier (maximize all dimensions).
        pub fn pareto_indices(metrics: &[Vec<f32>]) -> Option<Vec<usize>> {
            if metrics.is_empty() {
                return None;
            }
            let dim = metrics[0].len();
            if dim == 0 || metrics.iter().any(|v| v.len() != dim) {
                return None;
            }

            fn dominates(a: &[f32], b: &[f32]) -> bool {
                // a dominates b if a >= b in all dims and a > b in at least one.
                let mut any_strict = false;
                for i in 0..a.len() {
                    if a[i] < b[i] {
                        return false;
                    }
                    if a[i] > b[i] {
                        any_strict = true;
                    }
                }
                any_strict
            }

            let mut frontier: Vec<usize> = Vec::new();
            'outer: for i in 0..metrics.len() {
                for j in 0..metrics.len() {
                    if i == j {
                        continue;
                    }
                    if dominates(&metrics[j], &metrics[i]) {
                        continue 'outer;
                    }
                }
                frontier.push(i);
            }
            Some(frontier)
        }
    }

    mod muxer {
        use serde::Serialize;
        use std::collections::VecDeque;

        #[derive(Debug, Clone, Copy)]
        pub struct Outcome {
            pub ok: bool,
            pub http_429: bool,
            pub junk: bool,
            pub hard_junk: bool,
            pub cost_units: u64,
            pub elapsed_ms: u64,
        }

        #[derive(Debug, Clone, Default, Serialize)]
        pub struct Summary {
            pub calls: u64,
            pub ok: u64,
            pub http_429: u64,
            pub junk: u64,
            pub hard_junk: u64,
            pub cost_units: u64,
            pub elapsed_ms_sum: u64,
        }

        impl Summary {
            pub fn ok_rate(&self) -> f64 {
                if self.calls == 0 {
                    0.0
                } else {
                    (self.ok as f64) / (self.calls as f64)
                }
            }
            pub fn http_429_rate(&self) -> f64 {
                if self.calls == 0 {
                    0.0
                } else {
                    (self.http_429 as f64) / (self.calls as f64)
                }
            }
            pub fn junk_rate(&self) -> f64 {
                if self.calls == 0 {
                    0.0
                } else {
                    (self.junk as f64) / (self.calls as f64)
                }
            }
            pub fn hard_junk_rate(&self) -> f64 {
                if self.calls == 0 {
                    0.0
                } else {
                    (self.hard_junk as f64) / (self.calls as f64)
                }
            }
            pub fn mean_cost_units(&self) -> f64 {
                if self.calls == 0 {
                    0.0
                } else {
                    (self.cost_units as f64) / (self.calls as f64)
                }
            }
            pub fn mean_latency_ms(&self) -> f64 {
                if self.calls == 0 {
                    0.0
                } else {
                    (self.elapsed_ms_sum as f64) / (self.calls as f64)
                }
            }
        }

        #[derive(Debug, Clone)]
        pub struct Window {
            cap: usize,
            buf: VecDeque<Outcome>,
        }

        impl Window {
            pub fn new(cap: usize) -> Self {
                Self {
                    cap: cap.max(1),
                    buf: VecDeque::new(),
                }
            }

            pub fn push(&mut self, o: Outcome) {
                self.buf.push_back(o);
                while self.buf.len() > self.cap {
                    self.buf.pop_front();
                }
            }

            pub fn set_last_junk_level(&mut self, junk: bool, hard_junk: bool) {
                if let Some(last) = self.buf.back_mut() {
                    last.junk = junk;
                    last.hard_junk = hard_junk;
                }
            }

            pub fn summary(&self) -> Summary {
                let mut s = Summary::default();
                for o in &self.buf {
                    s.calls += 1;
                    s.ok += o.ok as u64;
                    s.http_429 += o.http_429 as u64;
                    s.junk += o.junk as u64;
                    s.hard_junk += o.hard_junk as u64;
                    s.cost_units = s.cost_units.saturating_add(o.cost_units);
                    s.elapsed_ms_sum = s.elapsed_ms_sum.saturating_add(o.elapsed_ms);
                }
                s
            }
        }

        #[derive(Debug, Clone)]
        pub struct MabConfig {
            pub exploration_c: f64,
            pub cost_weight: f64,
            pub latency_weight: f64,
            pub junk_weight: f64,
            pub hard_junk_weight: f64,
            pub max_junk_rate: Option<f64>,
            pub max_hard_junk_rate: Option<f64>,
            pub max_http_429_rate: Option<f64>,
            pub max_mean_cost_units: Option<f64>,
        }

        impl Default for MabConfig {
            fn default() -> Self {
                Self {
                    exploration_c: 0.7,
                    cost_weight: 0.0,
                    latency_weight: 0.0,
                    junk_weight: 0.0,
                    hard_junk_weight: 0.0,
                    max_junk_rate: None,
                    max_hard_junk_rate: None,
                    max_http_429_rate: None,
                    max_mean_cost_units: None,
                }
            }
        }

        #[derive(Debug, Clone, Serialize)]
        pub struct CandidateRow {
            pub name: String,
            pub calls: u64,
            pub ok_rate: f64,
            pub http_429_rate: f64,
            pub junk_rate: f64,
            pub hard_junk_rate: f64,
            pub mean_cost_units: f64,
            pub mean_latency_ms: f64,
            pub score: f64,
            pub filtered: bool,
        }

        #[derive(Debug, Clone, Serialize)]
        pub struct MabSelection {
            pub chosen: String,
            pub frontier: Vec<String>,
            pub candidates: Vec<CandidateRow>,
        }

        pub fn select_mab(
            order: &[String],
            summaries: &std::collections::BTreeMap<String, Summary>,
            cfg: &MabConfig,
        ) -> MabSelection {
            let total_calls: u64 = summaries.values().map(|s| s.calls).sum();
            let ln_total = ((total_calls.max(1)) as f64).ln();

            let mut rows: Vec<CandidateRow> = Vec::new();
            for name in order {
                let s = summaries.get(name).cloned().unwrap_or_default();
                let ok_rate = s.ok_rate();
                let http_429_rate = s.http_429_rate();
                let junk_rate = s.junk_rate();
                let hard_junk_rate = s.hard_junk_rate();
                let mean_cost_units = s.mean_cost_units();
                let mean_latency_ms = s.mean_latency_ms();

                let mut filtered = false;
                if let Some(max) = cfg.max_http_429_rate {
                    filtered |= http_429_rate > max;
                }
                if let Some(max) = cfg.max_junk_rate {
                    filtered |= junk_rate > max;
                }
                if let Some(max) = cfg.max_hard_junk_rate {
                    filtered |= hard_junk_rate > max;
                }
                if let Some(max) = cfg.max_mean_cost_units {
                    filtered |= mean_cost_units > max;
                }

                // Deterministic UCB-ish objective (maximize).
                // We use ok_rate as the primary reward and subtract weighted costs/risks.
                let base = ok_rate
                    - cfg.cost_weight * mean_cost_units
                    - cfg.latency_weight * (mean_latency_ms / 1000.0)
                    - cfg.junk_weight * junk_rate
                    - cfg.hard_junk_weight * hard_junk_rate;
                let explore = if s.calls == 0 {
                    cfg.exploration_c
                } else {
                    cfg.exploration_c * ((ln_total / (s.calls as f64)).sqrt())
                };
                let score = base + explore;

                rows.push(CandidateRow {
                    name: name.clone(),
                    calls: s.calls,
                    ok_rate,
                    http_429_rate,
                    junk_rate,
                    hard_junk_rate,
                    mean_cost_units,
                    mean_latency_ms,
                    score,
                    filtered,
                });
            }

            // If constraints filtered everything, fall back to unfiltered.
            let any_unfiltered = rows.iter().any(|r| !r.filtered);
            if !any_unfiltered {
                for r in &mut rows {
                    r.filtered = false;
                }
            }

            let mut chosen = String::new();
            let mut best = f64::NEG_INFINITY;
            for r in &rows {
                if r.filtered {
                    continue;
                }
                // Deterministic tie-breakers when scores are equal:
                // prefer lower hard-junk, then lower junk, then lower 429s, then lower cost/latency,
                // then earlier order.
                let eps = 1e-12;
                let better = if r.score > best + eps {
                    true
                } else if (r.score - best).abs() <= eps {
                    // Compare against current best row (if any).
                    if chosen.is_empty() {
                        true
                    } else {
                        let cur = rows.iter().find(|x| x.name == chosen);
                        if let Some(cur) = cur {
                            (
                                r.hard_junk_rate,
                                r.junk_rate,
                                r.http_429_rate,
                                r.mean_cost_units,
                                r.mean_latency_ms,
                            ) < (
                                cur.hard_junk_rate,
                                cur.junk_rate,
                                cur.http_429_rate,
                                cur.mean_cost_units,
                                cur.mean_latency_ms,
                            )
                        } else {
                            true
                        }
                    }
                } else {
                    false
                };
                if better {
                    best = r.score;
                    chosen = r.name.clone();
                }
            }
            if chosen.is_empty() {
                chosen = order.first().cloned().unwrap_or_default();
            }

            // Frontier: best-effort list of top candidates by score (bounded, stable).
            let mut frontier = rows
                .iter()
                .filter(|r| !r.filtered)
                .cloned()
                .collect::<Vec<_>>();
            frontier.sort_by(|a, b| {
                b.score
                    .partial_cmp(&a.score)
                    .unwrap_or(std::cmp::Ordering::Equal)
            });
            let frontier_names = frontier.into_iter().take(3).map(|r| r.name).collect();

            MabSelection {
                chosen,
                frontier: frontier_names,
                candidates: rows,
            }
        }
    }

    fn cache_dir_from_env() -> Option<PathBuf> {
        std::env::var("WEBPIPE_CACHE_DIR").ok().map(PathBuf::from)
    }

    #[derive(Clone, Copy, Debug, Eq, PartialEq)]
    enum PrivacyMode {
        Normal,
        Offline,
        Anonymous,
    }

    fn privacy_mode_from_env() -> PrivacyMode {
        // Public-ish contract:
        // - WEBPIPE_PRIVACY_MODE=offline|anonymous|normal
        // - WEBPIPE_OFFLINE_ONLY keeps working as a legacy switch for offline mode.
        let v = std::env::var("WEBPIPE_PRIVACY_MODE")
            .unwrap_or_default()
            .trim()
            .to_ascii_lowercase();
        match v.as_str() {
            "offline" => PrivacyMode::Offline,
            "anonymous" => PrivacyMode::Anonymous,
            "normal" | "" => {
                if offline_only_enabled() {
                    PrivacyMode::Offline
                } else {
                    PrivacyMode::Normal
                }
            }
            _ => {
                // Unknown values should not silently weaken privacy; fall back to Offline when
                // the legacy switch is set, else Normal.
                if offline_only_enabled() {
                    PrivacyMode::Offline
                } else {
                    PrivacyMode::Normal
                }
            }
        }
    }

    fn anon_proxy_from_env() -> Option<String> {
        // Prefer an explicit knob; fall back to standard proxy envs.
        for k in [
            "WEBPIPE_ANON_PROXY",
            "WEBPIPE_PROXY",
            "ALL_PROXY",
            "HTTPS_PROXY",
            "HTTP_PROXY",
        ] {
            if let Ok(v) = std::env::var(k) {
                let s = v.trim();
                if !s.is_empty() {
                    return Some(s.to_string());
                }
            }
        }
        None
    }

    fn offline_only_enabled() -> bool {
        // Full-privacy / offline-only mode: forbid non-localhost networking regardless of tool args.
        // Accept a few common truthy values.
        match std::env::var("WEBPIPE_OFFLINE_ONLY") {
            Ok(v) => matches!(
                v.trim().to_ascii_lowercase().as_str(),
                "1" | "true" | "yes" | "on"
            ),
            Err(_) => false,
        }
    }

    fn is_localhost_url(url: &str) -> bool {
        // Best-effort: enough to keep tests hermetic and avoid accidental outbound fetches.
        // (We treat parse failures as non-localhost.)
        let Ok(u) = reqwest::Url::parse(url.trim()) else {
            return false;
        };
        match u.host_str() {
            Some("localhost") => true,
            Some("127.0.0.1") => true,
            Some("::1") => true,
            Some(h) => h.ends_with(".localhost"),
            None => false,
        }
    }

    pub(crate) fn default_cache_dir() -> PathBuf {
        // Prefer a persistent per-user cache directory (so warm-cache + no_network works across
        // restarts), with a temp-dir fallback.
        dirs::cache_dir()
            .unwrap_or_else(std::env::temp_dir)
            .join("webpipe-cache")
    }

    fn has_env(k: &str) -> bool {
        std::env::var(k).ok().is_some_and(|v| !v.trim().is_empty())
    }

    fn normalize_warning_code(w: &str) -> &str {
        match w {
            // Canonicalize historical drift.
            "text_truncated_by_max_text_chars" => "text_truncated_by_max_chars",
            // Canonicalize “capability unavailable” variants.
            "links_unavailable_for_firecrawl" => "links_unavailable",
            "links_unavailable_for_pdf" => "links_unavailable",
            "headers_unavailable_for_firecrawl" => "headers_unavailable",
            "text_unavailable_for_pdf_use_web_extract" => "text_unavailable_for_pdf",
            // Default: pass through existing stable codes.
            _ => w,
        }
    }

    fn url_looks_like_auth_or_challenge(url: &str) -> bool {
        // Generic (non-domain-specific) heuristics to avoid agentic loops getting stuck on
        // auth walls / JS challenges / consent gates.
        let u = url.to_ascii_lowercase();
        // GitHub UI routes that almost always require auth (and are rarely substantive content).
        if u.contains("github.com/") && u.contains("/edit/") {
            return true;
        }
        // Path-ish signals.
        for needle in [
            "/login",
            "/log-in",
            "/signin",
            "/sign-in",
            "/signup",
            "/sign-up",
            "/register",
            "/auth",
            "/oauth",
            "/sso",
            "/captcha",
            "/challenge",
            "/consent",
        ] {
            if u.contains(needle) {
                return true;
            }
        }
        // Wiki / CMS patterns that often route to auth walls (but don't include "/login" etc).
        // Example observed in live use: Wikipedia `Special:CreateAccount` redirects.
        for needle in [
            "special:createaccount",
            "special:userlogin",
            "special:login",
        ] {
            if u.contains(needle) {
                return true;
            }
        }
        // Wikipedia special pages are navigation/utility, not substantive content.
        if u.contains("wikipedia.org/wiki/special:") {
            return true;
        }
        // Query-ish signals.
        for needle in [
            "returnurl=",
            "returnto=",
            "redirect=",
            "continue=",
            "next=",
            "callback=",
        ] {
            if u.contains(needle) {
                return true;
            }
        }
        // Common bot mitigation tokens (best-effort).
        for needle in ["cf_chl", "cloudflare", "turnstile"] {
            if u.contains(needle) {
                return true;
            }
        }
        false
    }

    fn url_looks_like_js_challenge_prone_host(url: &str) -> bool {
        // Domain-specific heuristics for hosts that commonly block headless/local fetching
        // with JS challenges (and therefore frequently produce low-signal extracts).
        //
        // Keep this list conservative: it is used only for score *penalties*, not hard exclusion.
        let u = url.to_ascii_lowercase();
        u.contains("dl.acm.org/")
    }

    fn url_is_localhost(url: &str) -> bool {
        // Used to interpret `no_network=true` as “no non-localhost networking”.
        //
        // This keeps offline/CI harnesses safe while still allowing deterministic localhost
        // fixture servers.
        let Ok(u) = reqwest::Url::parse(url.trim()) else {
            return false;
        };
        let Some(h) = u.host_str() else {
            return false;
        };
        let h = h.trim().to_ascii_lowercase();
        if h == "localhost" {
            return true;
        }
        if let Ok(ip) = h.parse::<std::net::IpAddr>() {
            return ip.is_loopback();
        }
        false
    }

    fn normalize_domain_rule(raw: &str) -> Option<String> {
        // Accept either a bare hostname ("example.com") or a full URL ("https://example.com/x").
        // Normalize to lowercase host only.
        let s = raw.trim();
        if s.is_empty() {
            return None;
        }
        let lower = s.to_ascii_lowercase();
        if lower.contains("://") {
            if let Ok(u) = reqwest::Url::parse(s) {
                if let Some(h) = u.host_str() {
                    let h = h.trim().to_ascii_lowercase();
                    return (!h.is_empty()).then_some(h);
                }
            }
            return None;
        }
        // Strip any path-ish suffix.
        let host = lower
            .split('/')
            .next()
            .unwrap_or("")
            .trim()
            .trim_start_matches('.');
        (!host.is_empty()).then_some(host.to_string())
    }

    fn host_matches_domain_rule(host: &str, rule: &str) -> bool {
        let h = host.trim().to_ascii_lowercase();
        let r = rule.trim().to_ascii_lowercase();
        if h.is_empty() || r.is_empty() {
            return false;
        }
        if h == r {
            return true;
        }
        h.ends_with(&format!(".{r}"))
    }

    fn url_allowed_by_domain_filters(
        url: &str,
        domains_allow: &[String],
        domains_deny: &[String],
    ) -> bool {
        // Deny wins over allow.
        let Ok(u) = reqwest::Url::parse(url.trim()) else {
            return false;
        };
        let Some(host) = u.host_str() else {
            return false;
        };
        let host = host.trim().to_ascii_lowercase();
        for d in domains_deny {
            if host_matches_domain_rule(&host, d) {
                return false;
            }
        }
        if !domains_allow.is_empty() {
            for a in domains_allow {
                if host_matches_domain_rule(&host, a) {
                    return true;
                }
            }
            return false;
        }
        true
    }

    fn url_looks_like_promo_or_tracking(url: &str) -> bool {
        // Heuristics for "marketing/promo" URLs that frequently show up in search results and
        // waste extraction budget (e.g. banners, home pages with UTM tags).
        //
        // We treat these as *soft junk*: skip when we have enough alternatives, but allow as
        // a fallback rather than returning an empty set.
        let u = url.to_ascii_lowercase();

        // Known low-value index pages that frequently collide with query tokens.
        if u.contains("jmlr.org/mloss") {
            return true;
        }
        // “Quick review” aggregators are usually derivative and noisy for evidence packs.
        if u.contains("liner.com/review/") {
            return true;
        }
        // Stack-style product/vuln aggregators are usually noisy for primary sources.
        if u.contains("stack.watch/") {
            return true;
        }
        // Changelog/announcements feeds are rarely a good answer for technical queries.
        if u.contains("github.blog/changelog") {
            return true;
        }
        // Donation pages are almost never relevant evidence.
        if u.contains("donate.wikimedia.org/") {
            return true;
        }
        // Wikidata entity pages are typically low-signal for definition queries.
        if u.contains("wikidata.org/wiki/special:entitypage/") {
            return true;
        }
        // Wikipedia utility/admin pages.
        if u.contains("wikipedia.org/wiki/special:")
            || u.contains("wikipedia.org/wiki/mediawiki:")
            || u.contains("wikipedia.org/wiki/wikipedia:")
            || u.contains("wikipedia.org/wiki/help:")
        {
            return true;
        }
        if u.contains("wikipedia.org/wiki/portal:")
            || u.contains("wikipedia.org/wiki/main_page")
            || u.contains("wikipedia.org/w/index.php?title=special:")
        {
            return true;
        }
        if u.contains("foundation.wikimedia.org/wiki/special:") {
            return true;
        }
        // Non-English Wikipedia is often a detour unless explicitly requested.
        // (Prefer the English canonical page when we have enough alternatives.)
        if u.contains("wikipedia.org/") {
            // Any non-English subdomain is treated as soft-junk by default.
            if !u.contains("://en.wikipedia.org/") && !u.contains("://www.wikipedia.org/") {
                return true;
            }
        }

        // Tracking params (very common across vendors).
        for needle in ["utm_", "gclid=", "fbclid=", "yclid=", "mc_cid=", "mc_eid="] {
            if u.contains(needle) {
                return true;
            }
        }

        // Common promo-ish paths. Keep this conservative.
        for needle in [
            "/pricing",
            "/enterprise",
            "/features",
            "/customers",
            "/case-studies",
            "/solutions",
            "/contact",
            "/newsletter",
            "/marketplace",
            "/home",
            "/showcase",
            "/events",
        ] {
            if u.contains(needle) {
                return true;
            }
        }

        false
    }

    fn url_looks_like_low_value_homepage(url: &str) -> bool {
        // Soft-junk heuristic: bare homepages are rarely the best answer for a specific query,
        // and they frequently waste extraction budget (especially for sites like ResearchGate,
        // generic portals, etc.). We treat these as "soft junk": skip when we have enough
        // alternatives, but allow as a fallback rather than returning nothing.
        let Ok(u) = reqwest::Url::parse(url.trim()) else {
            return false;
        };
        let p = u.path().trim();
        let is_home = p.is_empty() || p == "/" || p == "index.html" || p.ends_with("/index.html");
        if !is_home {
            return false;
        }
        // Homepages with tracking/query params are already handled elsewhere, but keep this
        // conservative and require the querystring to be empty here.
        u.query().unwrap_or("").trim().is_empty()
    }

    fn looks_like_js_challenge(status: u16, extracted_text: &str, title: Option<&str>) -> bool {
        // Generic “JS required / challenge page” detector, used for warnings + agentic bailout.
        if status == 403 {
            // Many challenge pages show up as 403.
            return true;
        }
        // For non-403 pages (including 429), avoid false positives where a real docs page *mentions* a challenge
        // string (e.g., “challenge-error-text”) as part of troubleshooting content.
        //
        // Use a small weighted signal score. Only return true when the page looks like an actual
        // interstitial, not when it merely mentions the terms.
        let t = extracted_text.to_ascii_lowercase();
        let mut score: i32 = 0;

        // Strong “interstitial” signals.
        if t.contains("just a moment") {
            score += 3;
        }
        if t.contains("checking your browser") {
            score += 3;
        }
        if t.contains("verify you are human") {
            score += 4;
        }
        if t.contains("attention required") {
            score += 3;
        }

        // Common challenge boilerplate.
        if t.contains("enable javascript") {
            score += 2;
        }
        if t.contains("enable cookies") || t.contains("cookies are required") {
            score += 2;
        }
        if t.contains("cloudflare ray id") || t.contains("cf-ray") {
            score += 2;
        }
        if t.contains("ddos protection") {
            score += 2;
        }
        if t.contains("access denied") && t.contains("cloudflare") {
            score += 2;
        }

        // Weak / ambiguous signals: count, but don’t trigger alone.
        if t.contains("challenge-error-text") {
            score += 1;
        }

        if let Some(tt) = title {
            let tl = tt.to_ascii_lowercase();
            if tl.contains("just a moment") {
                score += 3;
            }
            if tl.contains("attention required") {
                score += 3;
            }
            if tl.contains("verify you are human") {
                score += 4;
            }
        }

        score >= 4
    }

    fn looks_like_silent_throttle(status: u16, extracted_text: &str, title: Option<&str>) -> bool {
        // “Silent throttling” / interstitial detectors: some sites return HTTP 200 but show
        // a rate-limit / bot-check / “unusual traffic” page.
        //
        // Keep this conservative: we already have `blocked_by_js_challenge` for hard cases.
        if status >= 400 {
            return false;
        }
        let t = extracted_text.to_ascii_lowercase();
        let mut score: i32 = 0;
        for needle in [
            "unusual traffic",
            "automated queries",
            "we have detected unusual",
            "please verify you are a human",
            "verify you are human",
            "are you a robot",
            "captcha",
            "temporarily blocked",
            "access to this page has been denied",
            "request blocked",
            "too many requests",
            "rate limit",
            "try again later",
        ] {
            if t.contains(needle) {
                score += 2;
            }
        }
        if let Some(tt) = title {
            let tl = tt.to_ascii_lowercase();
            if tl.contains("access denied") || tl.contains("too many requests") {
                score += 2;
            }
        }
        score >= 4
    }

    fn structure_looks_like_ui_shell(
        structure: &webpipe_local::extract::ExtractedStructure,
    ) -> bool {
        // Generic heuristic: UI shells tend to be mostly short list items / nav labels.
        // We avoid host-specific rules; this is purely shape-based.
        let total = structure.blocks.len();
        if total == 0 {
            return false;
        }
        let listish = structure
            .blocks
            .iter()
            .filter(|b| b.kind == "list_item")
            .count();
        let short_blocks = structure
            .blocks
            .iter()
            .filter(|b| b.text.chars().count() <= 40)
            .count();
        let list_ratio = (listish as f32) / (total as f32);
        let short_ratio = (short_blocks as f32) / (total as f32);
        if (list_ratio >= 0.60 && short_ratio >= 0.60) || (list_ratio >= 0.85) {
            return true;
        }

        // UI shells also frequently contain auth/consent words even when list items are longer.
        if list_ratio >= 0.60 {
            let sl = structure.structure_text.to_ascii_lowercase();
            for needle in [
                "sign up", "log in", "login", "cookie", "consent", "privacy", "terms",
            ] {
                if sl.contains(needle) {
                    return true;
                }
            }
        }

        false
    }

    fn extracted_text_looks_like_nav_shell(text: &str) -> bool {
        // Secondary guard for `main_content_low_signal`: avoid false positives where the
        // page structure is nav-heavy but the extracted text is actually substantive docs.
        //
        // Keep it cheap + deterministic (no tokenizers/ML).
        let t = text.trim();
        if t.is_empty() {
            return true;
        }
        let lc = t.to_ascii_lowercase();
        let mut chrome_hits = 0usize;
        for needle in [
            "skip to content",
            "search documentation",
            "search docs",
            "table of contents",
            "feedback",
            "menu",
        ] {
            if lc.contains(needle) {
                chrome_hits = chrome_hits.saturating_add(1);
            }
        }
        // Prefer precision over recall: this signal gates fallbacks that can increase
        // cost (render/firecrawl) and should not trigger on “real docs” pages.
        if chrome_hits >= 2 {
            return true;
        }

        // Shape heuristic (weaker): only treat “many short lines” as nav-like if we also
        // saw at least one chrome token above.
        if chrome_hits >= 1 {
            let mut total = 0usize;
            let mut short = 0usize;
            for line in t
                .lines()
                .map(|l| l.trim())
                .filter(|l| !l.is_empty())
                .take(400)
            {
                total = total.saturating_add(1);
                if line.chars().count() <= 40 {
                    short = short.saturating_add(1);
                }
            }
            if total >= 40 && short.saturating_mul(4) >= total.saturating_mul(3) {
                // >= ~75% short lines.
                return true;
            }
        }

        false
    }

    fn warning_codes_from(warnings: &[&'static str]) -> Vec<&'static str> {
        warnings.iter().map(|w| normalize_warning_code(w)).collect()
    }

    fn is_http_status(msg: &str, code: u16) -> bool {
        msg.contains(&format!("HTTP {code}")) || msg.contains(&format!("{code} Too Many Requests"))
    }

    fn parse_searxng_arm(name: &str) -> Option<usize> {
        name.strip_prefix("searxng#")
            .and_then(|s| s.trim().parse::<usize>().ok())
    }

    #[derive(Debug, Clone, Copy)]
    struct SeedSpec {
        id: &'static str,
        url: &'static str,
        kind: &'static str,
        note: &'static str,
    }

    fn seed_registry() -> &'static [SeedSpec] {
        &[
            SeedSpec {
                id: "public-apis",
                url: "https://raw.githubusercontent.com/public-apis/public-apis/master/README.md",
                kind: "awesome_list",
                note: "Public APIs directory (huge; good for keyless URL seeds).",
            },
            SeedSpec {
                id: "awesome-selfhosted",
                url: "https://raw.githubusercontent.com/awesome-selfhosted/awesome-selfhosted/master/README.md",
                kind: "awesome_list",
                note: "Self-hosted services; many have public endpoints/docs.",
            },
            SeedSpec {
                id: "awesome",
                url: "https://raw.githubusercontent.com/sindresorhus/awesome/master/readme.md",
                kind: "awesome_list",
                note: "Meta-index of awesome lists (good pivot for more seeds).",
            },
            SeedSpec {
                id: "awesome-python",
                url: "https://raw.githubusercontent.com/vinta/awesome-python/master/README.md",
                kind: "awesome_list",
                note: "Python ecosystem index; good for tooling + libraries for adapters.",
            },
            SeedSpec {
                id: "free-for-dev",
                url: "https://raw.githubusercontent.com/ripienaar/free-for-dev/master/README.md",
                kind: "awesome_list",
                note: "Free tiers across many providers (useful for alternatives when rate-limited).",
            },
            SeedSpec {
                id: "awesome-public-datasets",
                url: "https://raw.githubusercontent.com/awesomedata/awesome-public-datasets/master/README.rst",
                kind: "awesome_list",
                note: "Public datasets index (good seed for offline-ish evidence gathering).",
            },
            SeedSpec {
                id: "awesome-go",
                url: "https://raw.githubusercontent.com/avelino/awesome-go/main/README.md",
                kind: "awesome_list",
                note: "Go ecosystem index (useful for tooling/services).",
            },
            SeedSpec {
                id: "awesome-sysadmin",
                url: "https://raw.githubusercontent.com/awesome-foss/awesome-sysadmin/master/README.md",
                kind: "awesome_list",
                note: "Sysadmin tooling index (good for deployable/self-hostable alternatives).",
            },
        ]
    }

    fn select_seed_urls(
        registry: &[SeedSpec],
        urls: Option<Vec<String>>,
        seed_ids: Option<Vec<String>>,
        max_urls: usize,
    ) -> (Vec<(String, String)>, Vec<String>) {
        if let Some(us) = urls {
            let picked = us
                .into_iter()
                .filter(|u| !u.trim().is_empty())
                .take(max_urls)
                .enumerate()
                .map(|(i, u)| (format!("custom#{i}"), u))
                .collect::<Vec<_>>();
            return (picked, Vec::new());
        }

        let mut unknown_seed_ids: Vec<String> = Vec::new();

        let chosen: Vec<SeedSpec> = if let Some(ids) = seed_ids {
            let mut out: Vec<SeedSpec> = Vec::new();
            let mut seen = std::collections::BTreeSet::<String>::new();
            for id0 in ids {
                let id = id0.trim().to_string();
                if id.is_empty() {
                    continue;
                }
                if !seen.insert(id.clone()) {
                    continue;
                }
                if let Some(s) = registry.iter().find(|s| s.id == id) {
                    out.push(*s);
                } else {
                    unknown_seed_ids.push(id);
                }
            }
            out
        } else {
            registry.to_vec()
        };

        let picked = chosen
            .into_iter()
            .take(max_urls)
            .map(|s| (s.id.to_string(), s.url.to_string()))
            .collect::<Vec<_>>();

        (picked, unknown_seed_ids)
    }

    fn search_failed_hint(provider: &str, msg: &str, default_hint: &str) -> String {
        if is_http_status(msg, 429) {
            return format!(
                "{provider} is rate-limiting (HTTP 429). Retry later; reduce max_results; use urls=[...] offline mode; or switch providers."
            );
        }
        default_hint.to_string()
    }

    fn tool_result(payload: serde_json::Value) -> CallToolResult {
        // Always attach structured content for machine consumers, and include a text fallback
        // for older clients/tests that only read `content[0].text`.
        //
        // Also set MCP-level `isError` when our envelope says ok=false, so clients that
        // rely on MCP error signaling behave correctly.
        let ok = payload.get("ok").and_then(|v| v.as_bool()).unwrap_or(true);
        let mut r = if ok {
            CallToolResult::structured(payload.clone())
        } else {
            CallToolResult::structured_error(payload.clone())
        };
        r.content = vec![Content::text(payload.to_string())];
        r
    }

    fn tool_result_markdown_with_json(
        payload: serde_json::Value,
        markdown: String,
    ) -> CallToolResult {
        // Cursor (and similar UIs) display the `content` text and often render Markdown.
        // Keep the canonical machine payload in `structured_content`.
        //
        // Optional debug knob: include the full JSON payload as a second text item when
        // WEBPIPE_MCP_INCLUDE_JSON_TEXT=true (useful for copy/paste without digging into
        // structured_content tooling).
        let ok = payload.get("ok").and_then(|v| v.as_bool()).unwrap_or(true);
        let mut r = if ok {
            CallToolResult::structured(payload.clone())
        } else {
            CallToolResult::structured_error(payload.clone())
        };
        let mut content = vec![Content::text(markdown)];
        let include_json_text = std::env::var("WEBPIPE_MCP_INCLUDE_JSON_TEXT")
            .ok()
            .is_some_and(|v| {
                let s = v.trim();
                s == "1" || s.eq_ignore_ascii_case("true") || s.eq_ignore_ascii_case("yes")
            });
        if include_json_text {
            content.push(Content::text(payload.to_string()));
        }
        r.content = content;
        r
    }

    fn web_deep_research_markdown(payload: &serde_json::Value) -> String {
        let query = payload
            .get("query")
            .and_then(|v| v.as_str())
            .unwrap_or("")
            .trim();
        let ok = payload.get("ok").and_then(|v| v.as_bool()).unwrap_or(true);
        let kind = payload.get("kind").and_then(|v| v.as_str()).unwrap_or("");
        let answer = payload
            .get("answer")
            .and_then(|a| a.get("text"))
            .and_then(|v| v.as_str())
            .unwrap_or("")
            .trim();
        let truncated = payload
            .get("answer")
            .and_then(|a| a.get("truncated"))
            .and_then(|v| v.as_bool())
            .unwrap_or(false);

        let mut md = String::new();
        if !query.is_empty() {
            md.push_str("## Query\n\n");
            md.push_str(query);
            md.push_str("\n\n");
        }

        if !ok {
            md.push_str("## Error\n\n");
            let code = payload
                .get("error")
                .and_then(|e| e.get("code"))
                .and_then(|v| v.as_str())
                .unwrap_or("unknown_error");
            let msg = payload
                .get("error")
                .and_then(|e| e.get("message"))
                .and_then(|v| v.as_str())
                .unwrap_or("Unknown error.");
            let hint = payload
                .get("error")
                .and_then(|e| e.get("hint"))
                .and_then(|v| v.as_str())
                .unwrap_or("")
                .trim();

            md.push_str("- **code**: `");
            md.push_str(code);
            md.push_str("`\n");
            md.push_str("- **message**: ");
            md.push_str(msg);
            md.push('\n');
            if !hint.is_empty() {
                md.push_str("- **hint**: ");
                md.push_str(hint);
                md.push('\n');
            }
            md.push('\n');
        }

        // Stable, bounded "meta" that is useful in Cursor but not too noisy.
        if let Some(req) = payload.get("request").and_then(|v| v.as_object()) {
            md.push_str("## Request\n\n");
            // Keep ordering stable and avoid dumping huge nested objects.
            let show_str = |k: &str| req.get(k).and_then(|v| v.as_str()).unwrap_or("");
            let show_u64 = |k: &str| req.get(k).and_then(|v| v.as_u64());
            let show_bool = |k: &str| req.get(k).and_then(|v| v.as_bool());

            let provider = show_str("provider");
            if !provider.is_empty() {
                md.push_str("- **provider**: `");
                md.push_str(provider);
                md.push_str("`\n");
            }
            let fetch_backend = show_str("fetch_backend");
            if !fetch_backend.is_empty() {
                md.push_str("- **fetch_backend**: `");
                md.push_str(fetch_backend);
                md.push_str("`\n");
            }
            if let Some(nn) = show_bool("no_network") {
                md.push_str("- **no_network**: ");
                md.push_str(if nn { "true" } else { "false" });
                md.push('\n');
            }
            if let Some(ms) = show_u64("timeout_ms") {
                md.push_str("- **timeout_ms**: ");
                md.push_str(&ms.to_string());
                md.push('\n');
            }
            if let Some(n) = show_u64("max_results") {
                md.push_str("- **max_results**: ");
                md.push_str(&n.to_string());
                md.push('\n');
            }
            if let Some(n) = show_u64("max_urls") {
                md.push_str("- **max_urls**: ");
                md.push_str(&n.to_string());
                md.push('\n');
            }
            if let Some(n) = show_u64("top_chunks") {
                md.push_str("- **top_chunks**: ");
                md.push_str(&n.to_string());
                md.push('\n');
            }
            if let Some(n) = show_u64("max_chunk_chars") {
                md.push_str("- **max_chunk_chars**: ");
                md.push_str(&n.to_string());
                md.push('\n');
            }
            if let Some(n) = show_u64("max_chars") {
                md.push_str("- **max_chars**: ");
                md.push_str(&n.to_string());
                md.push('\n');
            }
            if let Some(sm) = req.get("search_mode").and_then(|v| v.as_str()) {
                if !sm.trim().is_empty() {
                    md.push_str("- **search_mode**: `");
                    md.push_str(sm.trim());
                    md.push_str("`\n");
                }
            }
            if let Some(re) = req.get("reasoning_effort").and_then(|v| v.as_str()) {
                if !re.trim().is_empty() {
                    md.push_str("- **reasoning_effort**: `");
                    md.push_str(re.trim());
                    md.push_str("`\n");
                }
            }
            md.push('\n');
        } else if !kind.is_empty() {
            // Defensive: if request isn't present, at least show kind for debugging.
            md.push_str("## Request\n\n");
            md.push_str("- **kind**: `");
            md.push_str(kind);
            md.push_str("`\n\n");
        }

        if let Some(usage) = payload.get("usage") {
            if usage.is_object() {
                md.push_str("## Usage\n\n");
                if let Some(t) = usage.get("total_tokens").and_then(|v| v.as_u64()) {
                    md.push_str("- **total_tokens**: ");
                    md.push_str(&t.to_string());
                    md.push('\n');
                }
                if let Some(t) = usage.get("prompt_tokens").and_then(|v| v.as_u64()) {
                    md.push_str("- **prompt_tokens**: ");
                    md.push_str(&t.to_string());
                    md.push('\n');
                }
                if let Some(t) = usage.get("completion_tokens").and_then(|v| v.as_u64()) {
                    md.push_str("- **completion_tokens**: ");
                    md.push_str(&t.to_string());
                    md.push('\n');
                }
                md.push('\n');
            }
        }

        if let Some(ms) = payload.get("elapsed_ms").and_then(|v| v.as_u64()) {
            md.push_str("## Timing\n\n");
            md.push_str("- **elapsed_ms**: ");
            md.push_str(&ms.to_string());
            md.push('\n');
            md.push('\n');
        }

        md.push_str("## Answer\n\n");
        if answer.is_empty() {
            md.push_str("_No answer text was produced._\n");
        } else {
            md.push_str(answer);
            md.push('\n');
        }
        if truncated {
            md.push_str("\n_Note: answer was truncated to stay bounded._\n");
        }

        if let Some(ws) = payload.get("warnings").and_then(|v| v.as_array()) {
            if !ws.is_empty() {
                md.push_str("\n## Warnings\n\n");
                for w in ws.iter().filter_map(|v| v.as_str()) {
                    md.push_str("- ");
                    md.push_str(w);
                    md.push('\n');
                }
            }
        }

        // Evidence summary (bounded).
        if let Some(ev) = payload.get("evidence").and_then(|v| v.as_object()) {
            let results_n = ev
                .get("results")
                .and_then(|v| v.as_array())
                .map(|a| a.len())
                .unwrap_or(0);
            let top_n = ev
                .get("top_chunks")
                .and_then(|v| v.as_array())
                .map(|a| a.len())
                .unwrap_or(0);
            if results_n > 0 || top_n > 0 {
                md.push_str("\n## Evidence (summary)\n\n");
                md.push_str("- **results**: ");
                md.push_str(&results_n.to_string());
                md.push('\n');
                md.push_str("- **top_chunks**: ");
                md.push_str(&top_n.to_string());
                md.push('\n');

                // Optional snippet mode: show a few top chunk excerpts.
                let include_snips = std::env::var("WEBPIPE_MCP_MARKDOWN_EVIDENCE_SNIPPETS")
                    .ok()
                    .is_some_and(|v| {
                        let s = v.trim();
                        s == "1" || s.eq_ignore_ascii_case("true") || s.eq_ignore_ascii_case("yes")
                    });
                if include_snips && top_n > 0 {
                    md.push_str("\n### Top chunks (excerpts)\n\n");
                    if let Some(arr) = ev.get("top_chunks").and_then(|v| v.as_array()) {
                        for c in arr.iter().take(3) {
                            let url = c.get("url").and_then(|v| v.as_str()).unwrap_or("");
                            let text0 = c.get("text").and_then(|v| v.as_str()).unwrap_or("");
                            // Keep excerpts bounded.
                            let mut excerpt = String::new();
                            for ch in text0.chars().take(300) {
                                excerpt.push(ch);
                            }
                            if !excerpt.is_empty() {
                                md.push_str("- ");
                                if !url.trim().is_empty() {
                                    md.push_str("**");
                                    md.push_str(url.trim());
                                    md.push_str("**: ");
                                }
                                md.push_str(&excerpt);
                                if text0.chars().count() > 300 {
                                    md.push('…');
                                }
                                md.push('\n');
                            }
                        }
                    }
                }
            }
        }

        if let Some(cs) = payload
            .get("answer")
            .and_then(|a| a.get("citations"))
            .and_then(|v| v.as_array())
        {
            if !cs.is_empty() {
                md.push_str("\n## Citations\n\n");
                for c in cs.iter() {
                    if let Some(u) = c.as_str() {
                        md.push_str("- ");
                        md.push_str(u);
                        md.push('\n');
                    }
                }
            }
        }

        md
    }

    fn web_search_extract_markdown(payload: &serde_json::Value) -> String {
        let ok = payload.get("ok").and_then(|v| v.as_bool()).unwrap_or(true);
        let query = payload
            .get("query")
            .and_then(|v| v.as_str())
            .unwrap_or("")
            .trim();
        let mode = payload
            .get("mode")
            .and_then(|v| v.as_str())
            .unwrap_or("")
            .trim();
        let provider = payload
            .get("provider")
            .and_then(|v| v.as_str())
            .unwrap_or("")
            .trim();
        let backend_provider = payload
            .get("backend_provider")
            .and_then(|v| v.as_str())
            .unwrap_or("")
            .trim();
        let req = payload.get("request").unwrap_or(&serde_json::Value::Null);
        let fetch_backend = req
            .get("fetch_backend")
            .and_then(|v| v.as_str())
            .unwrap_or("")
            .trim();
        let auto_mode = req
            .get("auto_mode")
            .and_then(|v| v.as_str())
            .unwrap_or("")
            .trim();
        let selection_mode = req
            .get("selection_mode")
            .and_then(|v| v.as_str())
            .unwrap_or("")
            .trim();
        let url_selection_mode = req
            .get("url_selection_mode")
            .and_then(|v| v.as_str())
            .unwrap_or("")
            .trim();

        let url_count_in = payload
            .get("url_count_in")
            .and_then(|v| v.as_u64())
            .unwrap_or(0);
        let url_count_used = payload
            .get("url_count_used")
            .and_then(|v| v.as_u64())
            .unwrap_or(0);
        let results_count = payload
            .get("results")
            .and_then(|v| v.as_array())
            .map(|a| a.len())
            .unwrap_or(0);
        let top_chunks_count = payload
            .get("top_chunks")
            .and_then(|v| v.as_array())
            .map(|a| a.len())
            .unwrap_or(0);

        // Precompute stable source URL order so chunks can cite sources as [1], [2], ...
        // (Even when we print chunks before the Sources section.)
        let mut source_urls: Vec<String> = Vec::new();
        let mut source_final_urls: std::collections::BTreeMap<String, String> =
            std::collections::BTreeMap::new();
        if let Some(arr) = payload.get("results").and_then(|v| v.as_array()) {
            for r in arr.iter() {
                let url = r.get("url").and_then(|v| v.as_str()).unwrap_or("").trim();
                if !url.is_empty() {
                    source_urls.push(url.to_string());
                    let final_url = r
                        .get("final_url")
                        .and_then(|v| v.as_str())
                        .unwrap_or("")
                        .trim();
                    if !final_url.is_empty() && final_url != url {
                        source_final_urls.insert(url.to_string(), final_url.to_string());
                    }
                }
            }
        }

        let mut md = String::new();
        let chunk_excerpts = matches!(
            std::env::var("WEBPIPE_MCP_MARKDOWN_CHUNK_EXCERPTS")
                .unwrap_or_else(|_| "0".to_string())
                .trim()
                .to_ascii_lowercase()
                .as_str(),
            "1" | "true" | "yes" | "on"
        );

        fn norm_one_line(s: &str) -> String {
            s.split_whitespace().collect::<Vec<_>>().join(" ")
        }

        // Chunks first: this is what users want to see immediately.
        // Keep it compact; full details remain available in structured_content.
        let mut top_chunks_already_printed = false;
        if let Some(arr) = payload.get("top_chunks").and_then(|v| v.as_array()) {
            if !arr.is_empty() {
                top_chunks_already_printed = true;
                md.push_str("## Top chunks\n\n");
                for (i, c) in arr.iter().enumerate() {
                    let url = c.get("url").and_then(|v| v.as_str()).unwrap_or("").trim();
                    let score = c.get("score").and_then(|v| v.as_u64()).unwrap_or(0);
                    let text0 = c.get("text").and_then(|v| v.as_str()).unwrap_or("").trim();
                    if text0.is_empty() {
                        continue;
                    }

                    let mut src_i: Option<usize> = None;
                    if !url.is_empty() {
                        for (j, u) in source_urls.iter().enumerate() {
                            if u == url {
                                src_i = Some(j + 1);
                                break;
                            }
                        }
                    }

                    let short = norm_one_line(text0).chars().take(260).collect::<String>();
                    let mut line = format!("- **Chunk {}**", i + 1);
                    if let Some(si) = src_i {
                        line.push_str(&format!(" (source [{si}])"));
                    }
                    if score != 0 {
                        line.push_str(&format!(" (score {score})"));
                    }
                    if !url.is_empty() {
                        line.push_str(&format!(": {url}"));
                        if let Some(fu) = source_final_urls.get(url) {
                            let fu = fu.trim();
                            if !fu.is_empty() {
                                line.push_str(" (→ ");
                                line.push_str(fu);
                                line.push(')');
                            }
                        }
                    }
                    line.push_str(" — ");
                    line.push_str(short.trim_end());
                    md.push_str(&line);
                    md.push('\n');

                    // Optional excerpts: keep bounded and readable.
                    if chunk_excerpts && i < 3 {
                        md.push_str("\n```text\n");
                        md.push_str(text0);
                        md.push_str("\n```\n\n");
                    }
                }
                md.push('\n');
            }
        }

        // Dense summary: one bullet line.
        md.push_str("## Summary\n\n");
        let mut parts: Vec<String> = Vec::new();
        parts.push(format!("**ok**: {}", if ok { "true" } else { "false" }));
        if !mode.is_empty() {
            parts.push(format!("**mode**: `{mode}`"));
        }
        if !provider.is_empty() && mode != "urls" {
            parts.push(format!("**provider**: `{provider}`"));
        }
        if !backend_provider.is_empty() {
            parts.push(format!("**backend_provider**: `{backend_provider}`"));
        }
        // When minimal_output stripped request/results, avoid "urls: 0→0; results: 0" when we have chunks.
        let minimal_summary = url_count_in == 0 && results_count == 0 && top_chunks_count > 0;
        if !minimal_summary {
            parts.push(format!("**urls**: {url_count_in}→{url_count_used}"));
            parts.push(format!("**results**: {results_count}"));
        }
        parts.push(format!("**chunks**: {top_chunks_count}"));
        if minimal_summary && top_chunks_count > 0 {
            let unique_urls: std::collections::BTreeSet<&str> = payload
                .get("top_chunks")
                .and_then(|v| v.as_array())
                .map(|arr| {
                    arr.iter()
                        .filter_map(|c| c.get("url").and_then(|v| v.as_str()))
                        .filter(|s| !s.is_empty())
                        .collect()
                })
                .unwrap_or_default();
            if !unique_urls.is_empty() {
                parts.push(format!("**sources**: {}", unique_urls.len()));
            }
        }
        if let Some(ms) = payload.get("elapsed_ms").and_then(|v| v.as_u64()) {
            parts.push(format!("**elapsed_ms**: {ms}"));
        }
        if let Some(codes) = payload.get("warning_codes").and_then(|v| v.as_array()) {
            let codes_s: Vec<&str> = codes.iter().filter_map(|v| v.as_str()).collect();
            if !codes_s.is_empty() {
                let shown = codes_s
                    .iter()
                    .take(4)
                    .map(|c| format!("`{}`", c))
                    .collect::<Vec<_>>()
                    .join(", ");
                parts.push(format!("**warn**: {shown}"));
            }
        }
        md.push_str("- ");
        md.push_str(&parts.join("; "));
        md.push_str("\n\n");

        // Request knobs (stable ordering; lower priority than summary).
        md.push_str("## Request\n\n");
        if !mode.is_empty() {
            md.push_str("- **mode**: `");
            md.push_str(mode);
            md.push_str("`\n");
        }
        // In urls-mode, showing a “provider” is confusing (no search provider is used).
        if !provider.is_empty() && mode != "urls" {
            md.push_str("- **provider**: `");
            md.push_str(provider);
            md.push_str("`\n");
        }
        if !auto_mode.is_empty() {
            md.push_str("- **auto_mode**: `");
            md.push_str(auto_mode);
            md.push_str("`\n");
        }
        if !selection_mode.is_empty() {
            md.push_str("- **selection_mode**: `");
            md.push_str(selection_mode);
            md.push_str("`\n");
        }
        if !url_selection_mode.is_empty() {
            md.push_str("- **url_selection_mode**: `");
            md.push_str(url_selection_mode);
            md.push_str("`\n");
        }
        if !fetch_backend.is_empty() {
            md.push_str("- **fetch_backend**: `");
            md.push_str(fetch_backend);
            md.push_str("`\n");
        }
        if let Some(nn) = req.get("no_network").and_then(|v| v.as_bool()) {
            md.push_str("- **no_network**: ");
            md.push_str(if nn { "true" } else { "false" });
            md.push('\n');
        }
        for (k, label) in [
            ("max_results", "max_results"),
            ("max_urls", "max_urls"),
            ("max_parallel_urls", "max_parallel_urls"),
            ("max_chars", "max_chars"),
            ("top_chunks", "top_chunks"),
            ("max_chunk_chars", "max_chunk_chars"),
        ] {
            if let Some(n) = req.get(k).and_then(|v| v.as_u64()) {
                md.push_str("- **");
                md.push_str(label);
                md.push_str("**: ");
                md.push_str(&n.to_string());
                md.push('\n');
            }
        }
        md.push('\n');

        if !query.is_empty() {
            md.push_str("## Query\n\n");
            md.push_str(query);
            md.push_str("\n\n");
        }

        if !ok {
            md.push_str("## Error\n\n");
            let code = payload
                .get("error")
                .and_then(|e| e.get("code"))
                .and_then(|v| v.as_str())
                .unwrap_or("unknown_error");
            let msg = payload
                .get("error")
                .and_then(|e| e.get("message"))
                .and_then(|v| v.as_str())
                .unwrap_or("Unknown error.");
            let hint = payload
                .get("error")
                .and_then(|e| e.get("hint"))
                .and_then(|v| v.as_str())
                .unwrap_or("")
                .trim();
            md.push_str("- **code**: `");
            md.push_str(code);
            md.push_str("`\n");
            md.push_str("- **message**: ");
            md.push_str(msg);
            md.push('\n');
            if !hint.is_empty() {
                md.push_str("- **hint**: ");
                md.push_str(hint);
                md.push('\n');
            }
            md.push('\n');
        }

        // Agentic summary (bounded).
        if let Some(a) = payload.get("agentic").and_then(|v| v.as_object()) {
            if a.get("enabled").and_then(|v| v.as_bool()) == Some(true) {
                md.push_str("## Agentic\n\n");
                for k in [
                    "search_rounds",
                    "stuck_events",
                    "frontier_added_total",
                    "frontier_len_final",
                    "urls_fetched",
                ] {
                    if let Some(n) = a.get(k).and_then(|v| v.as_u64()) {
                        md.push_str("- **");
                        md.push_str(k);
                        md.push_str("**: ");
                        md.push_str(&n.to_string());
                        md.push('\n');
                    }
                }
                md.push('\n');
            }
        }

        // Warnings summary (bounded).
        if let Some(ws) = payload.get("warnings").and_then(|v| v.as_array()) {
            if !ws.is_empty() {
                md.push_str("## Warnings\n\n");
                for w in ws.iter().filter_map(|v| v.as_str()).take(12) {
                    md.push_str("- ");
                    md.push_str(w);
                    md.push('\n');
                }
                md.push('\n');
            }
        }
        if let Some(codes) = payload.get("warning_codes").and_then(|v| v.as_array()) {
            let codes_s: Vec<&str> = codes.iter().filter_map(|v| v.as_str()).collect();
            if !codes_s.is_empty() {
                md.push_str("### Warning codes\n\n");
                for c in codes_s.iter().take(12) {
                    md.push_str("- `");
                    md.push_str(c);
                    md.push_str("`\n");
                }
                md.push('\n');
            }
        }
        if let Some(hints) = payload.get("warning_hints").and_then(|v| v.as_object()) {
            if !hints.is_empty() {
                md.push_str("### Warning hints\n\n");
                let mut ks: Vec<&String> = hints.keys().collect();
                ks.sort();
                for k in ks.into_iter().take(12) {
                    let hint = hints.get(k).and_then(|v| v.as_str()).unwrap_or("").trim();
                    if hint.is_empty() {
                        continue;
                    }
                    md.push_str("- `");
                    md.push_str(k);
                    md.push_str("`: ");
                    md.push_str(hint);
                    md.push('\n');
                }
                md.push('\n');
            }
        }

        // Sources (bounded by max_urls <= 10). Use stable indices so chunks can cite sources as [1], [2], ...
        let include_text = req
            .get("include_text")
            .and_then(|v| v.as_bool())
            .unwrap_or(false);
        let include_structure = req
            .get("include_structure")
            .and_then(|v| v.as_bool())
            .unwrap_or(false);
        if let Some(arr) = payload.get("results").and_then(|v| v.as_array()) {
            if !arr.is_empty() {
                md.push_str("## Sources\n\n");
                for (i, r) in arr.iter().enumerate() {
                    let url = r.get("url").and_then(|v| v.as_str()).unwrap_or("").trim();
                    if url.is_empty() {
                        continue;
                    }
                    let final_url = r
                        .get("final_url")
                        .and_then(|v| v.as_str())
                        .unwrap_or("")
                        .trim();
                    // Prefer a human title when available (only present when include_structure=true).
                    let title = r
                        .pointer("/extract/structure/title")
                        .and_then(|v| v.as_str())
                        .unwrap_or("")
                        .trim();
                    let status = r.get("status").and_then(|v| v.as_u64()).unwrap_or(0);
                    let fb = r
                        .get("fetch_backend")
                        .and_then(|v| v.as_str())
                        .unwrap_or("")
                        .trim();
                    let engine = r
                        .pointer("/extract/engine")
                        .and_then(|v| v.as_str())
                        .unwrap_or("")
                        .trim();
                    let text_chars = r
                        .pointer("/extract/text_chars")
                        .and_then(|v| v.as_u64())
                        .unwrap_or(0);
                    let warn_codes: Vec<&str> = r
                        .get("warning_codes")
                        .and_then(|v| v.as_array())
                        .map(|a| a.iter().filter_map(|x| x.as_str()).take(8).collect())
                        .unwrap_or_default();

                    md.push_str(&format!("### [{}] ", i + 1));
                    if !title.is_empty() {
                        md.push_str(title);
                    } else {
                        md.push_str(url);
                    }
                    md.push('\n');
                    md.push('\n');
                    md.push_str("- url: ");
                    md.push_str(url);
                    md.push('\n');
                    if !final_url.is_empty() && final_url != url {
                        md.push_str("- final_url: ");
                        md.push_str(final_url);
                        md.push('\n');
                    }
                    if status != 0 {
                        md.push_str("- status: ");
                        md.push_str(&status.to_string());
                        md.push('\n');
                    }
                    if !fb.is_empty() {
                        md.push_str("- fetch_backend: `");
                        md.push_str(fb);
                        md.push_str("`\n");
                    }
                    if !engine.is_empty() {
                        md.push_str("- engine: `");
                        md.push_str(engine);
                        md.push_str("`\n");
                    }
                    if engine == "redirect" {
                        let target = r
                            .pointer("/extract/text")
                            .and_then(|v| v.as_str())
                            .unwrap_or("")
                            .trim();
                        if !target.is_empty() {
                            md.push_str("- redirect_target: ");
                            md.push_str(target);
                            md.push('\n');
                        }
                    }
                    if text_chars != 0 {
                        md.push_str("- text_chars: ");
                        md.push_str(&text_chars.to_string());
                        md.push('\n');
                    }
                    if !warn_codes.is_empty() {
                        md.push_str("- warning_codes: ");
                        for (j, c) in warn_codes.iter().enumerate() {
                            if j > 0 {
                                md.push_str(", ");
                            }
                            md.push('`');
                            md.push_str(c);
                            md.push('`');
                        }
                        md.push('\n');
                    }
                    md.push('\n');

                    if include_structure {
                        if let Some(outline) = r
                            .pointer("/extract/structure/outline")
                            .and_then(|v| v.as_array())
                        {
                            if !outline.is_empty() {
                                md.push_str("#### Outline\n\n");
                                for it in outline.iter().filter_map(|v| v.as_str()).take(12) {
                                    let t = it.trim();
                                    if t.is_empty() {
                                        continue;
                                    }
                                    md.push_str("- ");
                                    md.push_str(t);
                                    md.push('\n');
                                }
                                md.push('\n');
                            }
                        }
                    }

                    // Show text preview always; show full text when include_text=true.
                    let preview0 = r
                        .pointer("/extract/text_preview")
                        .and_then(|v| v.as_str())
                        .unwrap_or("")
                        .trim();
                    if !preview0.is_empty() {
                        md.push_str("#### Excerpt\n\n");
                        md.push_str("```text\n");
                        md.push_str(preview0.trim_end());
                        md.push_str("\n```\n\n");
                    }
                    if include_text {
                        let full = r
                            .pointer("/extract/text")
                            .and_then(|v| v.as_str())
                            .unwrap_or("")
                            .trim();
                        let blocked = warn_codes.iter().any(|&c| {
                            normalize_warning_code(c) == "blocked_by_js_challenge"
                                || normalize_warning_code(c) == "unsupported_content_no_text"
                        });
                        let tiny_hint =
                            engine == "html_hint" && text_chars > 0 && text_chars <= 200;
                        if !full.is_empty() && !blocked && !tiny_hint {
                            md.push_str("#### Text\n\n");
                            md.push_str("```text\n");
                            md.push_str(full);
                            md.push_str("\n```\n\n");
                        } else if blocked || tiny_hint {
                            md.push_str("#### Text\n\n");
                            md.push_str("- Omitted (blocked/low-signal). Use `fetch_backend=\"render\"` or a PDF/arXiv/PMLR mirror.\n\n");
                        }
                    }
                }
            }
        }

        // Top chunks were already emitted earlier (scan-first).
        let _ = top_chunks_already_printed;

        // Next steps (actionable, bounded).
        let mut all_codes = std::collections::BTreeSet::<String>::new();
        if let Some(codes) = payload.get("warning_codes").and_then(|v| v.as_array()) {
            for c in codes.iter().filter_map(|v| v.as_str()) {
                let cc = normalize_warning_code(c).to_string();
                if !cc.is_empty() {
                    all_codes.insert(cc);
                }
            }
        }
        if let Some(arr) = payload.get("results").and_then(|v| v.as_array()) {
            for r in arr {
                if let Some(codes) = r.get("warning_codes").and_then(|v| v.as_array()) {
                    for c in codes.iter().filter_map(|v| v.as_str()) {
                        let cc = normalize_warning_code(c).to_string();
                        if !cc.is_empty() {
                            all_codes.insert(cc);
                        }
                    }
                }
                if let Some(ws) = r.get("warnings").and_then(|v| v.as_array()) {
                    for w in ws.iter().filter_map(|v| v.as_str()) {
                        let cc = normalize_warning_code(w).to_string();
                        if !cc.is_empty() {
                            all_codes.insert(cc);
                        }
                    }
                }
            }
        }

        let fetch_backend = payload
            .pointer("/request/fetch_backend")
            .and_then(|v| v.as_str())
            .unwrap_or("local")
            .trim()
            .to_ascii_lowercase();

        #[derive(Clone)]
        struct NextStep {
            id: &'static str,
            text: String,
        }
        let mut next: Vec<NextStep> = Vec::new();

        let has_js_challenge = all_codes.contains("blocked_by_js_challenge");
        let has_low_signal = all_codes.contains("main_content_low_signal")
            || all_codes.contains("chunks_filtered_low_signal")
            || all_codes.contains("empty_extraction");
        let has_trunc = all_codes.contains("body_truncated_by_max_bytes");
        let has_trunc_retry = all_codes.contains("retried_due_to_truncation");
        let has_github_repo_readme = all_codes.contains("github_repo_rewritten_to_raw_readme");

        // Priority 1: challenge gating.
        if has_js_challenge {
            if fetch_backend == "render" {
                next.push(NextStep {
                    id: "challenge_rendered",
                    text: "This source looks challenge-gated even after rendering. Prefer a different URL/source (or complete the challenge in a real browser and use a non-gated mirror).".to_string(),
                });
            } else {
                next.push(NextStep {
                    id: "challenge_try_render",
                    text: "Try `fetch_backend=\"render\"` (or set `render_fallback_on_low_signal=true`) to get past JS-only shells/challenges; if it still looks gated, pick a different source URL.".to_string(),
                });
            }
        }

        // Priority 2: low-signal extraction (avoid repeating render advice if we already suggested it).
        if has_low_signal {
            if fetch_backend != "render" {
                if !has_js_challenge {
                    next.push(NextStep {
                        id: "low_signal_try_render",
                        text: "If excerpts are mostly navigation/UI, enable render fallback: `render_fallback_on_low_signal=true` (or force `fetch_backend=\"render\"` for these URLs).".to_string(),
                    });
                }
                next.push(NextStep {
                    id: "low_signal_deeplink",
                    text: "If results are nav-heavy, pass `urls=[...]` with a deep link (and optionally set `agentic=false`) to force extraction from the intended page.".to_string(),
                });
            } else {
                next.push(NextStep {
                    id: "low_signal_deeplink_render",
                    text: "If the page renders but still extracts poorly, pass a more specific `urls=[...]` (deep link) or try a different domain/source for the same topic.".to_string(),
                });
            }
        }

        // Priority 3: fetch truncation.
        if has_trunc {
            next.push(NextStep {
                id: "trunc_increase_max_bytes",
                text: if has_trunc_retry {
                    "Fetch was still truncated after a retry. Increase `max_bytes` (and/or `truncation_retry_max_bytes`) or use a smaller/deeper URL."
                        .to_string()
                } else {
                    "Increase `max_bytes` (or set `retry_on_truncation=true`) to avoid truncating the fetched body before extraction."
                        .to_string()
                },
            });
        }

        // Priority 4: repo/docs workflows (reduce “maybe try…” ambiguity).
        if has_github_repo_readme {
            let qlc = query.to_ascii_lowercase();
            let apiish = qlc.contains("api")
                || qlc.contains("endpoint")
                || qlc.contains("method")
                || qlc.contains("function")
                || qlc.contains("docs")
                || qlc.contains("documentation");
            if apiish || query.is_empty() {
                next.push(NextStep {
                    id: "github_repo_try_repo_ingest",
                    text: "If you need more than README (API surface / examples), use `repo_ingest` (bounded GitHub API fetch) or pass urls=[...] that point to specific `github.com/.../blob/...` docs/code pages."
                        .to_string(),
                });
            }
        }

        // Priority 5: search hygiene when results skew toward forums (e.g. Reddit).
        if mode == "search" && !query.is_empty() {
            let mut forum_like = false;
            for u in source_urls
                .iter()
                .map(|s| s.as_str())
                .chain(source_final_urls.values().map(|s| s.as_str()))
            {
                let lc = u.to_ascii_lowercase();
                if lc.contains("reddit.com")
                    || lc.contains("news.ycombinator.com")
                    || lc.contains("stackoverflow.com")
                    || lc.contains("stackexchange.com")
                {
                    forum_like = true;
                    break;
                }
            }
            if forum_like {
                let qlc = query.to_ascii_lowercase();
                let rustish = qlc.contains("rust")
                    || qlc.contains("cargo")
                    || qlc.contains("crate")
                    || qlc.contains("crates.io")
                    || qlc.contains("docs.rs");
                next.push(NextStep {
                    id: "search_restrict_domains",
                    text: if rustish {
                        "Search results include forum-like domains. For Rust crates, restrict domains (e.g. `domains_allow=[\"docs.rs\",\"crates.io\",\"github.com\"]`) or use a query like `site:docs.rs <crate> OR site:crates.io <crate>`."
                            .to_string()
                    } else {
                        "Search results include forum-like domains. Restrict domains with `domains_allow`/`domains_deny` (or pass urls=[...]) to force higher-signal docs/articles."
                            .to_string()
                    },
                });
            }
        }

        // Keep this short; only emit when it adds value.
        if !next.is_empty() {
            md.push_str("## Next\n\n");
            let mut seen = std::collections::BTreeSet::<&'static str>::new();
            let mut emitted = 0usize;
            for s in next {
                if emitted >= 3 {
                    break;
                }
                if !seen.insert(s.id) {
                    continue;
                }
                md.push_str("- ");
                md.push_str(&s.text);
                md.push('\n');
                emitted += 1;
            }
            md.push('\n');
        }

        md.push_str("## Notes\n\n");
        md.push_str("- The Markdown above is a summary view. The canonical JSON payload is always in `structured_content`.\n");
        md.push_str("- To include full extracted page text per URL, set `include_text=true` (bounded by `max_chars`).\n\n");

        md
    }

    fn paper_search_markdown(payload: &serde_json::Value) -> String {
        let query = payload
            .get("query")
            .and_then(|v| v.as_str())
            .unwrap_or("")
            .trim();
        let ok = payload.get("ok").and_then(|v| v.as_bool()).unwrap_or(true);
        let papers_n = payload
            .get("papers")
            .and_then(|v| v.as_array())
            .map(|a| a.len())
            .unwrap_or(0);
        let warnings_n = payload
            .get("warnings")
            .and_then(|v| v.as_array())
            .map(|a| {
                a.iter()
                    .filter_map(|v| v.as_str())
                    .collect::<std::collections::BTreeSet<_>>()
                    .len()
            })
            .unwrap_or(0);

        let mut md = String::new();
        md.push_str("## Summary\n\n");
        md.push_str("- **ok**: ");
        md.push_str(if ok { "true" } else { "false" });
        md.push('\n');
        md.push_str("- **papers**: ");
        md.push_str(&papers_n.to_string());
        md.push('\n');
        if warnings_n > 0 {
            md.push_str("- **warnings**: ");
            md.push_str(&warnings_n.to_string());
            md.push('\n');
        }
        if let Some(ms) = payload.get("elapsed_ms").and_then(|v| v.as_u64()) {
            md.push_str("- **elapsed_ms**: ");
            md.push_str(&ms.to_string());
            md.push('\n');
        }
        if warnings_n > 0 && papers_n == 0 {
            md.push_str("- **next**: try `arxiv_search` (often more reliable for arXiv-heavy topics) or adjust `backends` / credentials.\n");
        }
        md.push('\n');

        if !query.is_empty() {
            md.push_str("## Query\n\n");
            md.push_str(query);
            md.push_str("\n\n");
        }

        if !ok {
            md.push_str("## Error\n\n");
            let code = payload
                .get("error")
                .and_then(|e| e.get("code"))
                .and_then(|v| v.as_str())
                .unwrap_or("unknown_error");
            let msg = payload
                .get("error")
                .and_then(|e| e.get("message"))
                .and_then(|v| v.as_str())
                .unwrap_or("Unknown error.");
            let hint = payload
                .get("error")
                .and_then(|e| e.get("hint"))
                .and_then(|v| v.as_str())
                .unwrap_or("")
                .trim();
            md.push_str("- **code**: `");
            md.push_str(code);
            md.push_str("`\n");
            md.push_str("- **message**: ");
            md.push_str(msg);
            md.push('\n');
            if !hint.is_empty() {
                md.push_str("- **hint**: ");
                md.push_str(hint);
                md.push('\n');
            }
            md.push('\n');
        }

        // Small request summary (avoid dumping nested JSON).
        if let Some(req) = payload.get("request").and_then(|v| v.as_object()) {
            md.push_str("## Request\n\n");
            if let Some(bs) = req.get("backends").and_then(|v| v.as_array()) {
                let xs: Vec<String> = bs
                    .iter()
                    .filter_map(|v| v.as_str())
                    .map(|s| format!("`{}`", s.trim()))
                    .filter(|s| s != "``")
                    .collect();
                if !xs.is_empty() {
                    md.push_str("- **backends**: ");
                    md.push_str(&xs.join(", "));
                    md.push('\n');
                }
            }
            if let Some(ys) = req.get("years").and_then(|v| v.as_array()) {
                let xs: Vec<String> = ys
                    .iter()
                    .filter_map(|v| v.as_u64())
                    .map(|y| y.to_string())
                    .collect();
                if !xs.is_empty() {
                    md.push_str("- **years**: ");
                    md.push_str(&xs.join(", "));
                    md.push('\n');
                }
            }
            if let Some(n) = req.get("limit").and_then(|v| v.as_u64()) {
                md.push_str("- **limit**: ");
                md.push_str(&n.to_string());
                md.push('\n');
            }
            if let Some(ms) = req.get("timeout_ms").and_then(|v| v.as_u64()) {
                md.push_str("- **timeout_ms**: ");
                md.push_str(&ms.to_string());
                md.push('\n');
            }
            if let Some(b) = req.get("include_abstract").and_then(|v| v.as_bool()) {
                md.push_str("- **include_abstract**: ");
                md.push_str(if b { "true" } else { "false" });
                md.push('\n');
            }
            md.push('\n');
        }

        if let Some(ms) = payload.get("elapsed_ms").and_then(|v| v.as_u64()) {
            md.push_str("## Timing\n\n");
            md.push_str("- **elapsed_ms**: ");
            md.push_str(&ms.to_string());
            md.push_str("\n\n");
        }

        md.push_str("## Papers\n\n");
        let papers = payload.get("papers").and_then(|v| v.as_array());
        if papers.is_none_or(|a| a.is_empty()) {
            md.push_str("_No papers returned._\n");
        } else if let Some(ps) = papers {
            for (i, p) in ps.iter().take(20).enumerate() {
                let title = p.get("title").and_then(|v| v.as_str()).unwrap_or("").trim();
                if title.is_empty() {
                    continue;
                }
                let year = p
                    .get("year")
                    .and_then(|v| v.as_u64())
                    .map(|y| y.to_string());
                let venue = p.get("venue").and_then(|v| v.as_str()).unwrap_or("").trim();
                let cites = p
                    .get("citation_count")
                    .and_then(|v| v.as_u64())
                    .map(|n| n.to_string());
                let url = p.get("url").and_then(|v| v.as_str()).unwrap_or("").trim();
                let pdf = p
                    .get("pdf_url")
                    .and_then(|v| v.as_str())
                    .unwrap_or("")
                    .trim();
                let src = p
                    .get("source")
                    .and_then(|v| v.as_str())
                    .unwrap_or("")
                    .trim();

                md.push_str(&format!("{}. **{}**\n", i + 1, title));
                let mut meta = Vec::new();
                if let Some(y) = year {
                    meta.push(format!("year {y}"));
                }
                if !venue.is_empty() {
                    meta.push(venue.to_string());
                }
                if let Some(c) = cites {
                    meta.push(format!("{c} citations"));
                }
                if !src.is_empty() {
                    meta.push(format!("source `{src}`"));
                }
                if !meta.is_empty() {
                    md.push_str("   - ");
                    md.push_str(&meta.join(" · "));
                    md.push('\n');
                }
                if !url.is_empty() {
                    md.push_str("   - ");
                    md.push_str(url);
                    md.push('\n');
                }
                if !pdf.is_empty() && pdf != url {
                    md.push_str("   - PDF: ");
                    md.push_str(pdf);
                    md.push('\n');
                }
            }
        }

        if let Some(ws) = payload.get("warnings").and_then(|v| v.as_array()) {
            if !ws.is_empty() {
                md.push_str("\n## Warnings\n\n");
                let hints = payload.get("warning_hints").and_then(|v| v.as_object());
                for w in ws.iter().filter_map(|v| v.as_str()) {
                    md.push_str("- ");
                    md.push_str(w);
                    if let Some(ob) = hints.and_then(|h| h.get(w)).and_then(|v| v.as_str()) {
                        md.push_str(": ");
                        md.push_str(ob);
                    }
                    md.push('\n');
                }
            }
        }

        md
    }

    fn web_fetch_markdown(payload: &serde_json::Value) -> String {
        let ok = payload.get("ok").and_then(|v| v.as_bool()).unwrap_or(true);
        let url = payload
            .get("url")
            .and_then(|v| v.as_str())
            .unwrap_or("")
            .trim();
        let final_url = payload
            .get("final_url")
            .and_then(|v| v.as_str())
            .unwrap_or("")
            .trim();
        let status = payload.get("status").and_then(|v| v.as_u64());
        let content_type = payload
            .get("content_type")
            .and_then(|v| v.as_str())
            .unwrap_or("")
            .trim();
        let source = payload
            .get("source")
            .and_then(|v| v.as_str())
            .unwrap_or("")
            .trim();
        let fetch_backend = payload
            .get("fetch_backend")
            .and_then(|v| v.as_str())
            .unwrap_or("")
            .trim();
        let bytes = payload.get("bytes").and_then(|v| v.as_u64());
        let truncated = payload
            .get("truncated")
            .and_then(|v| v.as_bool())
            .unwrap_or(false);
        let text = payload
            .get("body_text")
            .and_then(|v| v.as_str())
            .unwrap_or("");
        let text_truncated = payload
            .get("text_truncated")
            .and_then(|v| v.as_bool())
            .unwrap_or(false);

        let mut md = String::new();
        md.push_str("## Summary\n\n");
        md.push_str("- **ok**: ");
        md.push_str(if ok { "true" } else { "false" });
        md.push('\n');
        if let Some(s) = status {
            md.push_str("- **status**: ");
            md.push_str(&s.to_string());
            md.push('\n');
        }
        if !content_type.is_empty() {
            md.push_str("- **content_type**: `");
            md.push_str(content_type);
            md.push_str("`\n");
        }
        if !fetch_backend.is_empty() {
            md.push_str("- **fetch_backend**: `");
            md.push_str(fetch_backend);
            md.push_str("`\n");
        }
        if !source.is_empty() {
            md.push_str("- **source**: `");
            md.push_str(source);
            md.push_str("`\n");
        }
        if let Some(b) = bytes {
            md.push_str("- **bytes**: ");
            md.push_str(&b.to_string());
            md.push('\n');
        }
        if truncated {
            md.push_str("- **truncated**: true\n");
        }
        if let Some(ms) = payload.get("elapsed_ms").and_then(|v| v.as_u64()) {
            md.push_str("- **elapsed_ms**: ");
            md.push_str(&ms.to_string());
            md.push('\n');
        }
        if !final_url.is_empty() && final_url != url {
            md.push_str("- **final_url**: ");
            md.push_str(final_url);
            md.push('\n');
        }
        md.push('\n');

        if !url.is_empty() {
            md.push_str("## URL\n\n");
            md.push_str(url);
            md.push_str("\n\n");
        }

        md.push_str("## Notes\n\n");
        md.push_str("- The Markdown below is a summary view. The canonical JSON payload is always in `structured_content`.\n");
        md.push_str("- If you want full page body text, set `include_text=true` (bounded by `max_text_chars`).\n\n");

        if !ok {
            md.push_str("## Error\n\n");
            let code = payload
                .get("error")
                .and_then(|e| e.get("code"))
                .and_then(|v| v.as_str())
                .unwrap_or("unknown_error");
            let msg = payload
                .get("error")
                .and_then(|e| e.get("message"))
                .and_then(|v| v.as_str())
                .unwrap_or("Unknown error.");
            let hint = payload
                .get("error")
                .and_then(|e| e.get("hint"))
                .and_then(|v| v.as_str())
                .unwrap_or("")
                .trim();
            md.push_str("- **code**: `");
            md.push_str(code);
            md.push_str("`\n");
            md.push_str("- **message**: ");
            md.push_str(msg);
            md.push('\n');
            if !hint.is_empty() {
                md.push_str("- **hint**: ");
                md.push_str(hint);
                md.push('\n');
            }
            md.push('\n');
        }

        if !text.is_empty() {
            // Keep the UI readable: we show at most ~1500 chars even if payload has more.
            let preview: String = text.chars().take(1500).collect();
            md.push_str("## Body (preview)\n\n```text\n");
            md.push_str(preview.trim_end());
            md.push_str("\n```\n");
            if text_truncated || text.chars().count() > 1500 {
                md.push_str(
                    "\n_Note: body preview is truncated; full text is in structured output._\n",
                );
            }
            md.push('\n');
        }

        if let Some(ws) = payload.get("warnings").and_then(|v| v.as_array()) {
            if !ws.is_empty() {
                md.push_str("## Warnings\n\n");
                for w in ws.iter().filter_map(|v| v.as_str()).take(12) {
                    md.push_str("- ");
                    md.push_str(w);
                    md.push('\n');
                }
                md.push('\n');
            }
        }
        if let Some(codes) = payload.get("warning_codes").and_then(|v| v.as_array()) {
            let codes_s: Vec<&str> = codes.iter().filter_map(|v| v.as_str()).collect();
            if !codes_s.is_empty() {
                md.push_str("### Warning codes\n\n");
                for c in codes_s.iter().take(12) {
                    md.push_str("- `");
                    md.push_str(c);
                    md.push_str("`\n");
                }
                md.push('\n');
            }
        }
        if let Some(hints) = payload.get("warning_hints").and_then(|v| v.as_object()) {
            if !hints.is_empty() {
                md.push_str("### Warning hints\n\n");
                let mut ks: Vec<&String> = hints.keys().collect();
                ks.sort();
                for k in ks.into_iter().take(12) {
                    let hint = hints.get(k).and_then(|v| v.as_str()).unwrap_or("").trim();
                    if hint.is_empty() {
                        continue;
                    }
                    md.push_str("- `");
                    md.push_str(k);
                    md.push_str("`: ");
                    md.push_str(hint);
                    md.push('\n');
                }
                md.push('\n');
            }
        }

        md
    }

    fn web_search_markdown(payload: &serde_json::Value) -> String {
        let ok = payload.get("ok").and_then(|v| v.as_bool()).unwrap_or(true);
        let query = payload
            .get("query")
            .and_then(|v| v.as_str())
            .unwrap_or("")
            .trim();
        let provider = payload
            .get("provider")
            .and_then(|v| v.as_str())
            .unwrap_or("")
            .trim();
        let backend_provider = payload
            .get("backend_provider")
            .and_then(|v| v.as_str())
            .unwrap_or("")
            .trim();
        let results = payload.get("results").and_then(|v| v.as_array());

        let mut md = String::new();
        if !query.is_empty() {
            md.push_str("## Query\n\n");
            md.push_str(query);
            md.push_str("\n\n");
        }
        md.push_str("## Summary\n\n");
        md.push_str("- **ok**: ");
        md.push_str(if ok { "true" } else { "false" });
        md.push('\n');
        if !provider.is_empty() {
            md.push_str("- **provider**: `");
            md.push_str(provider);
            md.push_str("`\n");
        }
        if !backend_provider.is_empty() {
            md.push_str("- **backend_provider**: `");
            md.push_str(backend_provider);
            md.push_str("`\n");
        }
        if let Some(ms) = payload.get("elapsed_ms").and_then(|v| v.as_u64()) {
            md.push_str("- **elapsed_ms**: ");
            md.push_str(&ms.to_string());
            md.push('\n');
        }
        md.push('\n');

        if !ok {
            md.push_str("## Error\n\n");
            let code = payload
                .get("error")
                .and_then(|e| e.get("code"))
                .and_then(|v| v.as_str())
                .unwrap_or("unknown_error");
            let msg = payload
                .get("error")
                .and_then(|e| e.get("message"))
                .and_then(|v| v.as_str())
                .unwrap_or("Unknown error.");
            let hint = payload
                .get("error")
                .and_then(|e| e.get("hint"))
                .and_then(|v| v.as_str())
                .unwrap_or("")
                .trim();
            md.push_str("- **code**: `");
            md.push_str(code);
            md.push_str("`\n");
            md.push_str("- **message**: ");
            md.push_str(msg);
            md.push('\n');
            if !hint.is_empty() {
                md.push_str("- **hint**: ");
                md.push_str(hint);
                md.push('\n');
            }
            md.push('\n');
        }

        md.push_str("## Results\n\n");
        if results.is_none_or(|a| a.is_empty()) {
            md.push_str("_No results returned._\n");
        } else if let Some(rs) = results {
            for (i, r) in rs.iter().take(10).enumerate() {
                let title = r.get("title").and_then(|v| v.as_str()).unwrap_or("").trim();
                let url = r.get("url").and_then(|v| v.as_str()).unwrap_or("").trim();
                let snippet = r
                    .get("snippet")
                    .and_then(|v| v.as_str())
                    .unwrap_or("")
                    .trim();
                if title.is_empty() && url.is_empty() {
                    continue;
                }
                md.push_str(&format!(
                    "{}. **{}**\n",
                    i + 1,
                    if title.is_empty() { url } else { title }
                ));
                if !url.is_empty() {
                    md.push_str("   - ");
                    md.push_str(url);
                    md.push('\n');
                }
                if !snippet.is_empty() {
                    let s: String = snippet.chars().take(240).collect();
                    md.push_str("   - ");
                    md.push_str(s.trim_end());
                    if snippet.chars().count() > 240 {
                        md.push('…');
                    }
                    md.push('\n');
                }
            }
        }
        md
    }

    fn web_extract_markdown(payload: &serde_json::Value) -> String {
        let ok = payload.get("ok").and_then(|v| v.as_bool()).unwrap_or(true);
        let query = payload
            .get("query")
            .and_then(|v| v.as_str())
            .unwrap_or("")
            .trim();
        let url = payload
            .get("url")
            .and_then(|v| v.as_str())
            .unwrap_or("")
            .trim();
        let final_url = payload
            .get("final_url")
            .and_then(|v| v.as_str())
            .unwrap_or("")
            .trim();
        let engine = payload
            .get("extract")
            .and_then(|e| e.get("engine"))
            .and_then(|v| v.as_str())
            .unwrap_or("")
            .trim();
        let text = payload
            .get("extract")
            .and_then(|e| e.get("text"))
            .and_then(|v| v.as_str())
            .unwrap_or("");
        let chunks = payload
            .get("extract")
            .and_then(|e| e.get("chunks"))
            .and_then(|v| v.as_array());
        let links_n = payload
            .get("extract")
            .and_then(|e| e.get("links"))
            .and_then(|v| v.as_array())
            .map(|a| a.len());
        let warn_codes: Vec<&str> = payload
            .get("warning_codes")
            .and_then(|v| v.as_array())
            .map(|a| a.iter().filter_map(|x| x.as_str()).take(8).collect())
            .unwrap_or_default();

        let mut md = String::new();
        md.push_str("## Summary\n\n");
        md.push_str("- **ok**: ");
        md.push_str(if ok { "true" } else { "false" });
        md.push('\n');
        if let Some(s) = payload.get("status").and_then(|v| v.as_u64()) {
            md.push_str("- **status**: ");
            md.push_str(&s.to_string());
            md.push('\n');
        }
        if !engine.is_empty() {
            md.push_str("- **engine**: `");
            md.push_str(engine);
            md.push_str("`\n");
        }
        if let Some(n) = payload
            .get("extract")
            .and_then(|e| e.get("text_chars"))
            .and_then(|v| v.as_u64())
        {
            md.push_str("- **text_chars**: ");
            md.push_str(&n.to_string());
            md.push('\n');
        }
        if let Some(n) = chunks.map(|a| a.len()) {
            md.push_str("- **chunks**: ");
            md.push_str(&n.to_string());
            md.push('\n');
        }
        if let Some(n) = links_n {
            md.push_str("- **links**: ");
            md.push_str(&n.to_string());
            md.push('\n');
        }
        if let Some(ms) = payload.get("elapsed_ms").and_then(|v| v.as_u64()) {
            md.push_str("- **elapsed_ms**: ");
            md.push_str(&ms.to_string());
            md.push('\n');
        }
        if !final_url.is_empty() && final_url != url {
            md.push_str("- **final_url**: ");
            md.push_str(final_url);
            md.push('\n');
        }
        if !warn_codes.is_empty() {
            md.push_str("- **warning_codes**: ");
            for (i, c) in warn_codes.iter().enumerate() {
                if i > 0 {
                    md.push_str(", ");
                }
                md.push('`');
                md.push_str(c);
                md.push('`');
            }
            md.push('\n');
        }
        md.push('\n');

        if !query.is_empty() {
            md.push_str("## Query\n\n");
            md.push_str(query);
            md.push_str("\n\n");
        }
        if !url.is_empty() {
            md.push_str("## URL\n\n");
            md.push_str(url);
            md.push_str("\n\n");
        }

        if !ok {
            md.push_str("## Error\n\n");
            let code = payload
                .get("error")
                .and_then(|e| e.get("code"))
                .and_then(|v| v.as_str())
                .unwrap_or("unknown_error");
            let msg = payload
                .get("error")
                .and_then(|e| e.get("message"))
                .and_then(|v| v.as_str())
                .unwrap_or("Unknown error.");
            let hint = payload
                .get("error")
                .and_then(|e| e.get("hint"))
                .and_then(|v| v.as_str())
                .unwrap_or("")
                .trim();
            md.push_str("- **code**: `");
            md.push_str(code);
            md.push_str("`\n");
            md.push_str("- **message**: ");
            md.push_str(msg);
            md.push('\n');
            if !hint.is_empty() {
                md.push_str("- **hint**: ");
                md.push_str(hint);
                md.push('\n');
            }
            md.push('\n');
        }

        if !text.is_empty() {
            // Full text (already bounded by max_chars upstream). Prefer full Markdown evidence over
            // a short preview: Cursor tool output is often read via the initial lines + expand.
            md.push_str("## Text\n\n```text\n");
            md.push_str(text.trim_end());
            md.push_str("\n```\n\n");
        } else if let Some(cs) = chunks {
            md.push_str("## Top chunks\n\n");
            for (i, c) in cs.iter().enumerate() {
                let sc = c.get("start_char").and_then(|v| v.as_u64()).unwrap_or(0);
                let ec = c.get("end_char").and_then(|v| v.as_u64()).unwrap_or(0);
                let score = c.get("score").and_then(|v| v.as_u64()).unwrap_or(0);
                let t = c.get("text").and_then(|v| v.as_str()).unwrap_or("").trim();
                if t.is_empty() {
                    continue;
                }
                md.push_str(&format!("### Chunk {}\n\n", i + 1));
                if score != 0 {
                    md.push_str("- score: ");
                    md.push_str(&score.to_string());
                    md.push('\n');
                }
                if sc != 0 || ec != 0 {
                    md.push_str("- span: ");
                    md.push_str(&format!("{sc}..{ec}"));
                    md.push('\n');
                }
                md.push('\n');
                md.push_str("```text\n");
                md.push_str(t);
                md.push_str("\n```\n\n");
            }
        }

        if let Some(ws) = payload.get("warnings").and_then(|v| v.as_array()) {
            if !ws.is_empty() {
                md.push_str("## Warnings\n\n");
                for w in ws.iter().filter_map(|v| v.as_str()).take(12) {
                    md.push_str("- ");
                    md.push_str(w);
                    md.push('\n');
                }
                md.push('\n');
            }
        }
        if let Some(codes) = payload.get("warning_codes").and_then(|v| v.as_array()) {
            let codes_s: Vec<&str> = codes.iter().filter_map(|v| v.as_str()).collect();
            if !codes_s.is_empty() {
                md.push_str("### Warning codes\n\n");
                for c in codes_s.iter().take(12) {
                    md.push_str("- `");
                    md.push_str(c);
                    md.push_str("`\n");
                }
                md.push('\n');
            }
        }
        if let Some(hints) = payload.get("warning_hints").and_then(|v| v.as_object()) {
            if !hints.is_empty() {
                md.push_str("### Warning hints\n\n");
                let mut ks: Vec<&String> = hints.keys().collect();
                ks.sort();
                for k in ks.into_iter().take(12) {
                    let hint = hints.get(k).and_then(|v| v.as_str()).unwrap_or("").trim();
                    if hint.is_empty() {
                        continue;
                    }
                    md.push_str("- `");
                    md.push_str(k);
                    md.push_str("`: ");
                    md.push_str(hint);
                    md.push('\n');
                }
                md.push('\n');
            }
        }
        md
    }

    fn web_seed_search_extract_markdown(payload: &serde_json::Value) -> String {
        let ok = payload.get("ok").and_then(|v| v.as_bool()).unwrap_or(true);
        let query = payload
            .get("query")
            .and_then(|v| v.as_str())
            .unwrap_or("")
            .trim();
        let merged = payload
            .get("results")
            .and_then(|r| r.get("merged_chunks"))
            .and_then(|v| v.as_array());

        let mut md = String::new();
        if !query.is_empty() {
            md.push_str("## Query\n\n");
            md.push_str(query);
            md.push_str("\n\n");
        }
        md.push_str("## Summary\n\n");
        md.push_str("- **ok**: ");
        md.push_str(if ok { "true" } else { "false" });
        md.push('\n');
        if let Some(n) = merged.map(|a| a.len()) {
            md.push_str("- **merged_chunks**: ");
            md.push_str(&n.to_string());
            md.push('\n');
        }
        if let Some(ms) = payload.get("elapsed_ms").and_then(|v| v.as_u64()) {
            md.push_str("- **elapsed_ms**: ");
            md.push_str(&ms.to_string());
            md.push('\n');
        }
        md.push('\n');

        if !ok {
            md.push_str("## Error\n\n");
            let code = payload
                .get("error")
                .and_then(|e| e.get("code"))
                .and_then(|v| v.as_str())
                .unwrap_or("unknown_error");
            let msg = payload
                .get("error")
                .and_then(|e| e.get("message"))
                .and_then(|v| v.as_str())
                .unwrap_or("Unknown error.");
            let hint = payload
                .get("error")
                .and_then(|e| e.get("hint"))
                .and_then(|v| v.as_str())
                .unwrap_or("")
                .trim();
            md.push_str("- **code**: `");
            md.push_str(code);
            md.push_str("`\n");
            md.push_str("- **message**: ");
            md.push_str(msg);
            md.push('\n');
            if !hint.is_empty() {
                md.push_str("- **hint**: ");
                md.push_str(hint);
                md.push('\n');
            }
            md.push('\n');
        }

        md.push_str("## Top merged chunks\n\n");
        if merged.is_none_or(|a| a.is_empty()) {
            md.push_str("_No chunks returned._\n");
        } else if let Some(cs) = merged {
            for (i, c) in cs.iter().take(8).enumerate() {
                let seed_id = c.get("seed_id").and_then(|v| v.as_str()).unwrap_or("");
                let score = c.get("score").and_then(|v| v.as_u64()).unwrap_or(0);
                let text = c.get("text").and_then(|v| v.as_str()).unwrap_or("").trim();
                if text.is_empty() {
                    continue;
                }
                let short: String = text.chars().take(260).collect();
                md.push_str(&format!(
                    "{}. **seed** `{}` (score {})\n",
                    i + 1,
                    seed_id,
                    score
                ));
                md.push_str("   - ");
                md.push_str(short.trim_end());
                if text.chars().count() > 260 {
                    md.push('…');
                }
                md.push('\n');
            }
        }
        md.push('\n');

        if let Some(ws) = payload.get("warnings").and_then(|v| v.as_array()) {
            if !ws.is_empty() {
                md.push_str("## Warnings\n\n");
                for w in ws.iter().filter_map(|v| v.as_str()).take(12) {
                    md.push_str("- ");
                    md.push_str(w);
                    md.push('\n');
                }
                md.push('\n');
            }
        }
        md
    }

    fn web_perplexity_markdown(payload: &serde_json::Value) -> String {
        let ok = payload.get("ok").and_then(|v| v.as_bool()).unwrap_or(true);
        let query = payload
            .get("query")
            .and_then(|v| v.as_str())
            .unwrap_or("")
            .trim();
        let elapsed_ms = payload
            .get("elapsed_ms")
            .and_then(|v| v.as_u64())
            .unwrap_or(0);

        let mut md = String::new();
        if ok {
            // Success path: the caller should build the answer/citations markdown inline.
            // This function is used for the error paths; fall through to the summary.
            let answer = payload
                .get("answer")
                .and_then(|a| a.get("text"))
                .and_then(|v| v.as_str())
                .unwrap_or("");
            let citations = payload
                .get("answer")
                .and_then(|a| a.get("citations"))
                .and_then(|v| v.as_array())
                .cloned()
                .unwrap_or_default();
            md.push_str("## Answer\n\n");
            md.push_str(answer);
            if !citations.is_empty() {
                md.push_str("\n\n## Citations\n");
                for c in citations.iter().filter_map(|v| v.as_str()).take(12) {
                    md.push_str("- ");
                    md.push_str(c);
                    md.push('\n');
                }
            }
        } else {
            md.push_str("## Error\n\n");
            let err = payload.get("error").cloned().unwrap_or_default();
            let code = err.get("code").and_then(|v| v.as_str()).unwrap_or("error");
            let message = err
                .get("message")
                .and_then(|v| v.as_str())
                .unwrap_or("unknown");
            let hint = err.get("hint").and_then(|v| v.as_str()).unwrap_or("");
            md.push_str(&format!("- **code**: `{code}`\n"));
            md.push_str(&format!("- **message**: {message}\n"));
            if !hint.is_empty() {
                md.push_str(&format!("\n**Hint**: {hint}\n"));
            }
        }

        md.push_str(&format!("\n---\n*elapsed: {elapsed_ms}ms*"));
        if !query.is_empty() {
            md.push_str(&format!(" | *query: {query}*"));
        }
        md.push('\n');
        md
    }

    fn webpipe_usage_markdown(payload: &serde_json::Value) -> String {
        let ok = payload.get("ok").and_then(|v| v.as_bool()).unwrap_or(true);
        let started = payload
            .get("started_at_epoch_s")
            .and_then(|v| v.as_u64())
            .unwrap_or(0);
        let now = payload
            .get("now_epoch_s")
            .and_then(|v| v.as_u64())
            .unwrap_or(0);
        let tool_calls = payload.get("tool_calls").and_then(|v| v.as_object());
        let warning_counts = payload
            .get("warnings")
            .and_then(|w| w.get("counts"))
            .and_then(|v| v.as_object());

        let mut md = String::new();
        md.push_str("## Summary\n\n");
        md.push_str("- **ok**: ");
        md.push_str(if ok { "true" } else { "false" });
        md.push('\n');
        if started != 0 {
            md.push_str("- **started_at_epoch_s**: ");
            md.push_str(&started.to_string());
            md.push('\n');
        }
        if now != 0 {
            md.push_str("- **now_epoch_s**: ");
            md.push_str(&now.to_string());
            md.push('\n');
        }
        if let Some(ms) = payload.get("elapsed_ms").and_then(|v| v.as_u64()) {
            md.push_str("- **elapsed_ms**: ");
            md.push_str(&ms.to_string());
            md.push('\n');
        }
        md.push('\n');

        md.push_str("## Tool calls\n\n");
        if tool_calls.is_none_or(|m| m.is_empty()) {
            md.push_str("_No tool calls recorded._\n\n");
        } else if let Some(m) = tool_calls {
            // stable-ish ordering: by descending count then name.
            let mut rows: Vec<(&String, u64)> = m
                .iter()
                .filter_map(|(k, v)| v.as_u64().map(|n| (k, n)))
                .collect();
            rows.sort_by(|a, b| b.1.cmp(&a.1).then_with(|| a.0.cmp(b.0)));
            for (k, n) in rows.into_iter().take(12) {
                md.push_str("- `");
                md.push_str(k);
                md.push_str("`: ");
                md.push_str(&n.to_string());
                md.push('\n');
            }
            md.push('\n');
        }

        md.push_str("## Warnings\n\n");
        if warning_counts.is_none_or(|m| m.is_empty()) {
            md.push_str("_No warnings recorded._\n");
        } else if let Some(m) = warning_counts {
            let mut rows: Vec<(&String, u64)> = m
                .iter()
                .filter_map(|(k, v)| v.as_u64().map(|n| (k, n)))
                .collect();
            rows.sort_by(|a, b| b.1.cmp(&a.1).then_with(|| a.0.cmp(b.0)));
            for (k, n) in rows.into_iter().take(12) {
                md.push_str("- `");
                md.push_str(k);
                md.push_str("`: ");
                md.push_str(&n.to_string());
                md.push('\n');
            }
        }
        md
    }

    fn webpipe_meta_markdown(payload: &serde_json::Value) -> String {
        let ok = payload.get("ok").and_then(|v| v.as_bool()).unwrap_or(true);
        let configured = payload.get("configured");
        let supported = payload.get("supported");
        let mcp_toolset = payload
            .get("mcp_toolset")
            .and_then(|v| v.as_str())
            .unwrap_or("")
            .trim();

        let mut md = String::new();
        md.push_str("## Summary\n\n");
        md.push_str("- **ok**: ");
        md.push_str(if ok { "true" } else { "false" });
        md.push('\n');
        if let Some(ms) = payload.get("elapsed_ms").and_then(|v| v.as_u64()) {
            md.push_str("- **elapsed_ms**: ");
            md.push_str(&ms.to_string());
            md.push('\n');
        }
        if !mcp_toolset.is_empty() {
            md.push_str("- **mcp_toolset**: `");
            md.push_str(mcp_toolset);
            md.push_str("`\n");
        }
        md.push('\n');

        if let Some(cfg) = configured {
            md.push_str("## Configured\n\n");
            if let Some(search) = cfg.get("search") {
                md.push_str("- **search**:\n");
                for k in ["brave", "tavily", "searxng"] {
                    let v = search.get(k).and_then(|v| v.as_bool()).unwrap_or(false);
                    md.push_str("  - `");
                    md.push_str(k);
                    md.push_str("`: ");
                    md.push_str(if v { "true" } else { "false" });
                    md.push('\n');
                }
            }
            if let Some(llm) = cfg.get("llm") {
                md.push_str("- **llm**:\n");
                for k in [
                    "perplexity",
                    "ollama",
                    "openai_compat",
                    "openrouter",
                    "groq",
                    "embeddings_openrouter",
                ] {
                    if let Some(v) = llm.get(k).and_then(|v| v.as_bool()) {
                        md.push_str("  - `");
                        md.push_str(k);
                        md.push_str("`: ");
                        md.push_str(if v { "true" } else { "false" });
                        md.push('\n');
                    }
                }
            }
            if let Some(vision) = cfg.get("vision") {
                if let Some(g) = vision.get("gemini").and_then(|v| v.as_bool()) {
                    md.push_str("- **vision.gemini**: ");
                    md.push_str(if g { "true" } else { "false" });
                    md.push('\n');
                }
            }
            md.push('\n');
        }

        if let Some(sup) = supported {
            md.push_str("## Tools\n\n");
            if let Some(tools) = sup.get("mcp_tools").and_then(|v| v.as_array()) {
                md.push_str("- **mcp_tools**: ");
                md.push_str(&tools.len().to_string());
                md.push('\n');
            }
            if let Some(vis) = sup.get("mcp_tools_visible").and_then(|v| v.as_array()) {
                md.push_str("- **mcp_tools_visible**: ");
                md.push_str(&vis.len().to_string());
                md.push('\n');
            }
            if let Some(groups) = sup.get("mcp_tool_groups").and_then(|v| v.as_object()) {
                md.push_str("- **groups**:\n");
                for (k, v) in groups.iter() {
                    let n = v.as_array().map(|a| a.len()).unwrap_or(0);
                    md.push_str("  - `");
                    md.push_str(k);
                    md.push_str("`: ");
                    md.push_str(&n.to_string());
                    md.push('\n');
                }
            }
            md.push('\n');
        }

        // Config-aware "Next" section: guide the user based on what's actually available.
        let available_providers = payload
            .get("available")
            .and_then(|a| a.get("providers"))
            .and_then(|p| p.as_array())
            .map(|a| a.len())
            .unwrap_or(0);
        let perplexity_available = payload
            .get("available")
            .and_then(|a| a.get("tools"))
            .and_then(|t| t.get("web_perplexity"))
            .and_then(|v| v.as_bool())
            .unwrap_or(false);

        md.push_str("## Next\n\n");
        md.push_str("- Use `search_evidence` for any question requiring web evidence (search + extract + ranked chunks).\n");
        md.push_str("- Use `web_extract` when you already have a URL and need readable text.\n");
        md.push_str("- Use `arxiv_search` for academic papers; `arxiv_enrich` for metadata on a specific paper.\n");
        md.push_str("- Pass `minimal_output=true` to `search_evidence` for a compact response (top_chunks + warning_codes only, ~10x smaller). Default is full response.\n");

        // Show search setup hint only when no providers are configured.
        if available_providers == 0 {
            md.push_str("\n**Search not configured** — `search_evidence` works in urls=[...] mode without keys, ");
            md.push_str("but query-mode requires a search provider. Set one of:\n");
            md.push_str("- `WEBPIPE_BRAVE_API_KEY` (Brave Search — recommended)\n");
            md.push_str("- `WEBPIPE_TAVILY_API_KEY` (Tavily)\n");
            md.push_str("- `WEBPIPE_SEARXNG_ENDPOINT` (self-hosted SearXNG)\n");
        }

        // Show Perplexity tip only when it is available.
        if perplexity_available {
            md.push_str("- Use `web_perplexity` for fast single-turn synthesis with live citations (Perplexity Sonar).\n");
        }

        md
    }

    fn web_seed_urls_markdown(payload: &serde_json::Value) -> String {
        let ok = payload.get("ok").and_then(|v| v.as_bool()).unwrap_or(true);
        let seeds = payload.get("seeds").and_then(|v| v.as_array());

        let mut md = String::new();
        md.push_str("## Summary\n\n");
        md.push_str("- **ok**: ");
        md.push_str(if ok { "true" } else { "false" });
        md.push('\n');
        if let Some(max) = payload.get("max").and_then(|v| v.as_u64()) {
            md.push_str("- **max**: ");
            md.push_str(&max.to_string());
            md.push('\n');
        }
        if let Some(ms) = payload.get("elapsed_ms").and_then(|v| v.as_u64()) {
            md.push_str("- **elapsed_ms**: ");
            md.push_str(&ms.to_string());
            md.push('\n');
        }
        md.push('\n');

        md.push_str("## Seeds\n\n");
        if seeds.is_none_or(|a| a.is_empty()) {
            md.push_str("_No seeds returned._\n");
        } else if let Some(ss) = seeds {
            for (i, s) in ss.iter().take(30).enumerate() {
                let id = s.get("id").and_then(|v| v.as_str()).unwrap_or("");
                let url = s.get("url").and_then(|v| v.as_str()).unwrap_or("");
                let kind = s.get("kind").and_then(|v| v.as_str()).unwrap_or("");
                let note = s.get("note").and_then(|v| v.as_str()).unwrap_or("").trim();
                md.push_str(&format!("{}. **`{}`**\n", i + 1, id));
                if !url.is_empty() {
                    md.push_str("   - ");
                    md.push_str(url);
                    md.push('\n');
                }
                if !kind.is_empty() {
                    md.push_str("   - kind: `");
                    md.push_str(kind);
                    md.push_str("`\n");
                }
                if !note.is_empty() {
                    let n: String = note.chars().take(140).collect();
                    md.push_str("   - ");
                    md.push_str(n.trim_end());
                    if note.chars().count() > 140 {
                        md.push('…');
                    }
                    md.push('\n');
                }
            }
        }
        md
    }

    fn webpipe_usage_reset_markdown(payload: &serde_json::Value) -> String {
        let ok = payload.get("ok").and_then(|v| v.as_bool()).unwrap_or(true);
        let at = payload
            .get("reset_at_epoch_s")
            .and_then(|v| v.as_u64())
            .unwrap_or(0);
        let mut md = String::new();
        md.push_str("## Summary\n\n");
        md.push_str("- **ok**: ");
        md.push_str(if ok { "true" } else { "false" });
        md.push('\n');
        if at != 0 {
            md.push_str("- **reset_at_epoch_s**: ");
            md.push_str(&at.to_string());
            md.push('\n');
        }
        if let Some(ms) = payload.get("elapsed_ms").and_then(|v| v.as_u64()) {
            md.push_str("- **elapsed_ms**: ");
            md.push_str(&ms.to_string());
            md.push('\n');
        }
        md
    }

    fn arxiv_search_markdown(payload: &serde_json::Value) -> String {
        let ok = payload.get("ok").and_then(|v| v.as_bool()).unwrap_or(true);
        let query = payload
            .get("query")
            .and_then(|v| v.as_str())
            .unwrap_or("")
            .trim();
        let papers = payload.get("papers").and_then(|v| v.as_array());
        let total = payload.get("total_results").and_then(|v| v.as_u64());
        let page = payload.get("page").and_then(|v| v.as_u64());

        let mut md = String::new();
        md.push_str("## Summary\n\n");
        md.push_str("- **ok**: ");
        md.push_str(if ok { "true" } else { "false" });
        md.push('\n');
        if let Some(p) = page {
            md.push_str("- **page**: ");
            md.push_str(&p.to_string());
            md.push('\n');
        }
        if let Some(t) = total {
            md.push_str("- **total_results**: ");
            md.push_str(&t.to_string());
            md.push('\n');
        }
        if let Some(n) = papers.map(|a| a.len()) {
            md.push_str("- **papers_returned**: ");
            md.push_str(&n.to_string());
            md.push('\n');
        }
        if let Some(ms) = payload.get("elapsed_ms").and_then(|v| v.as_u64()) {
            md.push_str("- **elapsed_ms**: ");
            md.push_str(&ms.to_string());
            md.push('\n');
        }
        md.push('\n');

        if !query.is_empty() {
            md.push_str("## Query\n\n");
            md.push_str(query);
            md.push_str("\n\n");
        }

        if !ok {
            md.push_str("## Error\n\n");
            let code = payload
                .get("error")
                .and_then(|e| e.get("code"))
                .and_then(|v| v.as_str())
                .unwrap_or("unknown_error");
            let msg = payload
                .get("error")
                .and_then(|e| e.get("message"))
                .and_then(|v| v.as_str())
                .unwrap_or("Unknown error.");
            let hint = payload
                .get("error")
                .and_then(|e| e.get("hint"))
                .and_then(|v| v.as_str())
                .unwrap_or("")
                .trim();
            md.push_str("- **code**: `");
            md.push_str(code);
            md.push_str("`\n");
            md.push_str("- **message**: ");
            md.push_str(msg);
            md.push('\n');
            if !hint.is_empty() {
                md.push_str("- **hint**: ");
                md.push_str(hint);
                md.push('\n');
            }
            md.push('\n');
        }

        md.push_str("## Papers\n\n");
        if papers.is_none_or(|a| a.is_empty()) {
            md.push_str("_No papers returned._\n");
        } else if let Some(ps) = papers {
            for (i, p) in ps.iter().take(10).enumerate() {
                let title = p.get("title").and_then(|v| v.as_str()).unwrap_or("").trim();
                let arxiv_id = p
                    .get("arxiv_id")
                    .and_then(|v| v.as_str())
                    .unwrap_or("")
                    .trim();
                let url = p
                    .get("abs_url")
                    .and_then(|v| v.as_str())
                    .unwrap_or("")
                    .trim();
                let pdf = p
                    .get("pdf_url")
                    .and_then(|v| v.as_str())
                    .unwrap_or("")
                    .trim();
                let year = p.get("year").and_then(|v| v.as_u64());
                let authors = p
                    .get("authors")
                    .and_then(|v| v.as_array())
                    .map(|a| {
                        a.iter()
                            .filter_map(|x| x.as_str())
                            .take(6)
                            .collect::<Vec<_>>()
                    })
                    .unwrap_or_default();

                md.push_str(&format!(
                    "{}. **{}**\n",
                    i + 1,
                    if title.is_empty() { arxiv_id } else { title }
                ));
                if !arxiv_id.is_empty() {
                    md.push_str("   - arXiv: `");
                    md.push_str(arxiv_id);
                    md.push_str("`\n");
                }
                if let Some(y) = year {
                    md.push_str("   - year: ");
                    md.push_str(&y.to_string());
                    md.push('\n');
                }
                if !authors.is_empty() {
                    md.push_str("   - authors: ");
                    md.push_str(&authors.join(", "));
                    md.push('\n');
                }
                if !url.is_empty() {
                    md.push_str("   - abs: ");
                    md.push_str(url);
                    md.push('\n');
                }
                if !pdf.is_empty() {
                    md.push_str("   - pdf: ");
                    md.push_str(pdf);
                    md.push('\n');
                }
            }
        }
        md.push('\n');
        if let Some(ws) = payload.get("warnings").and_then(|v| v.as_array()) {
            if !ws.is_empty() {
                md.push_str("## Warnings\n\n");
                for w in ws.iter().filter_map(|v| v.as_str()).take(12) {
                    md.push_str("- ");
                    md.push_str(w);
                    md.push('\n');
                }
            }
        }
        md
    }

    fn arxiv_enrich_markdown(payload: &serde_json::Value) -> String {
        let ok = payload.get("ok").and_then(|v| v.as_bool()).unwrap_or(true);
        let arxiv_id = payload
            .get("arxiv_id")
            .and_then(|v| v.as_str())
            .unwrap_or("")
            .trim();
        let abs_url = payload
            .get("abs_url")
            .and_then(|v| v.as_str())
            .unwrap_or("")
            .trim();
        let paper = payload.get("paper");

        let mut md = String::new();
        md.push_str("## Summary\n\n");
        md.push_str("- **ok**: ");
        md.push_str(if ok { "true" } else { "false" });
        md.push('\n');
        if !arxiv_id.is_empty() {
            md.push_str("- **arxiv_id**: `");
            md.push_str(arxiv_id);
            md.push_str("`\n");
        }
        if !abs_url.is_empty() {
            md.push_str("- **abs_url**: ");
            md.push_str(abs_url);
            md.push('\n');
        }
        if let Some(ms) = payload.get("elapsed_ms").and_then(|v| v.as_u64()) {
            md.push_str("- **elapsed_ms**: ");
            md.push_str(&ms.to_string());
            md.push('\n');
        }
        md.push('\n');

        if !ok {
            md.push_str("## Error\n\n");
            let code = payload
                .get("error")
                .and_then(|e| e.get("code"))
                .and_then(|v| v.as_str())
                .unwrap_or("unknown_error");
            let msg = payload
                .get("error")
                .and_then(|e| e.get("message"))
                .and_then(|v| v.as_str())
                .unwrap_or("Unknown error.");
            let hint = payload
                .get("error")
                .and_then(|e| e.get("hint"))
                .and_then(|v| v.as_str())
                .unwrap_or("")
                .trim();
            md.push_str("- **code**: `");
            md.push_str(code);
            md.push_str("`\n");
            md.push_str("- **message**: ");
            md.push_str(msg);
            md.push('\n');
            if !hint.is_empty() {
                md.push_str("- **hint**: ");
                md.push_str(hint);
                md.push('\n');
            }
            md.push('\n');
        }

        if let Some(p) = paper {
            let title = p.get("title").and_then(|v| v.as_str()).unwrap_or("").trim();
            let authors = p
                .get("authors")
                .and_then(|v| v.as_array())
                .map(|a| {
                    a.iter()
                        .filter_map(|x| x.as_str())
                        .take(8)
                        .collect::<Vec<_>>()
                })
                .unwrap_or_default();
            let pdf = p
                .get("pdf_url")
                .and_then(|v| v.as_str())
                .unwrap_or("")
                .trim();
            let year = p.get("year").and_then(|v| v.as_u64());
            md.push_str("## Paper\n\n");
            if !title.is_empty() {
                md.push_str("- **title**: ");
                md.push_str(title);
                md.push('\n');
            }
            if let Some(y) = year {
                md.push_str("- **year**: ");
                md.push_str(&y.to_string());
                md.push('\n');
            }
            if !authors.is_empty() {
                md.push_str("- **authors**: ");
                md.push_str(&authors.join(", "));
                md.push('\n');
            }
            if !pdf.is_empty() {
                md.push_str("- **pdf**: ");
                md.push_str(pdf);
                md.push('\n');
            }
            md.push('\n');
        }

        if let Some(ds) = payload.get("discussion_search") {
            md.push_str("## Discussion search\n\n");
            let ok2 = ds.get("ok").and_then(|v| v.as_bool()).unwrap_or(false);
            md.push_str("- **ok**: ");
            md.push_str(if ok2 { "true" } else { "false" });
            md.push('\n');
            if let Some(rs) = ds.get("results").and_then(|v| v.as_array()) {
                for (i, r) in rs.iter().take(5).enumerate() {
                    let title = r.get("title").and_then(|v| v.as_str()).unwrap_or("").trim();
                    let url = r.get("url").and_then(|v| v.as_str()).unwrap_or("").trim();
                    if title.is_empty() && url.is_empty() {
                        continue;
                    }
                    md.push_str(&format!(
                        "{}. **{}**\n",
                        i + 1,
                        if title.is_empty() { url } else { title }
                    ));
                    if !url.is_empty() {
                        md.push_str("   - ");
                        md.push_str(url);
                        md.push('\n');
                    }
                }
            }
            md.push('\n');
        }

        md
    }

    fn web_cache_search_extract_markdown(payload: &serde_json::Value) -> String {
        let ok = payload.get("ok").and_then(|v| v.as_bool()).unwrap_or(true);
        let query = payload
            .get("query")
            .and_then(|v| v.as_str())
            .unwrap_or("")
            .trim();
        let cache_dir = payload
            .get("cache_dir")
            .and_then(|v| v.as_str())
            .unwrap_or("")
            .trim();
        let results = payload.get("results").and_then(|v| v.as_array());

        let mut md = String::new();
        if !query.is_empty() {
            md.push_str("## Query\n\n");
            md.push_str(query);
            md.push_str("\n\n");
        }
        md.push_str("## Summary\n\n");
        md.push_str("- **ok**: ");
        md.push_str(if ok { "true" } else { "false" });
        md.push('\n');
        if !cache_dir.is_empty() {
            md.push_str("- **cache_dir**: `");
            md.push_str(cache_dir);
            md.push_str("`\n");
        }
        if let Some(n) = results.map(|a| a.len()) {
            md.push_str("- **docs_returned**: ");
            md.push_str(&n.to_string());
            md.push('\n');
        }
        if let Some(d) = payload.get("dedup").and_then(|v| v.as_object()) {
            let dropped = d.get("dropped").and_then(|v| v.as_u64()).unwrap_or(0);
            if dropped > 0 {
                md.push_str("- **dedup_dropped**: ");
                md.push_str(&dropped.to_string());
                md.push('\n');
            }
        }
        if let Some(ms) = payload.get("elapsed_ms").and_then(|v| v.as_u64()) {
            md.push_str("- **elapsed_ms**: ");
            md.push_str(&ms.to_string());
            md.push('\n');
        }
        md.push('\n');

        if !ok {
            md.push_str("## Error\n\n");
            let code = payload
                .get("error")
                .and_then(|e| e.get("code"))
                .and_then(|v| v.as_str())
                .unwrap_or("unknown_error");
            let msg = payload
                .get("error")
                .and_then(|e| e.get("message"))
                .and_then(|v| v.as_str())
                .unwrap_or("Unknown error.");
            let hint = payload
                .get("error")
                .and_then(|e| e.get("hint"))
                .and_then(|v| v.as_str())
                .unwrap_or("")
                .trim();
            md.push_str("- **code**: `");
            md.push_str(code);
            md.push_str("`\n");
            md.push_str("- **message**: ");
            md.push_str(msg);
            md.push('\n');
            if !hint.is_empty() {
                md.push_str("- **hint**: ");
                md.push_str(hint);
                md.push('\n');
            }
            md.push('\n');
        }

        md.push_str("## Matches\n\n");
        if results.is_none_or(|a| a.is_empty()) {
            md.push_str("_No matches._\n");
        } else if let Some(rs) = results {
            for (i, doc) in rs.iter().take(5).enumerate() {
                let url = doc.get("url").and_then(|v| v.as_str()).unwrap_or("").trim();
                let title = doc
                    .get("title")
                    .and_then(|v| v.as_str())
                    .unwrap_or("")
                    .trim();
                md.push_str(&format!(
                    "{}. **{}**\n",
                    i + 1,
                    if title.is_empty() {
                        if url.is_empty() {
                            "(untitled)"
                        } else {
                            url
                        }
                    } else {
                        title
                    }
                ));
                if !url.is_empty() {
                    md.push_str("   - ");
                    md.push_str(url);
                    md.push('\n');
                }
                if let Some(chunks) = doc
                    .get("extract")
                    .and_then(|e| e.get("chunks"))
                    .and_then(|v| v.as_array())
                {
                    for c in chunks.iter().take(2) {
                        let t = c.get("text").and_then(|v| v.as_str()).unwrap_or("").trim();
                        if t.is_empty() {
                            continue;
                        }
                        let short: String = t.chars().take(220).collect();
                        md.push_str("   - ");
                        md.push_str(short.trim_end());
                        if t.chars().count() > 220 {
                            md.push('…');
                        }
                        md.push('\n');
                    }
                }
            }
        }
        md.push('\n');
        if let Some(ws) = payload.get("warnings").and_then(|v| v.as_array()) {
            if !ws.is_empty() {
                md.push_str("## Warnings\n\n");
                for w in ws.iter().filter_map(|v| v.as_str()).take(12) {
                    md.push_str("- ");
                    md.push_str(w);
                    md.push('\n');
                }
            }
        }
        md
    }

    fn clean_text_for_output(s: String) -> String {
        // Keep MCP tool outputs “clean” and JSON-friendly even when upstream content contains
        // control characters or platform-newline weirdness.
        //
        // This is intentionally conservative: preserve \n and \t; replace other ASCII control
        // chars with spaces; drop BOM/ZWNBSP.
        let s = s.replace("\r\n", "\n").replace('\r', "\n");
        let mut out = String::with_capacity(s.len());
        for ch in s.chars() {
            if ch == '\u{FEFF}' {
                continue;
            }
            if ch == '\u{000C}' {
                out.push('\n');
                continue;
            }
            if (ch <= '\u{001F}' && ch != '\n' && ch != '\t') || ch == '\u{007F}' {
                out.push(' ');
                continue;
            }
            out.push(ch);
        }
        out
    }

    fn payload_from_result(r: &CallToolResult) -> serde_json::Value {
        if let Some(v) = r.structured_content.clone() {
            return v;
        }
        let s = r
            .content
            .first()
            .and_then(|c| c.as_text())
            .map(|t| t.text.clone())
            .unwrap_or_default();
        serde_json::from_str(&s).unwrap_or_else(|_| serde_json::json!({}))
    }

    /// Arguments for `webpipe_meta`.
    #[derive(Debug, Deserialize, JsonSchema, Default)]
    struct WebpipeMetaArgs {
        /// Operation to perform (default: "info").
        ///
        /// - "info" (default): server capabilities, configured backends, supported values, defaults.
        /// - "usage": runtime stats — tool call counts, provider cost units, warning counts.
        /// - "usage_reset": reset runtime stats (side effect; idempotent).
        #[serde(default)]
        method: Option<String>,
    }

    #[derive(Debug, Deserialize, JsonSchema, Default)]
    struct WebpipeUsageArgs {}

    #[derive(Debug, Deserialize, JsonSchema, Default)]
    struct WebSeedUrlsArgs {
        /// Max seed urls to return (bounded).
        #[serde(default)]
        max: Option<usize>,
        /// If true, return an additional `ids` array for convenience.
        #[serde(default)]
        include_ids: Option<bool>,
        /// If true, return only ids+urls (omit kind/note fields) in the `seeds` list.
        ///
        /// This is useful when you want maximum output stability (ids/urls only).
        #[serde(default)]
        ids_only: Option<bool>,
    }

    #[derive(Debug, Deserialize, JsonSchema, Default)]
    struct WebSeedSearchExtractArgs {
        /// Query to use for chunk selection.
        query: String,
        /// Optional explicit URL list. If provided, we use these instead of built-in seeds.
        #[serde(default)]
        urls: Option<Vec<String>>,
        /// Optional seed ids from `web_seed_urls`. Ignored when `urls` is provided.
        #[serde(default)]
        seed_ids: Option<Vec<String>>,
        /// Max URLs to fetch/extract (bounded).
        #[serde(default)]
        max_urls: Option<usize>,
        /// Which fetch backend to use (default: local). Allowed: local, firecrawl
        #[serde(default)]
        fetch_backend: Option<String>,
        /// If true, do not perform any network calls. For local fetch_backend, this means:
        /// - allow cache reads
        /// - error on cache miss
        ///
        /// For firecrawl fetch_backend, this always errors (firecrawl is network-only).
        #[serde(default)]
        no_network: Option<bool>,
        /// Width for text wrapping (default: 100).
        #[serde(default)]
        width: Option<usize>,
        /// Max chars in output text (default: 20_000).
        #[serde(default)]
        max_chars: Option<usize>,
        /// Max bytes to fetch (default: 2_000_000).
        #[serde(default)]
        max_bytes: Option<u64>,
        /// How many chunks to return per URL (default: 5).
        #[serde(default)]
        top_chunks: Option<usize>,
        /// Cap merged chunks per seed id (default: top_chunks).
        #[serde(default)]
        merged_max_per_seed: Option<usize>,
        /// Max chars per chunk snippet (default: 500).
        #[serde(default)]
        max_chunk_chars: Option<usize>,
        /// Allow cache reads (default: true).
        #[serde(default)]
        cache_read: Option<bool>,
        /// Allow cache writes (default: true).
        #[serde(default)]
        cache_write: Option<bool>,
        /// Optional cache TTL in seconds.
        #[serde(default)]
        cache_ttl_s: Option<u64>,
        /// Include full extracted text (default: false; usually too big for seeds).
        #[serde(default)]
        include_text: Option<bool>,
        /// Compact output (default: true).
        #[serde(default)]
        compact: Option<bool>,
    }

    /// Arguments for `web_fetch`.
    ///
    /// Output shape (high-level):
    /// - `ok` boolean
    /// - `url`, `final_url`, `status`, `content_type`, `bytes_len`
    /// - optional `text` (bounded by `max_text_chars`)
    /// - optional `headers` (when `include_headers=true`)
    /// - `warnings` + `warning_hints` when applicable
    #[derive(Debug, Deserialize, JsonSchema, Default)]
    struct WebFetchArgs {
        /// URL to fetch (required).
        #[serde(default)]
        url: Option<String>,
        /// Which fetch backend to use (default: local). Allowed: local, firecrawl
        #[serde(default)]
        fetch_backend: Option<String>,
        /// If true, do not perform any network calls. For local fetch_backend, this means:
        /// - allow cache reads
        /// - error on cache miss
        ///
        /// For firecrawl fetch_backend, this always errors (firecrawl is network-only).
        #[serde(default)]
        no_network: Option<bool>,
        /// Per-request timeout in milliseconds.
        #[serde(default)]
        timeout_ms: Option<u64>,
        /// Max bytes to fetch (hard bound).
        #[serde(default)]
        max_bytes: Option<u64>,
        /// Max chars in output text (default: 20_000).
        #[serde(default)]
        max_text_chars: Option<usize>,
        /// Optional extra request headers (some unsafe headers are dropped by default).
        #[serde(default)]
        headers: Option<BTreeMap<String, String>>,
        /// Allow cache reads (default: true).
        #[serde(default)]
        cache_read: Option<bool>,
        /// Allow cache writes (default: true).
        #[serde(default)]
        cache_write: Option<bool>,
        /// Optional cache TTL in seconds.
        #[serde(default)]
        cache_ttl_s: Option<u64>,
        /// Include response body text in output (default: false).
        #[serde(default)]
        include_text: Option<bool>,
        /// Include response headers in output (default: false).
        /// Include response headers in output (default: false).
        #[serde(default)]
        include_headers: Option<bool>,
    }

    /// Arguments for `web_extract`.
    ///
    /// Output shape (high-level):
    /// - `ok` boolean
    /// - extraction result (`text_chars`, `chunks`) and optional `text`
    /// - warnings + hints for common failure modes (paywall/JS-challenge/low-signal)
    #[derive(Debug, Deserialize, JsonSchema, Default)]
    struct WebExtractArgs {
        /// URL to fetch+extract (required).
        #[serde(default)]
        url: Option<String>,
        /// Which fetch backend to use (default: local). Allowed: local, firecrawl
        #[serde(default)]
        fetch_backend: Option<String>,
        /// If true, do not perform any network calls. For local fetch_backend, this means:
        /// - allow cache reads
        /// - error on cache miss
        ///
        /// For firecrawl fetch_backend, this always errors (firecrawl is network-only).
        #[serde(default)]
        no_network: Option<bool>,
        /// Width for text wrapping (default: 100).
        #[serde(default)]
        width: Option<usize>,
        /// Max chars in output text (default: 20_000).
        #[serde(default)]
        max_chars: Option<usize>,
        /// Optional query: if set, return top matching chunks.
        #[serde(default)]
        query: Option<String>,
        /// Max chunks to return when `query` is set (default: 5).
        #[serde(default)]
        top_chunks: Option<usize>,
        /// Max chars per chunk when `query` is set (default: 500).
        #[serde(default)]
        max_chunk_chars: Option<usize>,
        /// Include the full extracted text (default: true when query is omitted; false when query is set).
        #[serde(default)]
        include_text: Option<bool>,
        /// Include extracted links (default: false).
        #[serde(default)]
        include_links: Option<bool>,
        /// Max links to return (default: 50).
        #[serde(default)]
        max_links: Option<usize>,
        #[serde(default)]
        timeout_ms: Option<u64>,
        #[serde(default)]
        max_bytes: Option<u64>,
        /// If true, and the first fetch is truncated by max_bytes, retry once with a larger max_bytes
        /// (bounded) to recover tail content (default: auto for PDFs; false otherwise).
        #[serde(default)]
        retry_on_truncation: Option<bool>,
        /// Upper bound for the truncation retry max_bytes (default: 10_000_000; max: 20_000_000).
        #[serde(default)]
        truncation_retry_max_bytes: Option<u64>,
        /// Allow cache reads (default: true).
        #[serde(default)]
        cache_read: Option<bool>,
        /// Allow cache writes (default: true).
        #[serde(default)]
        cache_write: Option<bool>,
        /// Optional cache TTL in seconds.
        #[serde(default)]
        cache_ttl_s: Option<u64>,
        /// Include structured extraction (outline + blocks) (default: false).
        #[serde(default)]
        include_structure: Option<bool>,
        /// Max outline items returned in structure (default: 25; max: 200).
        #[serde(default)]
        max_outline_items: Option<usize>,
        /// Max blocks returned in structure (default: 40; max: 200).
        #[serde(default)]
        max_blocks: Option<usize>,
        /// Max chars per block in structure (default: 400; max: 2000).
        #[serde(default)]
        max_block_chars: Option<usize>,
        /// If true, compute semantic chunk scores using embeddings (default: false; requires feature).
        #[serde(default)]
        semantic_rerank: Option<bool>,
        /// If true, and semantic_rerank=false, opportunistically run a single semantic rerank pass
        /// when lexical chunk scoring appears ineffective (default: true).
        ///
        /// This is bounded (top_k only) and only uses embeddings if configured.
        #[serde(default)]
        semantic_auto_fallback: Option<bool>,
        /// Max semantic chunks to return when semantic_rerank=true (default: 5; max: 50).
        #[serde(default)]
        semantic_top_k: Option<usize>,
    }

    /// Arguments for `web_search`.
    ///
    /// Output shape (high-level):
    /// - `ok` boolean
    /// - `results[]` with `url/title/snippet`
    /// - provider selection details when `provider=auto`
    #[derive(Debug, Deserialize, JsonSchema, Default)]
    struct WebSearchArgs {
        /// Search query (required).
        #[serde(default)]
        query: Option<String>,
        /// Which provider to use (default: brave). Allowed: auto, brave, tavily, searxng
        #[serde(default)]
        provider: Option<String>,
        /// When provider="auto", choose routing mode:
        /// - "fallback" (default): pick the best available provider (brave-first) and fall back on failure.
        /// - "merge": query all configured providers and merge/dedup results (bounded).
        /// - "mab": adaptive + deterministic choice based on in-process usage stats.
        #[serde(default)]
        auto_mode: Option<String>,
        #[serde(default)]
        max_results: Option<usize>,
        #[serde(default)]
        language: Option<String>,
        #[serde(default)]
        country: Option<String>,
        /// Search timeout (ms). Default: 20_000; max: 60_000.
        ///
        /// Some search providers do not apply any default timeout; leaving this unset can hang.
        #[serde(default)]
        timeout_ms: Option<u64>,
    }

    /// Arguments for `web_perplexity`.
    ///
    /// This is the "Perplexity replacement" tool: ask Perplexity directly and return
    /// an answer with citations (bounded).
    #[derive(Debug, Deserialize, JsonSchema, Default)]
    struct WebPerplexityArgs {
        /// Prompt/question (required).
        #[serde(default)]
        query: Option<String>,
        /// Model name (default: "sonar").
        #[serde(default)]
        model: Option<String>,
        /// Max tokens to generate (default: 800; max: 4_000).
        #[serde(default)]
        max_tokens: Option<u64>,
        /// Temperature (optional).
        #[serde(default)]
        temperature: Option<f64>,
        /// Top-p (optional).
        #[serde(default)]
        top_p: Option<f64>,
        /// Perplexity search mode (optional). Common values: "web", "off".
        /// If omitted, defaults to "web" when no_network=false, else "off".
        #[serde(default)]
        search_mode: Option<String>,
        /// Timeout for the request (ms). Default: 20_000; max: 60_000.
        #[serde(default)]
        timeout_ms: Option<u64>,
        /// If true, disallow network browsing (forces search_mode="off").
        #[serde(default)]
        no_network: Option<bool>,
        /// Hard cap on returned answer characters (default: 4_000; max: 20_000).
        #[serde(default)]
        max_answer_chars: Option<usize>,
    }

    #[derive(Debug, Deserialize, JsonSchema, Default)]
    struct ArxivSearchArgs {
        /// Search query (required).
        #[serde(default)]
        query: Option<String>,
        /// Optional ArXiv categories like "cs.LG", "stat.ML".
        #[serde(default)]
        categories: Option<Vec<String>>,
        /// Optional filter by publication years.
        #[serde(default)]
        years: Option<Vec<u32>>,
        /// Page number (1-based). Default: 1.
        #[serde(default)]
        page: Option<usize>,
        /// Alias for per_page (matches web_search args). Default: 10; max: 50.
        ///
        /// If both `per_page` and `max_results` are provided, `per_page` wins.
        #[serde(default)]
        max_results: Option<usize>,
        /// Results per page. Default: 10; max: 50.
        #[serde(default)]
        per_page: Option<usize>,
        /// Timeout per request (ms). Default: 20_000.
        #[serde(default)]
        timeout_ms: Option<u64>,
        /// If true, rerank returned papers using semantic embeddings (feature-gated).
        #[serde(default)]
        semantic_rerank: Option<bool>,
        /// Max papers to keep after semantic rerank. Default: per_page; max: 50.
        #[serde(default)]
        semantic_top_k: Option<usize>,
    }

    #[derive(Debug, Deserialize, JsonSchema, Default)]
    struct PaperSearchArgs {
        /// Search query (required).
        #[serde(default)]
        query: Option<String>,
        /// Which paper backends to query (optional).
        ///
        /// Allowed (canonical):
        /// - "semantic_scholar"
        /// - "openalex"
        /// - "google_scholar_serpapi" (requires WEBPIPE_SERPAPI_API_KEY)
        ///
        /// Aliases accepted:
        /// - "s2" / "semanticscholar" -> semantic_scholar
        /// - "scholar" / "serpapi" -> google_scholar_serpapi
        #[serde(default)]
        backends: Option<Vec<String>>,
        /// Optional filter by publication years.
        #[serde(default)]
        years: Option<Vec<u32>>,
        /// Max papers to return per backend (default: 10; max: 50).
        #[serde(default)]
        limit: Option<usize>,
        /// Timeout per backend request (ms). Default: 20_000; max: 60_000.
        #[serde(default)]
        timeout_ms: Option<u64>,
        /// If true, include abstract text when available (default: false).
        #[serde(default)]
        include_abstract: Option<bool>,
    }

    #[derive(Debug, Deserialize, JsonSchema, Default)]
    struct ArxivEnrichArgs {
        /// ArXiv ID (e.g. "0805.3415") or an arxiv.org/abs URL.
        #[serde(default)]
        id_or_url: Option<String>,
        /// If true, find best-effort discussion/commentary via web search (bounded).
        #[serde(default)]
        include_discussions: Option<bool>,
        /// Max discussion pages to fetch/extract (default: 2; max: 5).
        #[serde(default)]
        max_discussion_urls: Option<usize>,
        /// Timeout per discussion fetch (ms). Default: 20_000.
        #[serde(default)]
        timeout_ms: Option<u64>,
    }

    /// Combined arXiv tool: search by topic (pass `query`) or get metadata for one paper (pass `id_or_url`).
    #[derive(Debug, Deserialize, JsonSchema, Default)]
    struct ArxivArgs {
        // ---- enrich fields (pass id_or_url to get metadata for one specific paper) ----
        /// ArXiv ID (e.g. "0805.3415") or an arxiv.org/abs URL.
        /// If provided, enrich mode: returns full metadata for this paper.
        /// Omit when searching by topic — use `query` instead.
        #[serde(default)]
        id_or_url: Option<String>,
        /// Include best-effort discussion/commentary links via web search (bounded). Enrich mode only.
        #[serde(default)]
        include_discussions: Option<bool>,
        /// Max discussion pages to include (default: 2; max: 5). Enrich mode only.
        #[serde(default)]
        max_discussion_urls: Option<usize>,

        // ---- search fields (pass query to search for papers by topic/keyword) ----
        /// Search query. If provided (and id_or_url is absent), search mode: returns papers[].
        #[serde(default)]
        query: Option<String>,
        /// ArXiv categories to filter by (e.g. "cs.AI", "cs.CL"). Search mode only.
        #[serde(default)]
        categories: Option<Vec<String>>,
        /// Filter by publication years. Search mode only.
        #[serde(default)]
        years: Option<Vec<u32>>,
        /// Page number (1-based, default: 1). Search mode only.
        #[serde(default)]
        page: Option<usize>,
        /// Results per page (default: 10; max: 50). Alias: max_results. Search mode only.
        #[serde(default)]
        per_page: Option<usize>,
        /// Alias for per_page. Search mode only.
        #[serde(default)]
        max_results: Option<usize>,
        /// Rerank returned papers using semantic embeddings. Search mode only.
        #[serde(default)]
        semantic_rerank: Option<bool>,
        /// Max papers after semantic rerank (default: per_page; max: 50). Search mode only.
        #[serde(default)]
        semantic_top_k: Option<usize>,

        // ---- shared ----
        /// Timeout (ms). Default: 20_000.
        #[serde(default)]
        timeout_ms: Option<u64>,
    }

    /// Arguments for `web_cache_search_extract`.
    ///
    /// Use this when you want **no network** and you have a populated `WEBPIPE_CACHE_DIR`.
    /// It scans the cache, extracts text, and returns top chunks for the query.
    ///
    /// Notes:
    /// - This is bounded by `max_docs`, `max_bytes`, `max_chars`, and `top_chunks`.
    /// - It is useful as an “offline-ish” regression check (same cache → comparable outputs).
    #[derive(Debug, Deserialize, JsonSchema, Default)]
    struct WebCacheSearchExtractArgs {
        /// Search query (required).
        #[serde(default)]
        query: Option<String>,
        /// Max cached documents to inspect (default: 50; max: 500).
        #[serde(default)]
        max_docs: Option<usize>,
        /// Max chars extracted per cached doc (default: 20_000; max: 200_000).
        #[serde(default)]
        max_chars: Option<usize>,
        /// Max bytes to read per cached doc body (default: 5_000_000).
        #[serde(default)]
        max_bytes: Option<u64>,
        /// Width for HTML-to-text wrapping (default: 100).
        #[serde(default)]
        width: Option<usize>,
        /// Max chunks per doc (default: 5; max: 50).
        #[serde(default)]
        top_chunks: Option<usize>,
        /// Max chars per chunk (default: 500; max: 5_000).
        #[serde(default)]
        max_chunk_chars: Option<usize>,
        /// Include structured extraction (default: false).
        #[serde(default)]
        include_structure: Option<bool>,
        #[serde(default)]
        max_outline_items: Option<usize>,
        #[serde(default)]
        max_blocks: Option<usize>,
        #[serde(default)]
        max_block_chars: Option<usize>,
        /// If true, include per-doc extracted text (default: false).
        #[serde(default)]
        include_text: Option<bool>,
        /// Max cache entries to scan for recency filtering (default: 2000; max: 20000).
        #[serde(default)]
        max_scan_entries: Option<usize>,
        /// If true, compute semantic chunk scores using embeddings (default: false; feature-gated).
        #[serde(default)]
        semantic_rerank: Option<bool>,
        /// Max semantic chunks per doc when semantic_rerank=true (default: 5; max: 50).
        #[serde(default)]
        semantic_top_k: Option<usize>,
        /// If true, return a more compact per-doc shape (default: true).
        ///
        /// This reduces duplication between top-level fields and `extract.*` by keeping:
        /// - stable identifiers (url/final_url/status/content_type/bytes/score)
        /// - `extract` (engine + chunks + optional structure/text)
        /// - warnings + warning_codes + warning_hints
        /// - semantic (if present)
        #[serde(default)]
        compact: Option<bool>,
    }

    /// Arguments for `web_search_extract`.
    ///
    /// This is the main “do the job” tool:
    /// - optional search (if `query` is set and `urls` is not)
    /// - fetch (cache-aware)
    /// - extract (bounded)
    /// - rank and return top chunks across URLs
    ///
    /// Common call shapes:
    /// - Query mode (online): set `query`, omit `urls`.
    /// - URLs mode (offline-friendly): set `urls`, optionally set `query` for chunk ranking.
    /// - Offline mode: set `no_network=true` and pass `urls` (cache-only fetch).
    /// - Cache-corpus mode (offline inspiration): set `no_network=true` and omit `urls`
    ///   to search over `WEBPIPE_CACHE_DIR` contents.
    ///
    /// Output is JSON (as text + structured_content) with:
    /// - `results[]` (per-URL) and `top_chunks[]` (merged, bounded)
    /// - `warnings` + `warning_hints` for common failure modes
    #[derive(Debug, Deserialize, JsonSchema, Default)]
    pub(crate) struct WebSearchExtractArgs {
        /// Search query. Required unless `urls` is provided.
        #[serde(default)]
        pub(crate) query: Option<String>,
        /// If provided, skip search and hydrate these URLs directly (offline-friendly).
        #[serde(default)]
        pub(crate) urls: Option<Vec<String>>,
        /// When urls=[...] is provided, how to choose which URLs to process under max_urls.
        ///
        /// - "auto" (default): if query is non-empty, use query_rank; else preserve
        /// - "preserve": preserve input order (after dedup)
        /// - "query_rank": stable rerank by query-token matches in the URL string
        #[serde(default)]
        pub(crate) url_selection_mode: Option<String>,
        /// Which search provider to use if `query` is used (default: auto). Allowed: auto, brave, tavily, searxng
        #[serde(default)]
        pub(crate) provider: Option<String>,
        /// When provider="auto", choose routing mode (default: "fallback"). Allowed: fallback, merge, mab
        #[serde(default)]
        pub(crate) auto_mode: Option<String>,
        /// How to select `top_chunks` across URLs (default: "score"). Allowed: score, pareto
        ///
        /// - "score": sort by chunk score (descending)
        /// - "pareto": non-dominated selection across score/cost/warnings (bounded)
        #[serde(default)]
        pub(crate) selection_mode: Option<String>,
        /// Which fetch backend to use (default: local). Allowed: local, firecrawl, render
        ///
        /// - local: reqwest + cache (best default; supports Tor SOCKS via WEBPIPE_ANON_PROXY in anonymous mode)
        /// - firecrawl: remote fetch (disabled in anonymous mode)
        /// - render: Playwright/Chromium render-to-HTML, then normal extraction (JS-heavy pages)
        #[serde(default)]
        pub(crate) fetch_backend: Option<String>,
        /// If true, do not perform any network calls:
        /// - query mode is disabled (you must pass urls=[...])
        /// - per-URL fetch becomes cache-only; cache misses return ok=false per URL
        /// - firecrawl fallback is disabled
        #[serde(default)]
        pub(crate) no_network: Option<bool>,
        /// If true, and fetch_backend="local", retry a single URL with Firecrawl when local extraction yields empty text.
        ///
        /// This is a bounded, per-URL fallback: it only triggers when local extraction produced `empty_extraction`,
        /// and only if Firecrawl is configured.
        #[serde(default)]
        pub(crate) firecrawl_fallback_on_empty_extraction: Option<bool>,
        /// If true, and fetch_backend="local", retry a single URL with Firecrawl when local extraction
        /// yields non-empty but clearly low-signal output (e.g. JS bundle/app-shell gunk).
        ///
        /// This is bounded and per-URL (like the empty-extraction fallback), and only triggers when
        /// Firecrawl is configured.
        #[serde(default)]
        pub(crate) firecrawl_fallback_on_low_signal: Option<bool>,
        /// If true, and fetch_backend="local", retry a single URL with Playwright render when local extraction yields empty text.
        ///
        /// This is bounded and per-URL (like Firecrawl fallback). It requires Playwright to be installed, and is disabled
        /// in privacy_mode=offline and in no_network=true.
        #[serde(default)]
        pub(crate) render_fallback_on_empty_extraction: Option<bool>,
        /// If true, and fetch_backend="local", retry a single URL with Playwright render when local extraction
        /// yields non-empty but clearly low-signal output (e.g. JS bundle/app-shell gunk).
        ///
        /// This is bounded and per-URL, and requires Playwright to be installed. It is disabled
        /// in privacy_mode=offline and in no_network=true.
        #[serde(default)]
        pub(crate) render_fallback_on_low_signal: Option<bool>,
        /// Max search results to request (default: 5; max: 20).
        #[serde(default)]
        pub(crate) max_results: Option<usize>,
        /// Max URLs to actually fetch/extract (default: 3; max: 10).
        #[serde(default)]
        pub(crate) max_urls: Option<usize>,
        /// Max in-flight URL fetch+extract operations (default: 3; max: 10).
        ///
        /// This enables "wide parallel" hydration for urls-mode and non-agentic runs.
        /// Results remain deterministic (input order preserved).
        #[serde(default)]
        pub(crate) max_parallel_urls: Option<usize>,
        /// Hard overall deadline for the full search→fetch→extract pipeline (ms).
        ///
        /// This is a "wall clock" cap used to prevent worst-case hangs from any single stage.
        /// When hit, the tool returns partial results (if any) and includes `deadline_exceeded_partial`
        /// in top-level `warning_codes`.
        #[serde(default)]
        pub(crate) deadline_ms: Option<u64>,
        /// Optional allow-list of domains/hosts. If set (non-empty), only URLs whose host matches
        /// one of these rules are considered.
        ///
        /// Rules are hostnames like `slsa.dev` or `learn.microsoft.com`. A rule matches either:
        /// - exact host, or
        /// - any subdomain of that host (e.g. `docs.slsa.dev` matches `slsa.dev`).
        #[serde(default)]
        pub(crate) domains_allow: Option<Vec<String>>,
        /// Optional deny-list of domains/hosts. If set, any URL whose host matches one of these
        /// rules is excluded (even if it also matches domains_allow).
        #[serde(default)]
        pub(crate) domains_deny: Option<Vec<String>>,
        /// Fetch timeout per URL (ms).
        #[serde(default)]
        pub(crate) timeout_ms: Option<u64>,
        /// Max bytes per URL.
        #[serde(default)]
        pub(crate) max_bytes: Option<u64>,
        /// If true, and fetch_backend="local", retry a single URL with a higher max_bytes
        /// when the body was truncated by max_bytes (default: false).
        ///
        /// This is an opt-in, per-URL retry to handle very large HTML pages (common on JS-heavy docs).
        /// The retry is bounded by `truncation_retry_max_bytes` (or a conservative default cap).
        #[serde(default)]
        pub(crate) retry_on_truncation: Option<bool>,
        /// When retry_on_truncation=true, the max_bytes to use for the retry.
        ///
        /// If omitted, webpipe uses a best-effort default (currently 2x max_bytes, capped).
        #[serde(default)]
        pub(crate) truncation_retry_max_bytes: Option<u64>,
        /// Width used for HTML->text extraction (default: 100).
        #[serde(default)]
        pub(crate) width: Option<usize>,
        /// Max chars of extracted text per URL (default: 30_000; max: 200_000).
        #[serde(default)]
        pub(crate) max_chars: Option<usize>,
        /// Max chunks per URL (default: 8; max: 50).
        #[serde(default)]
        pub(crate) top_chunks: Option<usize>,
        /// Max chars per chunk (default: 800; max: 5_000).
        #[serde(default)]
        pub(crate) max_chunk_chars: Option<usize>,
        /// Include extracted links (default: false).
        #[serde(default)]
        pub(crate) include_links: Option<bool>,
        /// Include structured extraction (outline + blocks) per URL (default: true).
        #[serde(default)]
        pub(crate) include_structure: Option<bool>,
        /// Max outline items returned in structure (default: 25; max: 200).
        #[serde(default)]
        pub(crate) max_outline_items: Option<usize>,
        /// Max blocks returned in structure (default: 40; max: 200).
        #[serde(default)]
        pub(crate) max_blocks: Option<usize>,
        /// Max chars per block in structure (default: 400; max: 2000).
        #[serde(default)]
        pub(crate) max_block_chars: Option<usize>,
        /// If true, compute semantic chunk scores per URL using embeddings (default: false; requires feature).
        #[serde(default)]
        pub(crate) semantic_rerank: Option<bool>,
        /// Max semantic chunks per URL when semantic_rerank=true (default: 5; max: 50).
        #[serde(default)]
        pub(crate) semantic_top_k: Option<usize>,
        /// Max links per URL (default: 25; max: 500).
        #[serde(default)]
        pub(crate) max_links: Option<usize>,
        /// Include full extracted text in per-URL results (default: false).
        #[serde(default)]
        pub(crate) include_text: Option<bool>,
        #[serde(default)]
        pub(crate) cache_read: Option<bool>,
        #[serde(default)]
        pub(crate) cache_write: Option<bool>,
        #[serde(default)]
        pub(crate) cache_ttl_s: Option<u64>,

        /// High-level width vs depth preset (optional).
        ///
        /// This does not override explicitly provided fields; it only fills in defaults.
        ///
        /// - "balanced" (default): current defaults
        /// - "wide": more breadth (more search + frontier) with shallower per-page extraction
        /// - "deep": fewer pages, deeper per-page extraction, enables agentic discovery
        /// - "smart": balanced but enables agentic discovery
        #[serde(default)]
        pub(crate) exploration: Option<String>,

        /// Enable an internal, bounded agentic loop:
        /// - fetch one URL
        /// - extract links (internally) and add to a frontier
        /// - re-rank which URL to fetch next (deterministic, bounded by max_urls)
        ///
        /// This helps “discover the right page” from adjacent pages rather than relying on URL-only heuristics.
        #[serde(default)]
        pub(crate) agentic: Option<bool>,

        /// Agentic selection strategy (when agentic=true):
        /// - "lexical" (default): deterministic URL-token ranking + link priors
        /// - "llm": ask OpenRouter (via `axi` structured output) to choose the next URL from a bounded candidate set
        ///   (falls back to lexical if not configured)
        #[serde(default)]
        pub(crate) agentic_selector: Option<String>,

        /// When agentic=true, allow up to this many search rounds total (default: 1).
        ///
        /// - 1 means “single search only” (current default behavior).
        /// - 2+ enables a bounded “search more” escape hatch.
        #[serde(default)]
        pub(crate) agentic_max_search_rounds: Option<usize>,

        /// When agentic=true, maximum frontier size (default: 200).
        #[serde(default)]
        pub(crate) agentic_frontier_max: Option<usize>,

        /// Max planner (LLM) calls for a single request (default: WEBPIPE_PLANNER_MAX_CALLS or 1).
        #[serde(default)]
        pub(crate) planner_max_calls: Option<usize>,

        /// If true, return a more compact output shape (default: true).
        ///
        /// This removes:
        /// - large agentic traces (replaced with a short summary)
        /// - some redundant per-URL fields not needed for downstream agent steps
        #[serde(default)]
        pub(crate) compact: Option<bool>,

        /// If true, return only the minimal signal set: ok, top_chunks,
        /// warning_codes, warning_hints, elapsed_ms, schema_version, kind.
        ///
        /// Strips the full per-URL results[], request echo, search.steps, backend_provider,
        /// agentic trace, and other verbose fields. Reduces response size by ~10x.
        ///
        /// Default: false (full response). Set true for tight agent loops that only need
        /// the top evidence chunks and whether any warnings occurred.
        ///
        /// Inspired by the minimal_output pattern from github/github-mcp-server.
        #[serde(default)]
        pub(crate) minimal_output: Option<bool>,
    }

    /// Arguments for `web_explore_extract`.
    ///
    /// This is a bounded local crawl starting from a URL:
    /// fetch → extract → extract links → expand frontier (within bounds) → return merged top chunks.
    #[derive(Debug, Deserialize, JsonSchema, Default)]
    struct WebExploreExtractArgs {
        /// Starting URL (required).
        start_url: String,
        /// Optional query used for chunk scoring + frontier prioritization.
        #[serde(default)]
        query: Option<String>,
        /// Max pages to fetch/extract (default: 10; max: 50).
        #[serde(default)]
        max_pages: Option<usize>,
        /// Max crawl depth from start_url (default: 2; max: 5). Depth 0 means only start_url.
        #[serde(default)]
        max_depth: Option<usize>,
        /// If true, only follow links on the same host as start_url (default: true).
        #[serde(default)]
        same_host_only: Option<bool>,
        /// Max links to consider per page (default: 50; max: 500).
        #[serde(default)]
        max_links_per_page: Option<usize>,
        /// Fetch timeout per page (ms).
        #[serde(default)]
        timeout_ms: Option<u64>,
        /// Max bytes per page.
        #[serde(default)]
        max_bytes: Option<u64>,
        /// Width used for HTML->text extraction (default: 100).
        #[serde(default)]
        width: Option<usize>,
        /// Max chars of extracted text per page (default: 20_000; max: 200_000).
        #[serde(default)]
        max_chars: Option<usize>,
        /// Max chunks per page (default: 5; max: 50).
        #[serde(default)]
        top_chunks: Option<usize>,
        /// Max chars per chunk (default: 500; max: 5_000).
        #[serde(default)]
        max_chunk_chars: Option<usize>,
        /// If true, do not perform any network calls; fetch becomes cache-only.
        #[serde(default)]
        no_network: Option<bool>,
        #[serde(default)]
        cache_read: Option<bool>,
        #[serde(default)]
        cache_write: Option<bool>,
        #[serde(default)]
        cache_ttl_s: Option<u64>,
        /// If true, include per-page extracted links (default: false).
        #[serde(default)]
        include_links: Option<bool>,
    }

    /// Arguments for `repo_ingest`.
    ///
    /// This is gitingest-like functionality: turn a repo into a bounded text corpus.
    #[derive(Debug, Deserialize, JsonSchema, Default)]
    struct RepoIngestArgs {
        /// Repository URL (currently: GitHub-style https://github.com/<owner>/<repo>).
        repo_url: String,
        /// Optional ref/branch/tag (default: repo default_branch, else main/master).
        #[serde(default)]
        reference: Option<String>,
        /// Optional include patterns (glob-ish: `*` and `?`). If set, only matching files are kept.
        /// Examples: `["*.rs", "*.md", "src/*"]`.
        #[serde(default)]
        include_patterns: Option<Vec<String>>,
        /// Optional exclude patterns (glob-ish: `*` and `?`). Matching files are dropped.
        /// Examples: `["node_modules/*", "target/*", "*.min.js"]`.
        #[serde(default)]
        exclude_patterns: Option<Vec<String>>,
        /// Max files to ingest (default: 100; max: 500).
        #[serde(default)]
        max_files: Option<usize>,
        /// Max total bytes fetched across all files (default: 1_000_000; max: 10_000_000).
        #[serde(default)]
        max_total_bytes: Option<u64>,
        /// Max bytes per file fetch (default: 200_000; max: 1_000_000).
        #[serde(default)]
        max_file_bytes: Option<u64>,
        /// Fetch timeout per request (ms).
        #[serde(default)]
        timeout_ms: Option<u64>,
        /// If true, include combined_text (default: true).
        #[serde(default)]
        include_combined_text: Option<bool>,
        /// If true, do not perform any network calls (cache-only raw fetch; GitHub API calls are disabled).
        #[serde(default)]
        no_network: Option<bool>,
        #[serde(default)]
        cache_read: Option<bool>,
        #[serde(default)]
        cache_write: Option<bool>,
        #[serde(default)]
        cache_ttl_s: Option<u64>,
    }

    /// Arguments for `web_sitemap_extract`.
    ///
    /// This is a bounded “index discovery” tool:
    /// - fetch robots.txt (optional)
    /// - fetch sitemap URLs (robots + /sitemap.xml)
    /// - extract <loc> URLs (bounded)
    /// - optionally call `web_search_extract(urls=[...])` to return top_chunks
    #[derive(Debug, Deserialize, JsonSchema, Default)]
    struct WebSitemapExtractArgs {
        /// Site root URL (required). Example: https://docs.example.com/
        site_url: String,
        /// Optional query to run extraction/ranking over the discovered URLs.
        #[serde(default)]
        query: Option<String>,
        /// Max sitemap URLs to fetch (default: 3; max: 10).
        #[serde(default)]
        max_sitemaps: Option<usize>,
        /// Max URLs to take from sitemaps (default: 200; max: 2000).
        #[serde(default)]
        max_urls: Option<usize>,
        /// If true, only keep URLs that start with site_url (default: true).
        #[serde(default)]
        restrict_prefix: Option<bool>,
        /// If true, also try fetching /sitemap.xml even if robots.txt is missing (default: true).
        #[serde(default)]
        try_default_sitemap: Option<bool>,
        /// Fetch timeout per request (ms).
        #[serde(default)]
        timeout_ms: Option<u64>,
        /// Max bytes per request.
        #[serde(default)]
        max_bytes: Option<u64>,
        /// If true, do not perform any network calls (cache-only).
        #[serde(default)]
        no_network: Option<bool>,
        #[serde(default)]
        cache_read: Option<bool>,
        #[serde(default)]
        cache_write: Option<bool>,
        #[serde(default)]
        cache_ttl_s: Option<u64>,
        /// If true, run extract+rank over discovered URLs (default: true when query is non-empty).
        #[serde(default)]
        extract: Option<bool>,
        /// Bounds for extraction if extract=true.
        #[serde(default)]
        fetch_backend: Option<String>,
        #[serde(default)]
        max_results: Option<usize>,
        #[serde(default)]
        width: Option<usize>,
        #[serde(default)]
        max_chars: Option<usize>,
        #[serde(default)]
        top_chunks: Option<usize>,
        #[serde(default)]
        max_chunk_chars: Option<usize>,
    }

    /// Arguments for `web_deep_research`.
    ///
    /// This is an agentic evidence-gathering tool:
    /// - gathers evidence via `web_search_extract` (and optional ArXiv)
    /// - optionally runs a synthesis step (provider-backed)
    ///
    /// If you care about auditability, prefer `audit=true` and `include_evidence=true`.
    #[derive(Debug, Deserialize, JsonSchema, Default)]
    struct WebDeepResearchArgs {
        /// User question / research prompt.
        query: String,

        /// If true, enforce a strict “auditability” contract (default: false):
        /// - reject Perplexity browsing (`search_mode` must be unset)
        /// - require include_evidence=true so the evidence is returned
        #[serde(default)]
        audit: Option<bool>,

        /// If false, skip synthesis and return only the evidence pack (default: true).
        ///
        /// Useful for: debugging evidence quality, offline-ish evaluation, and keeping runs inspectable.
        #[serde(default)]
        synthesize: Option<bool>,

        /// Optional: skip web search and use these URLs as the evidence set (offline-friendly).
        #[serde(default)]
        urls: Option<Vec<String>>,

        /// Which search provider to use for evidence gathering (default: auto).
        #[serde(default)]
        provider: Option<String>,
        /// When provider="auto", choose routing mode (default: "fallback"). Allowed: fallback, merge
        #[serde(default)]
        auto_mode: Option<String>,
        /// How to select `top_chunks` across URLs (default: "score"). Allowed: score, pareto
        #[serde(default)]
        selection_mode: Option<String>,

        /// High-level width vs depth preset (optional).
        ///
        /// This does not override explicitly provided fields; it only fills in defaults.
        ///
        /// - "balanced" (default): current defaults
        /// - "wide": more breadth (more search + more evidence URLs) with shallower per-page extraction
        /// - "deep": fewer evidence URLs, deeper per-page extraction
        #[serde(default)]
        exploration: Option<String>,
        /// Which fetch backend to use for evidence gathering (default: local). Allowed: local, firecrawl
        #[serde(default)]
        fetch_backend: Option<String>,
        /// If true, do not perform any network calls. This requires urls=[...] (offline evidence only),
        /// and disables Perplexity browsing via search_mode.
        #[serde(default)]
        no_network: Option<bool>,
        /// Max search results to request (default: 5; max: 20).
        #[serde(default)]
        max_results: Option<usize>,
        /// Max URLs to actually fetch/extract (default: 3; max: 10).
        #[serde(default)]
        max_urls: Option<usize>,

        /// Fetch timeout per URL (ms).
        #[serde(default)]
        timeout_ms: Option<u64>,
        /// Max bytes per URL.
        #[serde(default)]
        max_bytes: Option<u64>,
        /// Width used for HTML->text extraction (default: 100).
        #[serde(default)]
        width: Option<usize>,
        /// Max chars of extracted text per URL (default: 20_000; max: 200_000).
        #[serde(default)]
        max_chars: Option<usize>,
        /// Max chunks per URL (default: 5; max: 50).
        #[serde(default)]
        top_chunks: Option<usize>,
        /// Max chars per chunk (default: 500; max: 5_000).
        #[serde(default)]
        max_chunk_chars: Option<usize>,
        /// Include extracted links (default: false).
        #[serde(default)]
        include_links: Option<bool>,
        /// Max links per URL (default: 25; max: 500).
        #[serde(default)]
        max_links: Option<usize>,

        /// Optionally include a bounded ArXiv search as extra evidence.
        ///
        /// This is separate from `web_search` providers (it uses ArXiv's Atom API).
        ///
        /// Allowed:
        /// - "off": never query ArXiv
        /// - "auto": query ArXiv only when the prompt looks paper-like
        /// - "on": always query ArXiv (unless no_network=true)
        #[serde(default)]
        arxiv_mode: Option<String>,
        /// Max ArXiv papers to include (default: 3; max: 10).
        #[serde(default)]
        arxiv_max_papers: Option<usize>,
        /// Optional ArXiv categories (e.g. ["cs.CL", "cs.LG"]).
        #[serde(default)]
        arxiv_categories: Option<Vec<String>>,
        /// Optional years filter for ArXiv (e.g. [2023, 2024]).
        #[serde(default)]
        arxiv_years: Option<Vec<u32>>,
        /// ArXiv query timeout (ms) (default: 8_000; max: 30_000).
        #[serde(default)]
        arxiv_timeout_ms: Option<u64>,

        /// Optionally include a bounded "papers" search (Semantic Scholar / OpenAlex / optional Google Scholar) as extra evidence.
        ///
        /// Allowed:
        /// - "off": never query paper backends
        /// - "auto": query only when the prompt looks paper-like
        /// - "on": always query (unless no_network=true)
        #[serde(default)]
        papers_mode: Option<String>,
        /// Which paper backends to query (default: ["semantic_scholar","openalex"]).
        #[serde(default)]
        papers_backends: Option<Vec<String>>,
        /// Max papers to include in the evidence pack (default: 5; max: 20).
        #[serde(default)]
        papers_max_papers: Option<usize>,
        /// Optional years filter for paper backends (e.g. [2023, 2024]).
        #[serde(default)]
        papers_years: Option<Vec<u32>>,
        /// Paper backend timeout (ms) (default: 12_000; max: 60_000).
        #[serde(default)]
        papers_timeout_ms: Option<u64>,
        /// If true, include abstract text when available (default: false).
        #[serde(default)]
        papers_include_abstract: Option<bool>,

        /// Perplexity model to use (default: sonar-deep-research).
        #[serde(default)]
        model: Option<String>,

        /// Model name for non-Perplexity synthesis backends (e.g. OpenAI-compatible local gateways).
        ///
        /// - Used by `llm_backend="openai_compat"`.
        /// - Ignored by `llm_backend="ollama"` (which uses WEBPIPE_OLLAMA_MODEL).
        #[serde(default)]
        llm_model: Option<String>,
        /// Perplexity: optional search mode (provider-side browsing). If set, Perplexity may use extra sources
        /// beyond the evidence pack, reducing inspectability/determinism.
        #[serde(default)]
        search_mode: Option<String>,
        /// Perplexity-specific: reasoning effort for deep research models (low|medium|high).
        #[serde(default)]
        reasoning_effort: Option<String>,
        #[serde(default)]
        max_tokens: Option<u64>,
        #[serde(default)]
        temperature: Option<f64>,
        #[serde(default)]
        top_p: Option<f64>,

        /// Bound answer text returned in this tool output (default: 20_000; max: 200_000).
        #[serde(default)]
        max_answer_chars: Option<usize>,

        /// Also include the evidence pack in the output (default: true).
        #[serde(default)]
        include_evidence: Option<bool>,

        /// Override "now" for deterministic outputs.
        #[serde(default)]
        now_epoch_s: Option<u64>,

        /// Which LLM backend to use for synthesis.
        ///
        /// - "auto" (default): Perplexity if configured (and no_network=false), else OpenAI-compatible if configured, else Ollama if enabled.
        /// - "perplexity": require Perplexity API (network-only).
        /// - "ollama": use local Ollama (best-effort; defaults to localhost).
        /// - "openai_compat": call an OpenAI-compatible `/v1/chat/completions` endpoint (works well with `axi-gateway`).
        #[serde(default)]
        llm_backend: Option<String>,
    }

    #[derive(Debug, Clone)]
    struct ChunkCandidate {
        url: String,
        score: u64,
        start_char: usize,
        end_char: usize,
        text: String,
        warning_penalty: i64,
        cache_hit: bool,
    }

    #[derive(Debug, Clone, Default, serde::Serialize)]
    struct ProviderUsage {
        calls: u64,
        ok: u64,
        cost_units: u64,
        elapsed_ms_sum: u64,
        http_429: u64,
    }

    #[derive(Debug, Clone, serde::Serialize)]
    struct UsageStats {
        started_at_epoch_s: u64,
        tool_calls: std::collections::BTreeMap<String, u64>,
        search_providers: std::collections::BTreeMap<String, ProviderUsage>,
        #[serde(skip)]
        search_windows: std::collections::BTreeMap<String, muxer::Window>,
        #[serde(skip)]
        search_windows_by_query_key:
            std::collections::BTreeMap<String, std::collections::BTreeMap<String, muxer::Window>>,
        #[serde(skip)]
        routing_last_seen_tick: u64,
        #[serde(skip)]
        routing_query_last_seen: std::collections::BTreeMap<String, u64>,
        search_window_cap: usize,
        routing_max_contexts: usize,
        #[serde(skip)]
        routing_context: RoutingContext,
        llm_backends: std::collections::BTreeMap<String, ProviderUsage>,
        fetch_backends: std::collections::BTreeMap<String, ProviderUsage>,
        warning_counts: std::collections::BTreeMap<String, u64>,
    }

    #[derive(Debug, Clone, Copy, PartialEq, Eq)]
    enum RoutingContext {
        Global,
        QueryKey,
        Both,
    }

    impl UsageStats {
        fn new(now_epoch_s: u64) -> Self {
            let cap = std::env::var("WEBPIPE_ROUTING_WINDOW")
                .ok()
                .and_then(|v| v.trim().parse::<usize>().ok())
                .unwrap_or(50)
                .clamp(1, 500);
            let routing_max_contexts = std::env::var("WEBPIPE_ROUTING_MAX_CONTEXTS")
                .ok()
                .and_then(|v| v.trim().parse::<usize>().ok())
                .unwrap_or(200)
                .clamp(1, 2000);
            let routing_context = match std::env::var("WEBPIPE_ROUTING_CONTEXT")
                .ok()
                .unwrap_or_else(|| "global".to_string())
                .trim()
            {
                "query_key" | "querykey" | "qk" => RoutingContext::QueryKey,
                "both" => RoutingContext::Both,
                _ => RoutingContext::Global,
            };
            Self {
                started_at_epoch_s: now_epoch_s,
                tool_calls: std::collections::BTreeMap::new(),
                search_providers: std::collections::BTreeMap::new(),
                search_windows: std::collections::BTreeMap::new(),
                search_windows_by_query_key: std::collections::BTreeMap::new(),
                routing_last_seen_tick: 0,
                routing_query_last_seen: std::collections::BTreeMap::new(),
                search_window_cap: cap,
                routing_max_contexts,
                routing_context,
                llm_backends: std::collections::BTreeMap::new(),
                fetch_backends: std::collections::BTreeMap::new(),
                warning_counts: std::collections::BTreeMap::new(),
            }
        }
    }

    fn now_epoch_s() -> u64 {
        std::time::SystemTime::now()
            .duration_since(std::time::UNIX_EPOCH)
            .unwrap_or_else(|_| std::time::Duration::from_secs(0))
            .as_secs()
    }

    #[derive(Clone)]
    pub(crate) struct WebpipeMcp {
        prompt_router: RmcpPromptRouter<Self>,
        tool_router: RmcpToolRouter<Self>,
        fetcher: Arc<LocalFetcher>,
        http: reqwest::Client,
        stats: Arc<std::sync::Mutex<UsageStats>>,
    }

    #[tool_router]
    impl WebpipeMcp {
        pub(crate) fn new() -> Result<Self, McpError> {
            let cache_dir = cache_dir_from_env().or_else(|| Some(default_cache_dir()));
            let fetcher = LocalFetcher::new(cache_dir)
                .map_err(|e| McpError::internal_error(e.to_string(), None))?;
            let privacy = privacy_mode_from_env();
            let mut httpb = reqwest::Client::builder().user_agent("webpipe-mcp/0.1");
            if privacy == PrivacyMode::Anonymous {
                if let Some(p) = anon_proxy_from_env() {
                    httpb = httpb.proxy(reqwest::Proxy::all(p).map_err(|e| {
                        McpError::invalid_params(
                            format!("invalid proxy url: {e}"),
                            Some(serde_json::json!({"code":"invalid_proxy"})),
                        )
                    })?);
                }
            }
            Ok(Self {
                prompt_router: Self::prompt_router(),
                tool_router: Self::tool_router(),
                fetcher: Arc::new(fetcher),
                http: httpb
                    .build()
                    .map_err(|e| McpError::internal_error(e.to_string(), None))?,
                stats: Arc::new(std::sync::Mutex::new(UsageStats::new(now_epoch_s()))),
            })
        }

        fn stats_lock(&self) -> std::sync::MutexGuard<'_, UsageStats> {
            self.stats.lock().unwrap_or_else(|e| e.into_inner())
        }

        fn stats_inc_tool(&self, kind: &str) {
            let mut s = self.stats_lock();
            *s.tool_calls.entry(kind.to_string()).or_insert(0) += 1;
        }

        fn stats_record_provider(
            map: &mut std::collections::BTreeMap<String, ProviderUsage>,
            name: &str,
            ok: bool,
            cost_units: u64,
            elapsed_ms: u64,
            http_429: bool,
        ) {
            let entry = map.entry(name.to_string()).or_default();
            entry.calls += 1;
            if ok {
                entry.ok += 1;
            }
            entry.cost_units = entry.cost_units.saturating_add(cost_units);
            entry.elapsed_ms_sum = entry.elapsed_ms_sum.saturating_add(elapsed_ms);
            if http_429 {
                entry.http_429 += 1;
            }
        }

        fn stats_record_search_provider_qk(
            &self,
            name: &str,
            ok: bool,
            cost_units: u64,
            elapsed_ms: u64,
            err: Option<&str>,
            query_key: Option<&str>,
        ) {
            let http_429 = err.is_some_and(|m| is_http_status(m, 429));
            let mut s = self.stats_lock();
            Self::stats_record_provider(
                &mut s.search_providers,
                name,
                ok,
                cost_units,
                elapsed_ms,
                http_429,
            );
            let cap = s.search_window_cap;
            let record_global = matches!(
                s.routing_context,
                RoutingContext::Global | RoutingContext::Both
            );
            let record_qk = matches!(
                s.routing_context,
                RoutingContext::QueryKey | RoutingContext::Both
            ) && query_key.is_some_and(|qk| !qk.trim().is_empty());

            if record_global {
                let w = s
                    .search_windows
                    .entry(name.to_string())
                    .or_insert_with(|| muxer::Window::new(cap));
                w.push(muxer::Outcome {
                    ok,
                    http_429,
                    junk: false,
                    hard_junk: false,
                    cost_units,
                    elapsed_ms,
                });
            }

            if record_qk {
                let qk = query_key.unwrap().trim().to_string();
                s.routing_last_seen_tick = s.routing_last_seen_tick.saturating_add(1);
                let tick = s.routing_last_seen_tick;
                s.routing_query_last_seen.insert(qk.clone(), tick);
                let per_qk = s.search_windows_by_query_key.entry(qk.clone()).or_default();
                let w = per_qk
                    .entry(name.to_string())
                    .or_insert_with(|| muxer::Window::new(cap));
                w.push(muxer::Outcome {
                    ok,
                    http_429,
                    junk: false,
                    hard_junk: false,
                    cost_units,
                    elapsed_ms,
                });

                while s.search_windows_by_query_key.len() > s.routing_max_contexts {
                    let Some((evict_qk, _tick)) = s
                        .routing_query_last_seen
                        .iter()
                        .min_by_key(|(_k, v)| *v)
                        .map(|(k, v)| (k.clone(), *v))
                    else {
                        break;
                    };
                    s.search_windows_by_query_key.remove(&evict_qk);
                    s.routing_query_last_seen.remove(&evict_qk);
                }
            }
        }

        fn cosine_similarity(a: &[f32], b: &[f32]) -> f32 {
            if a.is_empty() || b.is_empty() || a.len() != b.len() {
                return 0.0;
            }
            let mut dot = 0.0f64;
            let mut na = 0.0f64;
            let mut nb = 0.0f64;
            for i in 0..a.len() {
                let x = a[i] as f64;
                let y = b[i] as f64;
                dot += x * y;
                na += x * x;
                nb += y * y;
            }
            if na <= 0.0 || nb <= 0.0 {
                return 0.0;
            }
            (dot / (na.sqrt() * nb.sqrt())) as f32
        }

        async fn semantic_rerank_chunks_best(
            &self,
            query: &str,
            candidates: &[(usize, usize, String)],
            top_k: usize,
        ) -> webpipe_local::semantic::SemanticRerankResult {
            let t0 = std::time::Instant::now();
            let top_k = top_k.max(1);
            let q = query.trim();
            if q.is_empty() || candidates.is_empty() {
                return webpipe_local::semantic::semantic_rerank_chunks(query, candidates, top_k);
            }

            // Best-effort: if OpenRouter embeddings is configured, use it; otherwise fall back
            // to the local lexical overlap scorer.
            let Some(api_key) = Self::openrouter_api_key_from_env() else {
                return webpipe_local::semantic::semantic_rerank_chunks(query, candidates, top_k);
            };

            let model = Self::openrouter_embeddings_model_from_env();
            let timeout_ms = Self::openrouter_embeddings_timeout_ms_from_env();
            let max_inputs = Self::semantic_embeddings_max_inputs_from_env();

            // Prefilter (bounded): we only embed up to max_inputs candidates.
            let mut pre =
                webpipe_local::semantic::semantic_rerank_chunks(query, candidates, max_inputs);
            let mut warnings: Vec<&'static str> = Vec::new();
            if candidates.len() > max_inputs {
                warnings.push("semantic_embeddings_prefiltered");
            }

            // Prepare inputs: [query, chunk1, chunk2, ...]
            let mut inputs: Vec<String> = Vec::with_capacity(pre.chunks.len() + 1);
            inputs.push(q.to_string());
            for ch in &pre.chunks {
                // Keep embeddings inputs bounded and deterministic.
                let t: String = ch.text.chars().take(1200).collect();
                inputs.push(t);
            }
            // Proxy “cost units”: number of embeddings requested (query + each candidate).
            // We intentionally do not try to guess tokens/$ pricing here.
            let embedding_units = inputs.len() as u64;

            let client = match webpipe_local::openai_compat::OpenAiCompatClient::new(
                self.http.clone(),
                // Client appends `/v1/embeddings`.
                "https://openrouter.ai/api".to_string(),
                Some(api_key),
                model.clone(),
            ) {
                Ok(c) => c,
                Err(_) => {
                    warnings.push("semantic_embeddings_client_not_configured");
                    let mut out =
                        webpipe_local::semantic::semantic_rerank_chunks(query, candidates, top_k);
                    out.warnings.extend(warnings);
                    return out;
                }
            };

            let embs = match client.embeddings(inputs, timeout_ms).await {
                Ok(v) => v,
                Err(_) => {
                    self.stats_record_llm_backend(
                        "openrouter_embeddings",
                        false,
                        t0.elapsed().as_millis() as u64,
                        None,
                    );
                    warnings.push("semantic_embeddings_failed_fallback_to_lexical");
                    let mut out =
                        webpipe_local::semantic::semantic_rerank_chunks(query, candidates, top_k);
                    out.warnings.extend(warnings);
                    return out;
                }
            };

            if embs.len() != pre.chunks.len() + 1 {
                self.stats_record_llm_backend(
                    "openrouter_embeddings",
                    false,
                    t0.elapsed().as_millis() as u64,
                    None,
                );
                warnings.push("semantic_embeddings_bad_shape_fallback_to_lexical");
                let mut out =
                    webpipe_local::semantic::semantic_rerank_chunks(query, candidates, top_k);
                out.warnings.extend(warnings);
                return out;
            }

            let q_emb = &embs[0];
            for (i, ch) in pre.chunks.iter_mut().enumerate() {
                ch.score = Self::cosine_similarity(q_emb, &embs[i + 1]);
            }
            pre.chunks.sort_by(|a, b| {
                b.score
                    .partial_cmp(&a.score)
                    .unwrap_or(std::cmp::Ordering::Equal)
                    .then_with(|| a.start_char.cmp(&b.start_char))
                    .then_with(|| a.end_char.cmp(&b.end_char))
            });
            pre.chunks.truncate(top_k);

            pre.backend = "openrouter_embeddings".to_string();
            pre.model_id = Some(model);
            pre.warnings.extend(warnings);
            self.stats_record_llm_backend_units(
                "openrouter_embeddings",
                true,
                embedding_units,
                t0.elapsed().as_millis() as u64,
                None,
            );
            pre
        }

        fn stats_set_last_search_outcome_junk_level_qk(
            &self,
            name: &str,
            junk: bool,
            hard_junk: bool,
            query_key: Option<&str>,
        ) {
            let mut s = self.stats_lock();
            if matches!(
                s.routing_context,
                RoutingContext::Global | RoutingContext::Both
            ) {
                if let Some(w) = s.search_windows.get_mut(name) {
                    w.set_last_junk_level(junk, hard_junk);
                }
            }
            if matches!(
                s.routing_context,
                RoutingContext::QueryKey | RoutingContext::Both
            ) && query_key.is_some_and(|qk| !qk.trim().is_empty())
            {
                let qk = query_key.unwrap().trim();
                if let Some(per_qk) = s.search_windows_by_query_key.get_mut(qk) {
                    if let Some(w) = per_qk.get_mut(name) {
                        w.set_last_junk_level(junk, hard_junk);
                    }
                }
            }
        }

        fn snapshot_search_summaries_for_query_key(
            &self,
            query_key: Option<&str>,
        ) -> (
            std::collections::BTreeMap<String, muxer::Summary>,
            &'static str,
        ) {
            let s = self.stats_lock();
            let global = || {
                let mut m = std::collections::BTreeMap::new();
                for (k, w) in &s.search_windows {
                    m.insert(k.clone(), w.summary());
                }
                (m, "global")
            };
            match s.routing_context {
                RoutingContext::Global => global(),
                RoutingContext::QueryKey | RoutingContext::Both => {
                    if let Some(qk) = query_key.map(|x| x.trim()).filter(|x| !x.is_empty()) {
                        if let Some(per_qk) = s.search_windows_by_query_key.get(qk) {
                            let mut m = std::collections::BTreeMap::new();
                            for (k, w) in per_qk {
                                m.insert(k.clone(), w.summary());
                            }
                            return (m, "query_key");
                        }
                    }
                    global()
                }
            }
        }

        fn compute_search_junk_label(
            hard_junk_urls: usize,
            soft_junk_urls: usize,
            total_urls_ok: usize,
        ) -> bool {
            if total_urls_ok == 0 {
                return false;
            }
            if hard_junk_urls > 0 {
                return true;
            }
            let min_soft = std::env::var("WEBPIPE_ROUTING_SOFT_JUNK_MIN")
                .ok()
                .and_then(|v| v.trim().parse::<usize>().ok())
                .unwrap_or(2)
                .clamp(1, 50);
            let frac = std::env::var("WEBPIPE_ROUTING_SOFT_JUNK_FRAC")
                .ok()
                .and_then(|v| v.trim().parse::<f64>().ok())
                .unwrap_or(0.5)
                .clamp(0.0, 1.0);
            if soft_junk_urls < min_soft {
                return false;
            }
            (soft_junk_urls as f64) >= frac * (total_urls_ok as f64)
        }

        fn stats_record_llm_backend(
            &self,
            name: &str,
            ok: bool,
            elapsed_ms: u64,
            err: Option<&str>,
        ) {
            let http_429 = err.is_some_and(|m| is_http_status(m, 429));
            let mut s = self.stats_lock();
            Self::stats_record_provider(&mut s.llm_backends, name, ok, 0, elapsed_ms, http_429);
        }

        fn stats_record_llm_backend_units(
            &self,
            name: &str,
            ok: bool,
            cost_units: u64,
            elapsed_ms: u64,
            err: Option<&str>,
        ) {
            let http_429 = err.is_some_and(|m| is_http_status(m, 429));
            let mut s = self.stats_lock();
            Self::stats_record_provider(
                &mut s.llm_backends,
                name,
                ok,
                cost_units,
                elapsed_ms,
                http_429,
            );
        }

        fn stats_record_fetch_backend(
            &self,
            name: &str,
            ok: bool,
            elapsed_ms: u64,
            err: Option<&str>,
        ) {
            let http_429 = err.is_some_and(|m| is_http_status(m, 429));
            let mut s = self.stats_lock();
            Self::stats_record_provider(&mut s.fetch_backends, name, ok, 0, elapsed_ms, http_429);
        }

        #[allow(clippy::too_many_arguments)]
        async fn maybe_rewrite_github_repo_url(
            &self,
            url: &str,
            timeout_ms: u64,
            max_bytes: u64,
            no_network: bool,
            cache_read: bool,
            cache_write: bool,
            cache_ttl_s: Option<u64>,
        ) -> (String, Option<serde_json::Value>) {
            let Some(cands) = webpipe_local::rewrite::github_repo_raw_readme_candidates(url) else {
                return (url.to_string(), None);
            };
            // Bounded: try a small number of candidates.
            let mut tried: Vec<serde_json::Value> = Vec::new();
            for cand in cands.into_iter().take(6) {
                let req = FetchRequest {
                    url: cand.clone(),
                    timeout_ms: Some(timeout_ms.min(10_000)),
                    max_bytes: Some(max_bytes.min(1_000_000)),
                    headers: BTreeMap::new(),
                    cache: FetchCachePolicy {
                        read: cache_read || no_network,
                        write: if no_network { false } else { cache_write },
                        ttl_s: cache_ttl_s,
                    },
                };
                let t0 = std::time::Instant::now();
                let resp = if no_network && !url_is_localhost(&req.url) {
                    self.fetcher.cache_get(&req).ok().flatten()
                } else {
                    self.fetcher.fetch(&req).await.ok()
                };
                match resp {
                    Some(r) if (200..300).contains(&(r.status as i32)) => {
                        tried.push(serde_json::json!({
                            "url": cand,
                            "ok": true,
                            "status": r.status,
                            "content_type": r.content_type,
                            "elapsed_ms": t0.elapsed().as_millis()
                        }));
                        let attempts = serde_json::json!({
                            "kind": "github_repo_raw_readme",
                            "from": url,
                            "to": r.url,
                            "tried": tried
                        });
                        return (r.url, Some(attempts));
                    }
                    Some(r) => {
                        tried.push(serde_json::json!({
                            "url": cand,
                            "ok": false,
                            "status": r.status,
                            "content_type": r.content_type,
                            "elapsed_ms": t0.elapsed().as_millis()
                        }));
                    }
                    None => {
                        tried.push(serde_json::json!({
                            "url": cand,
                            "ok": false,
                            "error": if no_network { "cache_miss_or_error" } else { "fetch_failed" },
                            "elapsed_ms": t0.elapsed().as_millis()
                        }));
                    }
                }
            }
            (
                url.to_string(),
                Some(serde_json::json!({
                    "kind": "github_repo_raw_readme",
                    "from": url,
                    "to": url,
                    "tried": tried
                })),
            )
        }

        fn maybe_rewrite_github_blob_url(&self, url: &str) -> (String, Option<serde_json::Value>) {
            let Some(cands) = webpipe_local::rewrite::github_blob_raw_candidates(url) else {
                return (url.to_string(), None);
            };
            let to = cands.into_iter().next().unwrap_or_else(|| url.to_string());
            if to == url {
                return (url.to_string(), None);
            }
            (
                to.clone(),
                Some(serde_json::json!({
                    "kind": "github_blob_to_raw",
                    "from": url,
                    "to": to
                })),
            )
        }

        fn maybe_rewrite_github_pr_or_commit_url(
            &self,
            url: &str,
        ) -> (String, Option<serde_json::Value>, Option<&'static str>) {
            if let Some(mut c) = webpipe_local::rewrite::github_pr_patch_candidates(url) {
                let to = c.remove(0);
                return (
                    to.clone(),
                    Some(serde_json::json!({
                        "kind": "github_pr_to_patch",
                        "from": url,
                        "to": to,
                        "candidates": c
                    })),
                    Some("github_pr_rewritten_to_patch"),
                );
            }
            if let Some(mut c) = webpipe_local::rewrite::github_commit_patch_candidates(url) {
                let to = c.remove(0);
                return (
                    to.clone(),
                    Some(serde_json::json!({
                        "kind": "github_commit_to_patch",
                        "from": url,
                        "to": to,
                        "candidates": c
                    })),
                    Some("github_commit_rewritten_to_patch"),
                );
            }
            (url.to_string(), None, None)
        }

        fn maybe_rewrite_gist_url(&self, url: &str) -> (String, Option<serde_json::Value>) {
            let Some(cands) = webpipe_local::rewrite::gist_raw_candidates(url) else {
                return (url.to_string(), None);
            };
            let to = cands.into_iter().next().unwrap_or_else(|| url.to_string());
            if to == url {
                return (url.to_string(), None);
            }
            (
                to.clone(),
                Some(serde_json::json!({
                    "kind": "gist_to_raw",
                    "from": url,
                    "to": to
                })),
            )
        }

        fn github_api_base_from_env() -> String {
            std::env::var("WEBPIPE_GITHUB_API_BASE")
                .ok()
                .unwrap_or_else(|| "https://api.github.com".to_string())
                .trim()
                .trim_end_matches('/')
                .to_string()
        }

        fn github_token_from_env() -> Option<String> {
            // Never log/echo this; only use to set Authorization headers.
            for k in ["WEBPIPE_GITHUB_TOKEN", "GITHUB_TOKEN"] {
                if let Ok(v) = std::env::var(k) {
                    let t = v.trim().to_string();
                    if !t.is_empty() {
                        return Some(t);
                    }
                }
            }
            None
        }

        fn maybe_rewrite_github_issue_url(
            &self,
            url: &str,
        ) -> (String, Option<serde_json::Value>, bool) {
            let api_base = Self::github_api_base_from_env();
            let Some(cands) = webpipe_local::rewrite::github_issue_api_candidates(url, &api_base)
            else {
                return (url.to_string(), None, false);
            };
            let to = cands.into_iter().next().unwrap_or_else(|| url.to_string());
            if to == url {
                return (url.to_string(), None, false);
            }
            (
                to.clone(),
                Some(serde_json::json!({
                    "kind": "github_issue_to_api",
                    "from": url,
                    "to": to
                })),
                true,
            )
        }

        fn maybe_rewrite_github_release_url(
            &self,
            url: &str,
        ) -> (String, Option<serde_json::Value>, bool) {
            let api_base = Self::github_api_base_from_env();
            let Some(cands) = webpipe_local::rewrite::github_release_api_candidates(url, &api_base)
            else {
                return (url.to_string(), None, false);
            };
            let to = cands.into_iter().next().unwrap_or_else(|| url.to_string());
            if to == url {
                return (url.to_string(), None, false);
            }
            (
                to.clone(),
                Some(serde_json::json!({
                    "kind": "github_release_to_api",
                    "from": url,
                    "to": to
                })),
                true,
            )
        }

        fn maybe_rewrite_arxiv_abs_url(&self, url: &str) -> (String, Option<serde_json::Value>) {
            let Some(cands) = webpipe_local::rewrite::arxiv_abs_pdf_candidates(url) else {
                return (url.to_string(), None);
            };
            let to = cands.into_iter().next().unwrap_or_else(|| url.to_string());
            if to == url {
                return (url.to_string(), None);
            }
            (
                to.clone(),
                Some(serde_json::json!({
                    "kind": "arxiv_abs_to_pdf",
                    "from": url,
                    "to": to
                })),
            )
        }

        fn stats_record_warnings(&self, warnings: &[&'static str]) {
            let mut s = self.stats_lock();
            for &w in warnings {
                *s.warning_counts
                    .entry(normalize_warning_code(w).to_string())
                    .or_insert(0) += 1;
            }
        }

        fn truncate_to_chars(s: &str, max_chars: usize) -> (String, usize, bool) {
            if max_chars == 0 {
                return (String::new(), 0, !s.is_empty());
            }
            // Find a valid UTF-8 boundary for the first `max_chars` chars, in one pass.
            let mut n = 0usize;
            let mut cut = s.len();
            for (i, _) in s.char_indices() {
                if n == max_chars {
                    cut = i;
                    break;
                }
                n += 1;
            }
            let clipped = cut < s.len();
            // If we clipped, we know we produced exactly `max_chars` chars.
            let n_out = if clipped { max_chars } else { n };
            (s[..cut].to_string(), n_out, clipped)
        }

        fn approx_bytes_len(s: &str) -> usize {
            s.len()
        }

        fn content_type_is_pdf(ct: Option<&str>) -> bool {
            ct.unwrap_or("")
                .trim()
                .to_ascii_lowercase()
                .starts_with("application/pdf")
        }

        fn url_looks_like_pdf(url: &str) -> bool {
            let u = url.trim();
            if u.is_empty() {
                return false;
            }
            if let Ok(p) = reqwest::Url::parse(u) {
                let path_lc = p.path().to_ascii_lowercase();
                // Common forms:
                // - .../file.pdf
                // - .../pdf/... (many hosts, including arXiv)
                path_lc.ends_with(".pdf") || path_lc.contains("/pdf/")
            } else {
                let u_lc = u.to_ascii_lowercase();
                u_lc.ends_with(".pdf") || u_lc.contains("/pdf/")
            }
        }

        fn looks_like_bundle_gunk(s: &str) -> bool {
            // Common “JS app shell” signatures we saw in the wild (e.g. Next.js hydration payloads).
            let t = s.trim();
            if t.is_empty() {
                return false;
            }
            let lc = t.to_ascii_lowercase();
            // Check for common blocking/challenge messages
            if lc.contains("please enable javascript")
                || lc.contains("enable javascript to continue")
                || lc.contains("javascript is required")
                || lc.contains("browser does not support javascript")
                || lc.contains("checking your browser")
                || lc.contains("cloudflare") && lc.contains("ray id")
                || lc.contains("verify you are human")
                || lc.contains("pardon our interruption")
                || lc.contains("turn on javascript")
                || lc.contains("cookie") && lc.contains("consent") && t.len() < 1000
                || lc.contains("403 forbidden") && t.len() < 500
                || lc.contains("access denied") && t.len() < 500
            {
                return true;
            }

            lc.contains("self.__next_s")
                || lc.contains("suppresshydrationwarning")
                || lc.contains("__webpack")
                || lc.contains("webpackchunk")
                || lc.contains("window.__nuxt")
                || lc.contains("(function(a,b,c,d)")
                || lc.contains(".push([0,{\"suppresshydrationwarning\"")
        }

        fn chunk_is_low_signal(c: &webpipe_local::extract::ScoredChunk) -> bool {
            let t = c.text.trim();
            if t.is_empty() {
                return true;
            }
            if Self::looks_like_bundle_gunk(t) {
                return true;
            }
            // Lightweight “readability” heuristic:
            // penalize chunks that are mostly punctuation/escaped JSON.
            let mut letters = 0usize;
            let mut digits = 0usize;
            let mut spaces = 0usize;
            let mut other = 0usize;
            for ch in t.chars() {
                if ch.is_alphabetic() {
                    letters += 1;
                } else if ch.is_ascii_digit() {
                    digits += 1;
                } else if ch.is_whitespace() {
                    spaces += 1;
                } else {
                    other += 1;
                }
            }
            let denom = letters + digits + spaces + other;
            if denom < 120 {
                return false;
            }
            let alpha_like = letters + spaces;
            // If < ~1/3 of chars are alphabetic/space, this is usually “gunk”.
            alpha_like.saturating_mul(3) < denom
        }

        fn filter_low_signal_chunks(
            chunks: Vec<webpipe_local::extract::ScoredChunk>,
        ) -> (Vec<webpipe_local::extract::ScoredChunk>, bool) {
            if chunks.is_empty() {
                return (chunks, false);
            }
            let mut out: Vec<webpipe_local::extract::ScoredChunk> = chunks
                .iter()
                .filter(|c| !Self::chunk_is_low_signal(c))
                .cloned()
                .collect();
            // Never return an empty list if we had chunks; fall back to original.
            //
            // Important: still report `filtered=true` in this case. “All chunks look low-signal”
            // is exactly the scenario where callers need a warning, even if we must fail-open.
            if out.is_empty() {
                out = chunks;
                return (out, true);
            }
            let filtered = out.len() < chunks.len();
            (out, filtered)
        }

        fn warning_penalty(warnings: &[&'static str]) -> i64 {
            // Heuristic severity weights. These are intentionally small and stable:
            // - hard blockers/challenges should dominate
            // - soft quality degradations are additive but not overwhelming
            // - informational warnings (cache_only) should not penalize
            let mut p: i64 = 0;
            for w in warnings {
                match normalize_warning_code(w) {
                    "blocked_by_js_challenge" => p += 100,
                    "empty_extraction" => p += 80,
                    "http_status_error" => p += 60,
                    "http_rate_limited" => p += 60,
                    "main_content_low_signal" => p += 25,
                    "chunks_filtered_low_signal" => p += 15,
                    "body_truncated_by_max_bytes" => p += 12,
                    "text_truncated_by_max_chars" => p += 8,
                    "retried_due_to_truncation" => p += 3,
                    "truncation_retry_used" => p += 3,
                    "truncation_retry_failed" => p += 8,
                    "pdf_extract_failed" => p += 15,
                    "pdf_extract_panicked" => p += 20,
                    "pdf_strings_fallback_used" => p += 10,
                    "pdf_shellout_used" => p += 2,
                    "pdf_shellout_unavailable" => p += 15,
                    "arxiv_pdf_fallback_to_html" => p += 0,
                    "openreview_pdf_fallback_to_forum" => p += 0,
                    "openreview_pdf_fallback_to_api" => p += 0,
                    "pandoc_failed" => p += 12,
                    "image_no_text_extraction" => p += 30,
                    "media_no_text_extraction" => p += 30,
                    "unsafe_request_headers_dropped" => p += 0,
                    "cache_only" => p += 0,
                    _ => p += 1,
                }
            }
            p
        }

        fn tokenize_for_overlap(query: &str) -> Vec<String> {
            // Deterministic, lightweight tokenization for quality judging.
            // Keep it similar to chunk match tokenization, but local and bounded.
            let q = textprep::scrub(query);
            let mut out: Vec<String> = q
                .split(|ch: char| !ch.is_alphanumeric())
                .filter_map(|t| {
                    let t = t.trim();
                    (t.len() >= 3).then_some(t.to_string())
                })
                .collect();
            out.sort();
            out.dedup();
            out.truncate(50);
            out
        }

        fn quality_scorecard(
            query: Option<&str>,
            extracted_text: &str,
            status: u16,
            extraction_engine: &str,
            warnings: &[&'static str],
        ) -> serde_json::Value {
            let text_chars = extracted_text.chars().count();
            let nonempty = extracted_text.chars().any(|c| !c.is_whitespace());
            let js_challenge = nonempty && looks_like_js_challenge(status, extracted_text, None);
            let bundle_gunk = nonempty && Self::looks_like_bundle_gunk(extracted_text);
            let has_empty = warnings
                .iter()
                .any(|w| normalize_warning_code(w) == "empty_extraction");
            let has_low_signal = warnings
                .iter()
                .any(|w| normalize_warning_code(w) == "main_content_low_signal")
                || warnings
                    .iter()
                    .any(|w| normalize_warning_code(w) == "chunks_filtered_low_signal")
                || bundle_gunk
                || js_challenge;

            let mut q_tokens = Vec::new();
            let mut overlap = 0usize;
            if let Some(q) = query.map(|s| s.trim()).filter(|s| !s.is_empty()) {
                q_tokens = Self::tokenize_for_overlap(q);
                if nonempty && !q_tokens.is_empty() {
                    let t_scrub = textprep::scrub(extracted_text);
                    for qt in &q_tokens {
                        if t_scrub.contains(qt) {
                            overlap += 1;
                        }
                    }
                }
            }

            let mut issues: Vec<&'static str> = Vec::new();
            if !nonempty || has_empty {
                issues.push("empty");
            }
            if js_challenge {
                issues.push("js_challenge");
            }
            if bundle_gunk {
                issues.push("gunk");
            }
            if warnings
                .iter()
                .any(|w| normalize_warning_code(w) == "main_content_low_signal")
            {
                issues.push("boilerplate");
            }
            if warnings
                .iter()
                .any(|w| normalize_warning_code(w) == "text_truncated_by_max_chars")
            {
                issues.push("text_truncated");
            }
            if warnings
                .iter()
                .any(|w| normalize_warning_code(w) == "body_truncated_by_max_bytes")
            {
                issues.push("body_truncated");
            }
            if query.is_some_and(|q| !q.trim().is_empty()) && nonempty && overlap == 0 {
                issues.push("no_query_overlap");
            }
            if nonempty && text_chars < 200 && extraction_engine != "html_hint" {
                issues.push("too_short");
            }

            issues.sort();
            issues.dedup();

            // Deterministic score heuristic: start from 100, subtract issue penalties.
            let mut score: i64 = 100;
            if issues.contains(&"empty") {
                score -= 100;
            }
            if issues.contains(&"js_challenge") {
                score -= 60;
            }
            if issues.contains(&"gunk") {
                score -= 45;
            }
            if issues.contains(&"boilerplate") {
                score -= 20;
            }
            if issues.contains(&"no_query_overlap") {
                score -= 25;
            }
            if issues.contains(&"too_short") {
                score -= 15;
            }
            if issues.contains(&"text_truncated") {
                score -= 10;
            }
            if issues.contains(&"body_truncated") {
                score -= 10;
            }
            score = score.clamp(0, 100);

            // “ok” is conservative: requires nonempty and not obviously junk/challenge.
            let ok = score >= 60 && nonempty && !js_challenge && !bundle_gunk;

            serde_json::json!({
                "schema_version": 1,
                "kind": "webpipe_extract_quality",
                "ok": ok,
                "score": score,
                "issues": issues,
                "signals": {
                    "text_chars": text_chars,
                    "nonempty": nonempty,
                    "query_token_count": q_tokens.len(),
                    "query_overlap": overlap,
                    "has_low_signal": has_low_signal,
                    "js_challenge": js_challenge,
                    "bundle_gunk": bundle_gunk,
                    "status": status,
                    "engine": extraction_engine
                }
            })
        }

        fn score_key(score: u64) -> i64 {
            // Keep ordering stable without float total-order deps.
            // (Scores come from our own chunker; treat as an integer signal.)
            score.min(i64::MAX as u64) as i64
        }

        fn select_top_chunks(
            mut candidates: Vec<ChunkCandidate>,
            top_k: usize,
            selection_mode: &str,
        ) -> Vec<ChunkCandidate> {
            if candidates.is_empty() || top_k == 0 {
                return Vec::new();
            }

            match selection_mode {
                "pareto" => {
                    // Multi-objective selection:
                    // - maximize score
                    // - prefer cache hits (cheaper / more reproducible)
                    // - prefer lower warning severity (higher reliability)
                    // - prefer shorter chunks (bounded evidence packs)
                    let metrics: Vec<Vec<f32>> = candidates
                        .iter()
                        .map(|c| {
                            let score = c.score as f32;
                            let cache = if c.cache_hit { 1.0 } else { 0.0 };
                            let warnings = -(c.warning_penalty as f32);
                            let len = -(c.text.chars().count() as f32);
                            vec![score, cache, warnings, len]
                        })
                        .collect();

                    let frontier = pare::pareto_indices(&metrics);
                    let mut picked: Vec<ChunkCandidate> = Vec::new();

                    if let Some(frontier) = frontier {
                        let mut idxs = frontier;
                        // Stable, deterministic tie-break: high score, then lower warning penalty, then cache, then URL.
                        idxs.sort_by(|&ia, &ib| {
                            let a = &candidates[ia];
                            let b = &candidates[ib];
                            let ka = Self::score_key(a.score);
                            let kb = Self::score_key(b.score);
                            kb.cmp(&ka)
                                .then_with(|| a.warning_penalty.cmp(&b.warning_penalty))
                                .then_with(|| (b.cache_hit as u8).cmp(&(a.cache_hit as u8)))
                                .then_with(|| a.url.cmp(&b.url))
                        });

                        for i in idxs {
                            picked.push(candidates[i].clone());
                            if picked.len() >= top_k {
                                break;
                            }
                        }
                    }

                    if picked.len() < top_k {
                        // Fill remaining slots by score (descending), skipping already-picked items.
                        let mut seen = std::collections::BTreeSet::<(String, usize, usize)>::new();
                        for c in &picked {
                            seen.insert((c.url.clone(), c.start_char, c.end_char));
                        }

                        candidates.sort_by(|a, b| {
                            let ka = Self::score_key(a.score);
                            let kb = Self::score_key(b.score);
                            kb.cmp(&ka)
                                .then_with(|| a.warning_penalty.cmp(&b.warning_penalty))
                                .then_with(|| (b.cache_hit as u8).cmp(&(a.cache_hit as u8)))
                                .then_with(|| a.url.cmp(&b.url))
                        });

                        for c in candidates {
                            if picked.len() >= top_k {
                                break;
                            }
                            if seen.insert((c.url.clone(), c.start_char, c.end_char)) {
                                picked.push(c);
                            }
                        }
                    }

                    picked.truncate(top_k);
                    picked
                }
                // Default: score-only ranking (back-compat behavior).
                _ => {
                    candidates.sort_by(|a, b| {
                        let ka = Self::score_key(a.score);
                        let kb = Self::score_key(b.score);
                        kb.cmp(&ka)
                            .then_with(|| a.warning_penalty.cmp(&b.warning_penalty))
                            .then_with(|| a.url.cmp(&b.url))
                            .then_with(|| a.start_char.cmp(&b.start_char))
                    });
                    candidates.truncate(top_k);
                    candidates
                }
            }
        }

        fn query_key(query: &str) -> Option<String> {
            let q = query.trim();
            if q.is_empty() {
                None
            } else {
                Some(textprep::scrub(q))
            }
        }

        #[allow(dead_code)]
        pub(crate) fn openrouter_api_key_from_env() -> Option<String> {
            std::env::var("WEBPIPE_OPENROUTER_API_KEY")
                .ok()
                .filter(|v| !v.trim().is_empty())
                .or_else(|| {
                    std::env::var("OPENROUTER_API_KEY")
                        .ok()
                        .filter(|v| !v.trim().is_empty())
                })
        }

        pub(crate) fn openrouter_model_from_env() -> String {
            std::env::var("WEBPIPE_OPENROUTER_MODEL")
                .ok()
                .filter(|v| !v.trim().is_empty())
                .or_else(|| {
                    std::env::var("AXI_OPENROUTER_MODEL")
                        .ok()
                        .filter(|v| !v.trim().is_empty())
                })
                .or_else(|| {
                    std::env::var("OPENROUTER_MODEL")
                        .ok()
                        .filter(|v| !v.trim().is_empty())
                })
                // Default planner model: keep it fast/cheap and recent.
                // Override with WEBPIPE_OPENROUTER_MODEL / AXI_OPENROUTER_MODEL / OPENROUTER_MODEL.
                .unwrap_or_else(|| "openai/gpt-5.2".to_string())
        }

        pub(crate) fn openrouter_embeddings_model_from_env() -> String {
            std::env::var("WEBPIPE_OPENROUTER_EMBEDDINGS_MODEL")
                .ok()
                .filter(|v| !v.trim().is_empty())
                .or_else(|| {
                    std::env::var("OPENROUTER_EMBEDDINGS_MODEL")
                        .ok()
                        .filter(|v| !v.trim().is_empty())
                })
                // Default embedding model: keep it stable and broadly available.
                .unwrap_or_else(|| "openai/text-embedding-3-small".to_string())
        }

        pub(crate) fn openrouter_embeddings_timeout_ms_from_env() -> u64 {
            std::env::var("WEBPIPE_OPENROUTER_EMBEDDINGS_TIMEOUT_MS")
                .ok()
                .and_then(|s| s.trim().parse::<u64>().ok())
                .filter(|&v| v >= 1000)
                .unwrap_or(15_000)
        }

        pub(crate) fn semantic_embeddings_max_inputs_from_env() -> usize {
            std::env::var("WEBPIPE_SEMANTIC_EMBEDDINGS_MAX_INPUTS")
                .ok()
                .and_then(|s| s.trim().parse::<usize>().ok())
                .map(|v| v.clamp(4, 64))
                .unwrap_or(32)
        }

        pub(crate) fn semantic_embeddings_max_docs_from_env() -> usize {
            std::env::var("WEBPIPE_SEMANTIC_EMBEDDINGS_MAX_DOCS")
                .ok()
                .and_then(|s| s.trim().parse::<usize>().ok())
                .map(|v| v.clamp(1, 25))
                .unwrap_or(5)
        }

        #[allow(dead_code)]
        pub(crate) fn openai_api_key_from_env() -> Option<String> {
            std::env::var("WEBPIPE_OPENAI_API_KEY")
                .ok()
                .filter(|v| !v.trim().is_empty())
                .or_else(|| {
                    std::env::var("OPENAI_API_KEY")
                        .ok()
                        .filter(|v| !v.trim().is_empty())
                })
        }

        pub(crate) fn openai_model_from_env() -> String {
            std::env::var("WEBPIPE_OPENAI_MODEL")
                .ok()
                .filter(|v| !v.trim().is_empty())
                .or_else(|| {
                    std::env::var("AXI_OPENAI_MODEL")
                        .ok()
                        .filter(|v| !v.trim().is_empty())
                })
                .or_else(|| {
                    std::env::var("OPENAI_MODEL")
                        .ok()
                        .filter(|v| !v.trim().is_empty())
                })
                .unwrap_or_else(|| "gpt-4o-mini".to_string())
        }

        #[allow(dead_code)]
        pub(crate) fn groq_api_key_from_env() -> Option<String> {
            std::env::var("WEBPIPE_GROQ_API_KEY")
                .ok()
                .filter(|v| !v.trim().is_empty())
                .or_else(|| {
                    std::env::var("GROQ_API_KEY")
                        .ok()
                        .filter(|v| !v.trim().is_empty())
                })
        }

        pub(crate) fn groq_model_from_env() -> String {
            std::env::var("WEBPIPE_GROQ_MODEL")
                .ok()
                .filter(|v| !v.trim().is_empty())
                .or_else(|| {
                    std::env::var("AXI_GROQ_MODEL")
                        .ok()
                        .filter(|v| !v.trim().is_empty())
                })
                .or_else(|| {
                    std::env::var("GROQ_MODEL")
                        .ok()
                        .filter(|v| !v.trim().is_empty())
                })
                .unwrap_or_else(|| "llama-3.1-8b-instant".to_string())
        }

        pub(crate) fn planner_max_calls_from_env() -> usize {
            std::env::var("WEBPIPE_PLANNER_MAX_CALLS")
                .ok()
                .and_then(|v| v.trim().parse::<usize>().ok())
                .unwrap_or(1)
                .min(10)
        }

        fn select_urls_for_hydration(
            urls: Vec<String>,
            max_urls: usize,
            query: &str,
            url_selection_mode: &str,
        ) -> Vec<String> {
            if urls.is_empty() {
                return Vec::new();
            }
            let max_urls = max_urls.clamp(1, 10);

            match url_selection_mode {
                "auto" => {
                    if Self::query_key(query).is_some() {
                        Self::select_urls_for_hydration(urls, max_urls, query, "query_rank")
                    } else {
                        Self::select_urls_for_hydration(urls, max_urls, query, "preserve")
                    }
                }
                // Note: auto_plus is implemented in the main web_search_extract flow because it needs
                // bounded fetch+hint extraction.
                "query_rank" => {
                    let qkey = Self::query_key(query).unwrap_or_default();
                    let q_toks: Vec<&str> = qkey
                        .split(|ch: char| !ch.is_alphanumeric())
                        .filter(|t| t.len() >= 2)
                        .collect();
                    if q_toks.is_empty() {
                        return urls.into_iter().take(max_urls).collect();
                    }

                    let mut scored: Vec<(usize, u64, String)> = urls
                        .into_iter()
                        .enumerate()
                        .map(|(i, u)| {
                            let ukey = textprep::scrub(&u);
                            let mut s = 0u64;
                            for t in &q_toks {
                                if ukey.contains(t) {
                                    s += 1;
                                }
                            }
                            (i, s, u)
                        })
                        .collect();

                    // Stable rerank: higher score first; tie-break by original order.
                    scored.sort_by(|a, b| b.1.cmp(&a.1).then_with(|| a.0.cmp(&b.0)));
                    scored
                        .into_iter()
                        .take(max_urls)
                        .map(|(_i, _s, u)| u)
                        .collect()
                }
                _ => urls.into_iter().take(max_urls).collect(),
            }
        }

        #[tool(
            description = "Call this first to discover what is configured: which search providers (brave/tavily/searxng), fetch backends, privacy mode, and supported parameter enums are active. Use method=\"usage\" to check runtime cost/warning stats; method=\"usage_reset\" to clear them. Not this when you already know the config. Output: capabilities{}, configured{}, supported{}, defaults{} (no secrets).",
            input_schema = Arc::new(tool_input_schema_draft07::<WebpipeMetaArgs>()),
            annotations(title = "Webpipe meta", read_only_hint = true, open_world_hint = false)
        )]
        async fn webpipe_meta(
            &self,
            params: Parameters<Option<WebpipeMetaArgs>>,
        ) -> Result<CallToolResult, McpError> {
            let args = params.0.unwrap_or_default();
            // Method dispatch: route "usage" and "usage_reset" to their dedicated handlers.
            // This collapses three separate meta tools into one with a method param,
            // keeping the individual tools alive as backward-compat delegates.
            match args.method.as_deref().unwrap_or("info") {
                "usage" => return self.webpipe_usage(Parameters(None)).await,
                "usage_reset" => return self.webpipe_usage_reset().await,
                _ => {}
            }
            let t0 = std::time::Instant::now();
            self.stats_inc_tool("webpipe_meta");

            // Only report booleans / key names, never values.
            let tavily_configured = has_env("WEBPIPE_TAVILY_API_KEY") || has_env("TAVILY_API_KEY");
            let firecrawl_configured =
                has_env("WEBPIPE_FIRECRAWL_API_KEY") || has_env("FIRECRAWL_API_KEY");
            let brave_configured =
                has_env("WEBPIPE_BRAVE_API_KEY") || has_env("BRAVE_SEARCH_API_KEY");
            let searxng_configured =
                has_env("WEBPIPE_SEARXNG_ENDPOINT") || has_env("WEBPIPE_SEARXNG_ENDPOINTS");
            let perplexity_configured =
                has_env("WEBPIPE_PERPLEXITY_API_KEY") || has_env("PERPLEXITY_API_KEY");
            let openrouter_configured =
                has_env("WEBPIPE_OPENROUTER_API_KEY") || has_env("OPENROUTER_API_KEY");
            let openai_configured = has_env("WEBPIPE_OPENAI_API_KEY") || has_env("OPENAI_API_KEY");
            let groq_configured = has_env("WEBPIPE_GROQ_API_KEY") || has_env("GROQ_API_KEY");
            let gemini_configured = has_env("WEBPIPE_GEMINI_API_KEY")
                || has_env("GEMINI_API_KEY")
                || has_env("WEBPIPE_GOOGLE_API_KEY")
                || has_env("GOOGLE_API_KEY");

            let local_tools = serde_json::json!({
                "yt_dlp": webpipe_local::shellout::has("yt-dlp"),
                "pandoc": webpipe_local::shellout::has("pandoc"),
                "ffmpeg": webpipe_local::shellout::has("ffmpeg"),
                "tesseract": webpipe_local::shellout::has("tesseract"),
                "pdftotext": webpipe_local::shellout::has("pdftotext"),
                "mutool": webpipe_local::shellout::has("mutool"),
            });

            let mut providers = Vec::new();
            if brave_configured {
                providers.push("brave");
            }
            if tavily_configured {
                providers.push("tavily");
            }
            if searxng_configured {
                providers.push("searxng");
            }

            let mut remote_fetch = Vec::new();
            if firecrawl_configured {
                remote_fetch.push("firecrawl");
            }

            // Opinionated cost-aware default:
            // - Default `web_search.provider` stays "brave" (cheaper baseline; stable).
            // - "auto" prefers Brave when configured; uses other configured providers as fallbacks.
            let recommended_provider_order: Vec<&'static str> = if brave_configured {
                let mut v = vec!["brave"];
                // Prefer self-hosted “free-ish” SearXNG before paid providers when available.
                if searxng_configured {
                    v.push("searxng");
                }
                if tavily_configured {
                    v.push("tavily");
                }
                v
            } else if searxng_configured {
                let mut v = vec!["searxng"];
                if tavily_configured {
                    v.push("tavily");
                }
                v
            } else if tavily_configured {
                vec!["tavily"]
            } else {
                vec![]
            };

            // Help debug “which binary is Cursor actually running?” without leaking secrets.
            // We only include a short tail of the path.
            let exe = std::env::current_exe().ok().map(|p| {
                let comps: Vec<String> = p
                    .components()
                    .map(|c| c.as_os_str().to_string_lossy().to_string())
                    .collect();
                let n = 3usize;
                if comps.len() <= n {
                    comps.join("/")
                } else {
                    format!(".../{}", comps[comps.len() - n..].join("/"))
                }
            });

            let ts = mcp_toolset_from_env();
            let visible_tools: Vec<&'static str> = [
                "webpipe_meta",
                "webpipe_usage",
                "web_fetch",
                "web_extract",
                "search_evidence",
                "web_perplexity",
                "web_cache_search_extract",
                "web_deep_research",
                "web_search",
                "web_seed_urls",
                "web_seed_search_extract",
                "web_explore_extract",
                "web_sitemap_extract",
                "repo_ingest",
                "paper_search",
                "arxiv",
                "arxiv_search",
                "arxiv_enrich",
            ]
            .into_iter()
            .filter(|name| mcp_tool_allowed(name, ts))
            .collect();

            let mut payload = serde_json::json!({
                "ok": true,
                "name": "webpipe",
                "version": env!("CARGO_PKG_VERSION"),
                "cache_dir": cache_dir_from_env().map(|p| p.to_string_lossy().to_string()),
                "offline_only": offline_only_enabled(),
                "privacy_mode": match privacy_mode_from_env() {
                    PrivacyMode::Normal => "normal",
                    PrivacyMode::Offline => "offline",
                    PrivacyMode::Anonymous => "anonymous",
                },
                "anon_proxy_configured": anon_proxy_from_env().is_some(),
                "mcp_toolset": mcp_toolset_name(ts),
                "binary": {
                    "exe": exe,
                },
                "capabilities": {
                    "pdf_extract": true,
                    "semantic_rerank": true,
                    "embeddings_openai": false,
                    "embeddings_tei": false,
                    "embeddings_openrouter": Self::openrouter_api_key_from_env().is_some(),
                    "vision_gemini": cfg!(feature = "vision-gemini")
                },
                "supported": {
                    // Canonical tool surface ordered by "what users reach for first".
                    // Deprecated aliases (http_fetch, page_extract, web_search_extract, etc.)
                    // are listed under supported.deprecated_aliases.
                    "mcp_tools": [
                        "webpipe_meta",
                        "webpipe_usage",
                        "web_fetch",
                        "web_extract",
                        "search_evidence",
                        "web_perplexity",
                        "web_cache_search_extract",
                        "web_deep_research",
                        "web_search",
                        "web_seed_urls",
                        "web_seed_search_extract",
                        "web_explore_extract",
                        "web_sitemap_extract",
                        "repo_ingest",
                        "paper_search",
                        "arxiv",
                        "arxiv_search",
                        "arxiv_enrich"
                    ],
                    "mcp_tools_visible": visible_tools,
                    "mcp_tool_groups": {
                        "meta": ["webpipe_meta"],
                        "seeds": ["web_seed_urls", "web_seed_search_extract"],
                        "fetch_extract": ["web_fetch", "web_extract"],
                        "explore": ["web_explore_extract"],
                        "sitemap": ["web_sitemap_extract"],
                        "ingest": ["repo_ingest"],
                        "search": ["web_search", "search_evidence", "web_perplexity", "web_cache_search_extract"],
                        "research": ["web_deep_research", "paper_search", "arxiv"]
                    },
                    // Deprecated tool names and their canonical replacements.
                    // These tools remain callable but are excluded from the default visible set.
                    "deprecated_aliases": {
                        "http_fetch": "web_fetch",
                        "page_extract": "web_extract",
                        "page_extract_text": "web_extract",
                        "web_search_extract": "search_evidence",
                        "arxiv_search": "arxiv",
                        "arxiv_enrich": "arxiv"
                    },
                    // Values for web_search.provider
                    "providers": ["auto", "brave", "tavily", "searxng"],
                    // Values for web_search.auto_mode (when provider="auto")
                    "auto_modes": ["fallback", "merge", "mab"],
                    // Values for paper_search.backends
                    "paper_backends": ["semantic_scholar", "openalex", "google_scholar_serpapi"],
                    // Values for extraction engines
                    "extraction_engines": ["html2text", "html_main", "readability", "html_hint", "text", "json", "xml", "markdown", "pdf-extract", "pdf-pdftotext", "pdf-mutool", "pdf-strings", "youtube_transcript", "pandoc", "image", "image_ocr", "media", "media_subtitles", "gemini_vision"],
                    // Environment knobs (names only; no values) for opportunistic local tooling + multimodal.
                    "knobs": [
                        "WEBPIPE_SEARXNG_ENDPOINT",
                        "WEBPIPE_SEARXNG_ENDPOINTS",
                        "WEBPIPE_ARXIV_ENDPOINT",
                        "WEBPIPE_ARXIV_REWRITE_HOSTS",
                        "WEBPIPE_ARXIV_PDF_FALLBACK_BASE",
                        "WEBPIPE_OPENREVIEW_REWRITE_HOSTS",
                        "WEBPIPE_OPENREVIEW_API_BASE",
                        "WEBPIPE_GITHUB_API_BASE",
                        "WEBPIPE_GITHUB_RAW_HOST",
                        "WEBPIPE_GITHUB_TOKEN",
                        "WEBPIPE_GITHUB_REWRITE_HOSTS",
                        "WEBPIPE_GITHUB_REWRITE_BRANCHES",
                        "WEBPIPE_GIST_REWRITE_HOSTS",
                        "WEBPIPE_GIST_RAW_HOST",
                        "WEBPIPE_SERPAPI_API_KEY",
                        "SERPAPI_API_KEY",
                        "WEBPIPE_PERPLEXITY_ENDPOINT",
                        "WEBPIPE_PDF_SHELLOUT",
                        "WEBPIPE_PDF_SHELLOUT_MAX_PAGES",
                        "WEBPIPE_YOUTUBE_TRANSCRIPTS",
                        "WEBPIPE_YOUTUBE_LANGS",
                        "WEBPIPE_YOUTUBE_MAX_CHARS",
                        "WEBPIPE_PANDOC",
                        "WEBPIPE_PANDOC_TIMEOUT_MS",
                        "WEBPIPE_PANDOC_MAX_CHARS",
                        "WEBPIPE_OCR",
                        "WEBPIPE_OCR_TIMEOUT_MS",
                        "WEBPIPE_OCR_MAX_CHARS",
                        "WEBPIPE_MEDIA_SUBTITLES",
                        "WEBPIPE_MEDIA_SUBS_TIMEOUT_MS",
                        "WEBPIPE_MEDIA_SUBS_MAX_CHARS",
                        "WEBPIPE_NODE",
                        "WEBPIPE_RENDER_DISABLE",
                        "WEBPIPE_RENDER_CDP_ENDPOINT",
                        "WEBPIPE_RENDER_USER_DATA_DIR",
                        "WEBPIPE_RENDER_MAX_HTML_CHARS",
                        "WEBPIPE_RENDER_HARD_TIMEOUT_MS",
                        "WEBPIPE_VISION",
                        "WEBPIPE_GEMINI_API_KEY",
                        "GEMINI_API_KEY",
                        "WEBPIPE_GOOGLE_API_KEY",
                        "GOOGLE_API_KEY",
                        "WEBPIPE_GEMINI_MODEL",
                        "WEBPIPE_GEMINI_TIMEOUT_MS",
                        "WEBPIPE_GEMINI_MAX_CHARS",
                        "WEBPIPE_GEMINI_PROMPT",
                        "WEBPIPE_GEMINI_BASE_URL"
                    ],
                    // Values for web_search_extract.selection_mode / web_deep_research.selection_mode
                    "selection_modes": ["score", "pareto"],
                    // Values for web_search_extract.url_selection_mode
                    "url_selection_modes": ["auto", "auto_plus", "preserve", "query_rank"],
                    // Values for web_search_extract.agentic_selector
                    "agentic_selectors": ["auto", "lexical", "llm"],
                    // Values for web_search_extract.exploration
                    "explorations": ["balanced", "wide", "deep"],
                    // Values for web_* fetch_backend
                    "fetch_backends": ["local", "firecrawl", "render"],
                    // Note: firecrawl is a remote fetch backend (used by CLI eval harness and optionally by MCP tools when requested).
                    "remote_fetch": ["firecrawl"],
                    // Privacy contract (fail closed): which egress policy is intended.
                    "privacy_modes": ["normal", "offline", "anonymous"],
                    // Proxy schemes by backend:
                    // - local: supports socks5h:// (Tor-friendly; DNS via proxy)
                    // - render: requires an HTTP proxy endpoint (Playwright proxy settings do not support socks5h://)
                    "anon_proxy_schemes_local": ["socks5h", "socks5", "http", "https"],
                    "anon_proxy_schemes_render": ["http", "https"],
                    "llm_models": ["sonar-deep-research"],
                    "llm_planner_providers": ["openrouter", "openai", "groq"]
                },
                "defaults": {
                    "web_search": {
                        "provider": "brave",
                        "provider_auto_order": recommended_provider_order,
                        "auto_mode": "fallback",
                        "max_results": 10,
                        "max_results_max": 20
                    },
                    // Cursor-facing “what to call by default” guidance.
                    // Keep this boring and bounded: it’s a suggestion for agents, not a second API.
                    "recommended_defaults": {
                        // Prefer "auto" so the server can pick among configured providers.
                        "web_search": {
                            "provider": "auto",
                            "auto_mode": "fallback",
                            "max_results": 5
                        },
                        // For retrieval workflows: search_evidence is the canonical Normal-mode name.
                        // web_search_extract is the debug alias; same args, same handler.
                        "search_evidence": {
                            "provider": "auto",
                            "auto_mode": "fallback",
                            "fetch_backend": "local",
                            // Prefer Pareto by default: it tends to produce a cleaner evidence pack
                            // (cache hits, fewer warnings, shorter chunks) without sacrificing relevance.
                            "selection_mode": "pareto",
                            "exploration": "balanced",
                            "agentic": true,
                            "max_results": 5,
                            "max_urls": 3,
                            // Wide parallel hydration can materially reduce tail latency when callers
                            // provide multiple urls=[...] (and in a safe subset of agentic urls-mode).
                            "max_parallel_urls": 3,
                            // Prefer auto_plus when we have a query: do a bounded hint pre-pass to avoid
                            // burning max_urls on portal/app-shell pages.
                            "url_selection_mode": "auto_plus",
                            "timeout_ms": 20_000,
                            "max_bytes": 5_000_000,
                            "top_chunks": 5,
                            "max_chunk_chars": 500,
                            "include_links": false,
                            "include_text": false,
                            // Prefer structure-aware chunking when possible.
                            "include_structure": true,
                            // When Firecrawl is configured, a bounded fallback on low-signal/empty extraction
                            // improves real-world success rate on JS-heavy sites.
                            "firecrawl_fallback_on_empty_extraction": true,
                            "firecrawl_fallback_on_low_signal": true,
                            "no_network": false,
                            "cache": { "read": true, "write": true }
                        },
                        // Use web_extract for readability-focused extraction (chunks + optional structure).
                        "web_extract": {
                            "fetch_backend": "local",
                            "timeout_ms": 20_000,
                            "max_bytes": 5_000_000,
                            // Default to a bounded tail-recovery retry for PDFs (xref/trailer often lives at the end).
                            // For non-PDF pages, callers can opt in per-request with retry_on_truncation=true.
                            "retry_on_truncation": null,
                            "truncation_retry_max_bytes": 10_000_000,
                            "width": 100,
                            "max_chars": 20_000,
                            "top_chunks": 5,
                            "max_chunk_chars": 500,
                            // Default to structure output; it improves chunk quality and debuggability.
                            "include_structure": true,
                            "include_text": true,
                            "include_links": false,
                            "no_network": false,
                            "cache": { "read": true, "write": true }
                        },
                        // Use web_fetch for raw bytes/headers, and only include_text when needed.
                        "web_fetch": {
                            "fetch_backend": "local",
                            "timeout_ms": 15_000,
                            "max_bytes": 5_000_000,
                            "include_text": false,
                            "max_text_chars": 20_000,
                            "include_headers": false,
                            "no_network": false,
                            "cache": { "read": true, "write": true }
                        }
                    },
                    "web_fetch": {
                        "fetch_backend": "local",
                            "no_network": false,
                        "timeout_ms": 15_000,
                        "max_bytes": 5_000_000,
                        "max_text_chars": 20_000,
                        "max_text_chars_max": 200_000,
                        "include_text": false,
                        "include_headers": false
                    },
                    "web_extract": {
                        "fetch_backend": "local",
                            "no_network": false,
                        "timeout_ms": 20_000,
                        "max_bytes": 5_000_000,
                        "retry_on_truncation": null,
                        "truncation_retry_max_bytes": 10_000_000,
                        "width": 100,
                        "max_chars": 20_000,
                        "max_chars_max": 200_000,
                        "top_chunks": 5,
                        "top_chunks_max": 50,
                        "max_chunk_chars": 500,
                        "max_chunk_chars_max": 5_000,
                        "include_text": true,
                        "include_links": false,
                        "max_links": 50,
                        "max_links_max": 500,
                        "include_structure": true,
                        "max_outline_items": 25,
                        "max_blocks": 40,
                        "max_block_chars": 400,
                        "semantic_rerank": false,
                        "semantic_auto_fallback": true,
                        "semantic_top_k": 5
                    },
                    "web_cache_search_extract": {
                        "max_docs": 50,
                        "max_docs_max": 500,
                        "max_scan_entries": 2000,
                        "max_scan_entries_max": 20000,
                        "width": 100,
                        "max_chars": 20_000,
                        "max_chars_max": 200_000,
                        "top_chunks": 5,
                        "top_chunks_max": 50,
                        "max_chunk_chars": 500,
                        "max_chunk_chars_max": 5_000,
                        "include_text": false,
                        "include_structure": false,
                        "max_outline_items": 25,
                        "max_blocks": 40,
                        "max_block_chars": 400,
                        "semantic_rerank": false,
                        "semantic_top_k": 5,
                        "compact": true
                    },
                    // web_search_extract is the debug alias for search_evidence.
                    "search_evidence_defaults": {
                        "provider": "auto",
                        "auto_mode": "fallback",
                        "selection_mode": "pareto",
                        "fetch_backend": "local",
                            "no_network": false,
                        // Defaults are “smart” in code: these only activate when Firecrawl is configured.
                        "firecrawl_fallback_on_empty_extraction": true,
                        "firecrawl_fallback_on_low_signal": true,
                        "exploration": "balanced",
                        "max_results": 5,
                        "max_urls": 3,
                        "url_selection_mode": "auto_plus",
                        "agentic": true,
                        "agentic_selector": "auto",
                        "agentic_max_search_rounds": 1,
                        "agentic_frontier_max": 200,
                        "planner_max_calls": 1,
                        "timeout_ms": 20_000,
                        "max_bytes": 5_000_000,
                        "width": 100,
                        "max_chars": 20_000,
                        "max_chars_max": 200_000,
                        "top_chunks": 5,
                        "top_chunks_max": 50,
                        "max_chunk_chars": 500,
                        "max_chunk_chars_max": 5_000,
                        "include_links": false,
                        "max_links": 25,
                        "max_links_max": 500,
                        "include_text": false,
                        "include_structure": true,
                        "max_outline_items": 25,
                        "max_blocks": 40,
                        "max_block_chars": 400,
                        "semantic_rerank": false,
                        "semantic_top_k": 5,
                        "compact": true
                    },
                    "web_deep_research": {
                        "provider": "auto",
                        "auto_mode": "fallback",
                        "selection_mode": "score",
                        "fetch_backend": "local",
                        "exploration": "balanced",
                        "max_results": 5,
                        "max_urls": 3,
                        "timeout_ms": 20_000,
                        "max_bytes": 5_000_000,
                        "width": 100,
                        "max_chars": 20_000,
                        "top_chunks": 5,
                        "max_chunk_chars": 500,
                        "include_links": false,
                        "max_links": 25,
                        "model": "sonar-deep-research",
                        "search_mode": null,
                        "reasoning_effort": "medium",
                        "max_answer_chars": 20_000
                    }
                },
                "tool_output_summary": {
                    "web_fetch": "Fetch a URL (cache-aware). Returns status/content_type/bytes_len and optional bounded text/headers.",
                    "web_extract": "Fetch+extract readable text/chunks (bounded). Returns extract.engine, text_chars, extract.top_chunks, warnings/hints.",
                    "web_search": "Search only (bounded). Returns results[] with url/title/content and provider selection when provider=auto.",
                    "search_evidence": "Primary evidence tool: search -> fetch -> extract -> rank top_chunks. Full response by default; set minimal_output=true for compact (top_chunks + warning_codes only).",
                    "web_perplexity": "Perplexity-backed synthesis (requires API key). Returns answer text + citations[].",
                    "web_cache_search_extract": "Cache-only search: scan WEBPIPE_CACHE_DIR -> extract -> top_chunks (no network).",
                    "web_deep_research": "Evidence gatherer + optional synthesis. Prefer include_evidence for auditability.",
                    "arxiv": "arXiv papers: search by topic (pass query) or get metadata for a specific paper (pass id_or_url). Returns papers[] or paper{}.",
                    "arxiv_search": "DEPRECATED: use arxiv instead (same capabilities; pass query).",
                    "arxiv_enrich": "DEPRECATED: use arxiv instead (same capabilities; pass id_or_url).",
                    "paper_search": "Multi-backend academic search (Semantic Scholar / OpenAlex / Google Scholar). Returns papers[] with citation counts.",
                    // Deprecated aliases (still callable, not default-visible):
                    "http_fetch": "DEPRECATED: use web_fetch instead.",
                    "page_extract": "DEPRECATED: use web_extract instead.",
                    "page_extract_text": "DEPRECATED: use web_extract with include_text=true instead.",
                    "web_search_extract": "DEPRECATED: use search_evidence instead (same handler, same args)."
                },
                "configured": {
                    "providers": {
                        "brave": brave_configured,
                        "tavily": tavily_configured,
                        "searxng": searxng_configured
                    },
                    "fetch": {
                        "local": true
                    },
                    "remote_fetch": {
                        "firecrawl": firecrawl_configured
                    },
                    "llm": {
                        "perplexity": perplexity_configured,
                        "openrouter": openrouter_configured,
                        "openrouter_model": if openrouter_configured {
                            Some(Self::openrouter_model_from_env())
                        } else {
                            None
                        },
                        "openai": openai_configured,
                        "openai_model": if openai_configured {
                            Some(Self::openai_model_from_env())
                        } else {
                            None
                        },
                        "groq": groq_configured,
                        "groq_model": if groq_configured {
                            Some(Self::groq_model_from_env())
                        } else {
                            None
                        }
                    },
                    "vision": {
                        "gemini": gemini_configured
                    }
                },
                "available": {
                    // Currently configured search providers (keys present in env).
                    "providers": providers,
                    // Remote fetch backends that are configured.
                    "remote_fetch": remote_fetch,
                    // API-key-gated tools that are currently visible/callable in Normal mode.
                    // Empty = no optional tools configured; check configured.llm.* for what keys to add.
                    "tools": {
                        "web_perplexity": perplexity_configured
                    }
                },
                "local_tools": local_tools
            });
            add_envelope_fields(&mut payload, "webpipe_meta", t0.elapsed().as_millis());
            let md = webpipe_meta_markdown(&payload);
            Ok(tool_result_markdown_with_json(payload, md))
        }

        #[tool(
            description = "Curated seed URL registry for keyless/offline-ish workflows (bounded; JSON output)",
            input_schema = Arc::new(tool_input_schema_draft07::<WebSeedUrlsArgs>()),
            annotations(title = "Seed URLs", read_only_hint = true, open_world_hint = false)
        )]
        async fn web_seed_urls(
            &self,
            params: Parameters<Option<WebSeedUrlsArgs>>,
        ) -> Result<CallToolResult, McpError> {
            let t0 = std::time::Instant::now();
            self.stats_inc_tool("web_seed_urls");
            let args = params.0.unwrap_or_default();
            let max = args.max.unwrap_or(25).clamp(1, 50);
            let include_ids = args.include_ids.unwrap_or(true);
            let ids_only = args.ids_only.unwrap_or(false);

            // Curated seed registry (single source of truth).
            let seeds = seed_registry();

            let seeds_out = if ids_only {
                seeds
                    .iter()
                    .take(max)
                    .map(|s| {
                        serde_json::json!({
                            "id": s.id,
                            "url": s.url
                        })
                    })
                    .collect::<Vec<_>>()
            } else {
                seeds
                    .iter()
                    .take(max)
                    .map(|s| {
                        serde_json::json!({
                            "id": s.id,
                            "url": s.url,
                            "kind": s.kind,
                            "note": s.note
                        })
                    })
                    .collect::<Vec<_>>()
            };

            let mut payload = serde_json::json!({
                "ok": true,
                "max": max,
                "seeds": seeds_out,
                "notes": [
                    "Use these with web_extract or search_evidence(urls=[...], url_selection_mode=query_rank) to avoid paid search providers.",
                    "Prefer small bounds (max_urls/top_chunks/max_chars) and cache_read=true to stay deterministic."
                ]
            });
            if include_ids {
                payload["ids"] =
                    serde_json::json!(seeds.iter().take(max).map(|s| s.id).collect::<Vec<_>>());
            }
            add_envelope_fields(&mut payload, "web_seed_urls", t0.elapsed().as_millis());
            let md = web_seed_urls_markdown(&payload);
            Ok(tool_result_markdown_with_json(payload, md))
        }

        #[tool(
            description = "Fetch+extract a bounded set of seed URLs (or urls=...) and return merged top chunks for a query (cache-aware; JSON output)",
            input_schema = Arc::new(tool_input_schema_draft07::<WebSeedSearchExtractArgs>()),
            annotations(
                title = "Seed search+extract",
                read_only_hint = true,
                open_world_hint = true
            )
        )]
        async fn web_seed_search_extract(
            &self,
            params: Parameters<Option<WebSeedSearchExtractArgs>>,
        ) -> Result<CallToolResult, McpError> {
            let t0 = std::time::Instant::now();
            self.stats_inc_tool("web_seed_search_extract");

            let args = params.0.unwrap_or_default();
            let query = args.query.trim().to_string();
            if query.is_empty() {
                let mut payload = serde_json::json!({
                    "ok": false,
                    "query": "",
                    "error": error_obj(ErrorCode::InvalidParams, "query must be non-empty", "Provide a query string.")
                });
                add_envelope_fields(
                    &mut payload,
                    "web_seed_search_extract",
                    t0.elapsed().as_millis(),
                );
                let md = web_seed_search_extract_markdown(&payload);
                return Ok(tool_result_markdown_with_json(payload, md));
            }

            let max_urls = args.max_urls.unwrap_or(4).clamp(1, 10);
            let width = args.width.unwrap_or(100).clamp(20, 240);
            let max_chars = args.max_chars.unwrap_or(20_000).min(200_000);
            let max_bytes = args.max_bytes.unwrap_or(2_000_000).min(10_000_000);
            let top_chunks = args.top_chunks.unwrap_or(5).clamp(1, 25);
            let merged_max_per_seed = args.merged_max_per_seed.unwrap_or(top_chunks).clamp(1, 25);
            let max_chunk_chars = args.max_chunk_chars.unwrap_or(500).clamp(50, 5_000);
            let include_text = args.include_text.unwrap_or(false);
            let compact = args.compact.unwrap_or(true);

            let fetch_backend = args.fetch_backend.unwrap_or_else(|| "local".to_string());
            let no_network = args.no_network.unwrap_or(false);
            let cache_read = args.cache_read.unwrap_or(true);
            let cache_write = args.cache_write.unwrap_or(true);
            let cache_ttl_s = args.cache_ttl_s;

            let mut warnings: Vec<&'static str> = Vec::new();
            let seed_ids_were_provided = args.urls.is_none() && args.seed_ids.as_ref().is_some();
            let (urls, unknown_seed_ids) = select_seed_urls(
                seed_registry(),
                args.urls.clone(),
                args.seed_ids.clone(),
                max_urls,
            );
            if !unknown_seed_ids.is_empty() {
                warnings.push("unknown_seed_id");
            }
            if seed_ids_were_provided && urls.is_empty() {
                let mut payload = serde_json::json!({
                    "ok": false,
                    "error": error_obj(
                        ErrorCode::InvalidParams,
                        "seed_ids did not match any known seeds",
                        "Call web_seed_urls to see valid ids, or pass urls=[...] explicitly."
                    ),
                    "unknown_seed_ids": unknown_seed_ids,
                });
                add_envelope_fields(
                    &mut payload,
                    "web_seed_search_extract",
                    t0.elapsed().as_millis(),
                );
                let md = web_seed_search_extract_markdown(&payload);
                return Ok(tool_result_markdown_with_json(payload, md));
            }

            // Anonymous mode: if we are going to fetch non-localhost URLs, require a proxy unless
            // the caller explicitly asked for cache-only.
            if privacy_mode_from_env() == PrivacyMode::Anonymous
                && !no_network
                && anon_proxy_from_env().is_none()
                && urls.iter().any(|(_id, u)| !is_localhost_url(u))
            {
                let mut payload = serde_json::json!({
                    "ok": false,
                    "error": error_obj(
                        ErrorCode::NotConfigured,
                        "anonymous mode requires a proxy",
                        "Set WEBPIPE_ANON_PROXY (recommended for Tor: socks5h://127.0.0.1:9050), or set no_network=true for cache-only runs."
                    ),
                    "request": {
                        "privacy_mode": "anonymous",
                        "anon_proxy_configured": false,
                        "no_network": no_network
                    }
                });
                add_envelope_fields(
                    &mut payload,
                    "web_seed_search_extract",
                    t0.elapsed().as_millis(),
                );
                let md = web_seed_search_extract_markdown(&payload);
                return Ok(tool_result_markdown_with_json(payload, md));
            }

            #[derive(Debug, Clone, serde::Serialize)]
            struct PerUrlResult {
                id: String,
                url: String,
                ok: bool,
                extraction_engine: Option<String>,
                chunks: Vec<serde_json::Value>,
                warnings: Vec<String>,
                error: Option<serde_json::Value>,
            }

            let mut per_url: Vec<PerUrlResult> = Vec::new();
            let mut merged: Vec<(u64, String, serde_json::Value)> = Vec::new();

            for (id, url) in urls.iter() {
                let r = self
                    .web_extract(p(WebExtractArgs {
                        url: Some(url.clone()),
                        fetch_backend: Some(fetch_backend.clone()),
                        no_network: Some(no_network),
                        width: Some(width),
                        max_chars: Some(max_chars),
                        query: Some(query.clone()),
                        top_chunks: Some(top_chunks),
                        max_chunk_chars: Some(max_chunk_chars),
                        max_bytes: Some(max_bytes),
                        retry_on_truncation: None,
                        truncation_retry_max_bytes: None,
                        include_links: Some(false),
                        max_links: Some(0),
                        include_text: Some(include_text),
                        include_structure: Some(false),
                        max_outline_items: None,
                        max_blocks: None,
                        max_block_chars: None,
                        semantic_rerank: Some(false),
                        semantic_auto_fallback: Some(false),
                        semantic_top_k: None,
                        cache_read: Some(cache_read),
                        cache_write: Some(cache_write),
                        cache_ttl_s,
                        timeout_ms: None,
                    }))
                    .await?;
                let v = payload_from_result(&r);

                let ok = v["ok"].as_bool().unwrap_or(false);
                let engine = v
                    .get("extract")
                    .and_then(|x| x.get("engine"))
                    .and_then(|x| x.as_str())
                    .map(|s| s.to_string());
                let chunks = v
                    .get("extract")
                    .and_then(|x| x.get("chunks"))
                    .and_then(|x| x.as_array())
                    .cloned()
                    .unwrap_or_default();
                let warnings = v
                    // web_extract returns warnings at top-level.
                    .get("warnings")
                    .and_then(|x| x.as_array())
                    .cloned()
                    .unwrap_or_default()
                    .into_iter()
                    .filter_map(|x| x.as_str().map(|s| s.to_string()))
                    .collect::<Vec<_>>();
                let error = v.get("error").cloned();

                if ok {
                    for c in chunks.iter() {
                        let score = c.get("score").and_then(|x| x.as_u64()).unwrap_or(0);
                        merged.push((score, id.clone(), c.clone()));
                    }
                }

                per_url.push(PerUrlResult {
                    id: id.clone(),
                    url: url.clone(),
                    ok,
                    extraction_engine: engine,
                    chunks,
                    warnings,
                    error,
                });
            }

            // Merge: higher score first, then stable tiebreak by id.
            merged.sort_by(|a, b| b.0.cmp(&a.0).then_with(|| a.1.cmp(&b.1)));
            let max_total_chunks = (max_urls * top_chunks).clamp(1, 50);
            let mut per_seed_counts: std::collections::BTreeMap<String, usize> =
                std::collections::BTreeMap::new();
            let mut merged_chunks: Vec<serde_json::Value> = Vec::new();
            for (score, id, mut c) in merged.into_iter() {
                if merged_chunks.len() >= max_total_chunks {
                    break;
                }
                let n = per_seed_counts.get(&id).copied().unwrap_or(0);
                if n >= merged_max_per_seed {
                    continue;
                }
                per_seed_counts.insert(id.clone(), n + 1);
                c["seed_id"] = serde_json::json!(id);
                c["score"] = serde_json::json!(score);
                merged_chunks.push(c);
            }

            let mut payload = serde_json::json!({
                "ok": true,
                "query": query,
                "request": {
                    "max_urls": max_urls,
                    "fetch_backend": fetch_backend,
                    "no_network": no_network,
                    "width": width,
                    "max_chars": max_chars,
                    "max_bytes": max_bytes,
                    "top_chunks": top_chunks,
                    "merged_max_per_seed": merged_max_per_seed,
                    "max_chunk_chars": max_chunk_chars,
                    "include_text": include_text,
                    "compact": compact,
                    "cache_read": cache_read,
                    "cache_write": cache_write,
                    "cache_ttl_s": cache_ttl_s
                },
                "urls": urls.iter().map(|(id, url)| serde_json::json!({"id": id, "url": url})).collect::<Vec<_>>(),
                "results": {
                    "merged_chunks": merged_chunks,
                    "per_url": per_url
                }
            });

            if no_network
                && payload["results"]["per_url"]
                    .as_array()
                    .unwrap_or(&vec![])
                    .iter()
                    .any(|x| {
                        x.get("error")
                            .and_then(|e| e.get("code"))
                            .and_then(|c| c.as_str())
                            == Some("cache_error")
                    })
            {
                warnings.push("no_network_may_require_warm_cache");
            }
            if !unknown_seed_ids.is_empty() {
                payload["unknown_seed_ids"] = serde_json::json!(unknown_seed_ids);
            }
            if !warnings.is_empty() {
                payload["warnings"] = serde_json::json!(warnings);
                let codes = warning_codes_from(&warnings);
                payload["warning_codes"] = serde_json::json!(codes.clone());
                payload["warning_hints"] = warning_hints_from(&codes);
            }

            if compact {
                // Keep per_url but drop potentially large echoed chunks there; merged view is enough.
                if let Some(arr) = payload["results"]["per_url"].as_array_mut() {
                    for it in arr {
                        it["chunks"] = serde_json::json!([]);
                    }
                }
            }

            add_envelope_fields(
                &mut payload,
                "web_seed_search_extract",
                t0.elapsed().as_millis(),
            );
            let md = web_seed_search_extract_markdown(&payload);
            Ok(tool_result_markdown_with_json(payload, md))
        }

        #[tool(
            description = "Check provider cost unit consumption, per-tool call counts, and accumulated warning counts since server start. Best for budget tracking and diagnosing repeated warning patterns. Not this when you need search capability info — use webpipe_meta instead. Output: tool_calls{}, usage.search_providers{}, warnings.counts{}.",
            input_schema = Arc::new(tool_input_schema_draft07::<WebpipeUsageArgs>()),
            annotations(
                title = "Webpipe usage",
                read_only_hint = true,
                open_world_hint = false
            )
        )]
        async fn webpipe_usage(
            &self,
            _params: Parameters<Option<WebpipeUsageArgs>>,
        ) -> Result<CallToolResult, McpError> {
            let kind = "webpipe_usage";
            let t0 = std::time::Instant::now();
            self.stats_inc_tool(kind);

            fn env_u64(key: &str) -> Option<u64> {
                std::env::var(key)
                    .ok()
                    .and_then(|v| v.trim().parse::<u64>().ok())
            }
            fn env_f64(key: &str) -> Option<f64> {
                std::env::var(key)
                    .ok()
                    .and_then(|v| v.trim().parse::<f64>().ok())
            }

            // Optional, user-configured budgeting/pricing knobs.
            // These are intentionally “dumb”: we only multiply cost_units by usd_per_unit if the user sets it.
            // We do not hardcode provider pricing in code.
            let tavily_budget_units = env_u64("WEBPIPE_TAVILY_BUDGET_UNITS");
            let tavily_budget_usd = env_f64("WEBPIPE_TAVILY_BUDGET_USD");
            let tavily_usd_per_unit = env_f64("WEBPIPE_TAVILY_USD_PER_COST_UNIT");

            let brave_budget_units = env_u64("WEBPIPE_BRAVE_BUDGET_UNITS");
            let brave_budget_usd = env_f64("WEBPIPE_BRAVE_BUDGET_USD");
            let brave_usd_per_unit = env_f64("WEBPIPE_BRAVE_USD_PER_COST_UNIT");

            let s = self.stats_lock();
            let now = now_epoch_s();

            let tavily_units = s
                .search_providers
                .get("tavily")
                .map(|p| p.cost_units)
                .unwrap_or(0);
            let brave_units = s
                .search_providers
                .get("brave")
                .map(|p| p.cost_units)
                .unwrap_or(0);

            let tool_calls = s.tool_calls.clone();
            let search_providers = s.search_providers.clone();
            let search_window_cap = s.search_window_cap;
            let routing_context = match s.routing_context {
                RoutingContext::Global => "global",
                RoutingContext::QueryKey => "query_key",
                RoutingContext::Both => "both",
            };
            let routing_max_contexts = s.routing_max_contexts;
            let routing_contexts_in_memory = s.search_windows_by_query_key.len();
            let mut search_window_summaries: std::collections::BTreeMap<String, muxer::Summary> =
                std::collections::BTreeMap::new();
            for (k, w) in &s.search_windows {
                search_window_summaries.insert(k.clone(), w.summary());
            }
            let llm_backends = s.llm_backends.clone();
            let fetch_backends = s.fetch_backends.clone();
            let warning_counts = s.warning_counts.clone();

            let tavily_est_usd = tavily_usd_per_unit.map(|k| (tavily_units as f64) * k);
            let brave_est_usd = brave_usd_per_unit.map(|k| (brave_units as f64) * k);

            let mut payload = serde_json::json!({
                "ok": true,
                "started_at_epoch_s": s.started_at_epoch_s,
                "now_epoch_s": now,
                "tool_calls": tool_calls,
                "usage": {
                    "search_providers": search_providers,
                    "search_window": {
                        "cap": search_window_cap,
                        "summaries": search_window_summaries,
                        "routing_context": routing_context,
                        "routing_max_contexts": routing_max_contexts,
                        "routing_contexts_in_memory": routing_contexts_in_memory
                    },
                    "llm_backends": llm_backends,
                    "fetch_backends": fetch_backends
                },
                "warnings": {
                    "counts": warning_counts
                },
                "pricing_config": {
                    "tavily": {
                        "budget_units": tavily_budget_units,
                        "budget_usd": tavily_budget_usd,
                        "usd_per_cost_unit": tavily_usd_per_unit
                    },
                    "brave": {
                        "budget_units": brave_budget_units,
                        "budget_usd": brave_budget_usd,
                        "usd_per_cost_unit": brave_usd_per_unit
                    }
                },
                "estimates": {
                    "tavily": { "cost_units": tavily_units, "estimated_usd": tavily_est_usd },
                    "brave": { "cost_units": brave_units, "estimated_usd": brave_est_usd }
                }
            });

            add_envelope_fields(&mut payload, kind, t0.elapsed().as_millis());
            let md = webpipe_usage_markdown(&payload);
            Ok(tool_result_markdown_with_json(payload, md))
        }

        #[tool(
            description = "Reset in-process usage/cost stats (explicit side effect; JSON output)",
            input_schema = std::sync::Arc::new({
                let mut m = serde_json::Map::new();
                m.insert("type".to_string(), serde_json::json!("object"));
                m.insert("additionalProperties".to_string(), serde_json::json!(false));
                m
            }),
            annotations(
                title = "Reset webpipe usage",
                read_only_hint = false,
                destructive_hint = true,
                idempotent_hint = true,
                open_world_hint = false
            )
        )]
        async fn webpipe_usage_reset(&self) -> Result<CallToolResult, McpError> {
            let kind = "webpipe_usage_reset";
            let t0 = std::time::Instant::now();
            self.stats_inc_tool(kind);

            let now = now_epoch_s();
            let mut s = self.stats_lock();
            *s = UsageStats::new(now);

            let mut payload = serde_json::json!({
                "ok": true,
                "reset_at_epoch_s": now
            });
            add_envelope_fields(&mut payload, kind, t0.elapsed().as_millis());
            let md = webpipe_usage_reset_markdown(&payload);
            Ok(tool_result_markdown_with_json(payload, md))
        }

        #[tool(
            description = "Best for: finding academic papers on arXiv by topic, author, or keyword. Not this when you have a specific paper ID — use arxiv_enrich instead. Output: papers[] with title, abstract, authors, categories, pdf_url (bounded by per_page). Supports category filters (cs.AI, cs.CL, etc.) and semantic reranking.",
            input_schema = Arc::new(tool_input_schema_draft07::<ArxivSearchArgs>()),
            annotations(title = "arXiv search", read_only_hint = true, open_world_hint = true)
        )]
        async fn arxiv_search(
            &self,
            params: Parameters<Option<ArxivSearchArgs>>,
        ) -> Result<CallToolResult, McpError> {
            let args = params.0.unwrap_or_default();
            self.stats_inc_tool("arxiv_search");
            let t0 = std::time::Instant::now();

            let query = args.query.clone().unwrap_or_default().trim().to_string();
            if query.is_empty() {
                let mut payload = serde_json::json!({
                    "ok": false,
                    "query": args.query.unwrap_or_default(),
                    "error": error_obj(ErrorCode::InvalidParams, "query must be non-empty", "Pass a non-empty query string.")
                });
                add_envelope_fields(&mut payload, "arxiv_search", t0.elapsed().as_millis());
                let md = arxiv_search_markdown(&payload);
                return Ok(tool_result_markdown_with_json(payload, md));
            }

            let page = args.page.unwrap_or(1).max(1);
            let per_page = args
                .per_page
                .or(args.max_results)
                .unwrap_or(10)
                .clamp(1, 50);
            let timeout_ms = args.timeout_ms.unwrap_or(20_000);
            let categories = args.categories.unwrap_or_default();
            let years = args.years.unwrap_or_default();

            let semantic_rerank = args.semantic_rerank.unwrap_or(false);
            let semantic_top_k = args.semantic_top_k.unwrap_or(per_page).clamp(1, 50);

            let mut resp = webpipe_local::arxiv::arxiv_search(
                self.http.clone(),
                query.clone(),
                categories.clone(),
                years.clone(),
                page,
                per_page,
                timeout_ms,
            )
            .await
            .map_err(|e| McpError::internal_error(e.to_string(), None))?;

            let mut semantic: serde_json::Value = serde_json::Value::Null;
            if semantic_rerank {
                let cands: Vec<(usize, usize, String)> = resp
                    .papers
                    .iter()
                    .map(|p| {
                        let text =
                            format!("__ARXIV_ID__:{}\n{}\n\n{}", p.arxiv_id, p.title, p.summary);
                        (0, text.chars().count(), text)
                    })
                    .collect();
                let sem = self
                    .semantic_rerank_chunks_best(&query, &cands, semantic_top_k)
                    .await;
                if sem.ok {
                    let mut by_id = std::collections::BTreeMap::<
                        String,
                        webpipe_local::arxiv::ArxivPaper,
                    >::new();
                    for p in &resp.papers {
                        by_id.insert(p.arxiv_id.clone(), p.clone());
                    }
                    let mut out: Vec<webpipe_local::arxiv::ArxivPaper> = Vec::new();
                    let mut seen = std::collections::HashSet::<String>::new();
                    for ch in &sem.chunks {
                        let first = ch.text.lines().next().unwrap_or("");
                        let id = first.strip_prefix("__ARXIV_ID__:").unwrap_or("").trim();
                        if id.is_empty() {
                            continue;
                        }
                        if let Some(p) = by_id.get(id) {
                            if seen.insert(p.arxiv_id.clone()) {
                                out.push(p.clone());
                            }
                        }
                        if out.len() >= semantic_top_k {
                            break;
                        }
                    }
                    if !out.is_empty() {
                        resp.papers = out;
                    } else {
                        resp.warnings.push("semantic_rerank_no_matches");
                    }
                } else {
                    resp.warnings.extend(sem.warnings.iter().copied());
                }
                semantic = serde_json::to_value(sem).unwrap_or(serde_json::Value::Null);
            }

            let mut payload = serde_json::json!({
                "ok": resp.ok,
                "query": resp.query,
                "page": resp.page,
                "per_page": resp.per_page,
                "total_results": resp.total_results,
                "papers": resp.papers,
                "warnings": resp.warnings,
                "request": {
                    "query": query,
                    "categories": categories,
                    "years": years,
                    "page": page,
                    "per_page": per_page,
                    "timeout_ms": timeout_ms,
                    "semantic_rerank": semantic_rerank,
                    "semantic_top_k": semantic_top_k
                }
            });
            if !semantic.is_null() {
                payload["request"]["semantic"] = semantic;
            }
            add_envelope_fields(&mut payload, "arxiv_search", t0.elapsed().as_millis());
            let md = arxiv_search_markdown(&payload);
            Ok(tool_result_markdown_with_json(payload, md))
        }

        #[tool(
            description = "Best for: searching academic papers across Semantic Scholar and OpenAlex (free, no key required) or Google Scholar (requires WEBPIPE_SERPAPI_API_KEY). Broader venue coverage than arxiv_search. Caution: Semantic Scholar / OpenAlex are free-tier and may rate-limit; if you get paper_backend_failed, retry or use arxiv_search (more reliable). Output: papers[] with title, abstract, authors, venue, citation counts.",
            input_schema = Arc::new(tool_input_schema_draft07::<PaperSearchArgs>()),
            annotations(title = "Paper search", read_only_hint = true, open_world_hint = true)
        )]
        async fn paper_search(
            &self,
            params: Parameters<Option<PaperSearchArgs>>,
        ) -> Result<CallToolResult, McpError> {
            let args = params.0.unwrap_or_default();
            self.stats_inc_tool("paper_search");
            let t0 = std::time::Instant::now();

            let query = args.query.clone().unwrap_or_default().trim().to_string();
            if query.is_empty() {
                let mut payload = serde_json::json!({
                    "ok": false,
                    "query": args.query.unwrap_or_default(),
                    "error": error_obj(ErrorCode::InvalidParams, "query must be non-empty", "Pass a non-empty query string.")
                });
                add_envelope_fields(&mut payload, "paper_search", t0.elapsed().as_millis());
                return Ok(tool_result(payload));
            }

            let backends = args.backends.unwrap_or_default();
            let years = args.years.unwrap_or_default();
            let limit = args.limit.unwrap_or(10).clamp(1, 50);
            let timeout_ms = args.timeout_ms.unwrap_or(20_000).min(60_000);
            let include_abstract = args.include_abstract.unwrap_or(false);

            let resp = webpipe_local::papers::paper_search(
                self.http.clone(),
                query.clone(),
                backends.clone(),
                years.clone(),
                limit,
                timeout_ms,
                include_abstract,
            )
            .await
            .map_err(|e| McpError::internal_error(e.to_string(), None))?;

            // Warnings are set-like; keep them deduped for stable UX.
            let mut warnings = resp.warnings.clone();
            warnings.sort();
            warnings.dedup();
            let mut warning_codes: Vec<String> = warnings
                .iter()
                .map(|w| normalize_warning_code(w).to_string())
                .collect();
            warning_codes.sort();
            warning_codes.dedup();

            let mut payload = serde_json::json!({
                "ok": resp.ok,
                "query": resp.query,
                "backends": resp.backends,
                "papers": resp.papers,
                "warnings": warnings,
                "timings_ms": resp.timings_ms,
                "request": {
                    "query": query,
                    "backends": backends,
                    "years": years,
                    "limit": limit,
                    "timeout_ms": timeout_ms,
                    "include_abstract": include_abstract
                }
            });
            if !warning_codes.is_empty() {
                let codes_ref: Vec<&str> = warning_codes.iter().map(|s| s.as_str()).collect();
                payload["warning_codes"] = serde_json::json!(warning_codes);
                payload["warning_hints"] = warning_hints_from(&codes_ref);
            }
            add_envelope_fields(&mut payload, "paper_search", t0.elapsed().as_millis());
            let md = paper_search_markdown(&payload);
            Ok(tool_result_markdown_with_json(payload, md))
        }

        #[tool(
            description = "Best for: getting full metadata for a specific arXiv paper — abstract, authors, categories, PDF link, and optionally related discussion or review links. Requires an arXiv ID (e.g. 2401.12345) or URL. Not this when you need to discover papers — use arxiv_search first. Output: paper{} + optional discussions[].",
            input_schema = Arc::new(tool_input_schema_draft07::<ArxivEnrichArgs>()),
            annotations(title = "arXiv enrich", read_only_hint = true, open_world_hint = true)
        )]
        async fn arxiv_enrich(
            &self,
            params: Parameters<Option<ArxivEnrichArgs>>,
        ) -> Result<CallToolResult, McpError> {
            let args = params.0.unwrap_or_default();
            self.stats_inc_tool("arxiv_enrich");
            let t0 = std::time::Instant::now();

            let raw = args.id_or_url.unwrap_or_default().trim().to_string();
            if raw.is_empty() {
                let mut payload = serde_json::json!({
                    "ok": false,
                    "error": error_obj(ErrorCode::InvalidParams, "id_or_url must be non-empty", "Pass an arXiv ID like \"0805.3415\" or a full abs URL.")
                });
                add_envelope_fields(&mut payload, "arxiv_enrich", t0.elapsed().as_millis());
                let md = arxiv_enrich_markdown(&payload);
                return Ok(tool_result_markdown_with_json(payload, md));
            }

            let arxiv_id = if raw.contains("arxiv.org/") {
                raw.split("/abs/")
                    .nth(1)
                    .unwrap_or(&raw)
                    .trim()
                    .trim_matches('/')
                    .to_string()
            } else {
                raw.clone()
            };
            let abs_url = webpipe_local::arxiv::arxiv_abs_url(&arxiv_id);

            let paper = webpipe_local::arxiv::arxiv_lookup_by_id(
                self.http.clone(),
                arxiv_id.clone(),
                args.timeout_ms.unwrap_or(20_000),
            )
            .await
            .map_err(|e| McpError::internal_error(e.to_string(), None))?;

            let mut payload = serde_json::json!({
                "ok": true,
                "arxiv_id": arxiv_id,
                "abs_url": abs_url,
                "paper": paper,
            });

            let include_discussions = args.include_discussions.unwrap_or(false);
            if include_discussions {
                let max_discussion_urls = args.max_discussion_urls.unwrap_or(2).clamp(1, 5);

                // Cheap + bounded: do a web search for discussion/commentary pages.
                // (Extraction can be done separately via `web_search_extract` if desired.)
                let q = format!("{} discussion", abs_url);
                let r = self
                    .web_search(p(WebSearchArgs {
                        query: Some(q),
                        provider: Some("auto".to_string()),
                        auto_mode: Some("mab".to_string()),
                        max_results: Some(max_discussion_urls),
                        language: None,
                        country: None,
                        ..Default::default()
                    }))
                    .await?;
                payload["discussion_search"] = payload_from_result(&r);
            }

            add_envelope_fields(&mut payload, "arxiv_enrich", t0.elapsed().as_millis());
            let md = arxiv_enrich_markdown(&payload);
            Ok(tool_result_markdown_with_json(payload, md))
        }

        #[tool(
            description = "arXiv papers: search by topic/keyword (pass query) or get full metadata for a specific paper (pass id_or_url). Provide exactly one. Search output: papers[] with title, abstract, authors, categories, pdf_url. Enrich output: paper{} + optional discussions[].",
            input_schema = Arc::new(tool_input_schema_draft07::<ArxivArgs>()),
            annotations(title = "arXiv", read_only_hint = true, open_world_hint = true)
        )]
        async fn arxiv(
            &self,
            params: Parameters<Option<ArxivArgs>>,
        ) -> Result<CallToolResult, McpError> {
            let args = params.0.unwrap_or_default();
            self.stats_inc_tool("arxiv");
            let t0 = std::time::Instant::now();

            if args.id_or_url.is_some() {
                // --- ENRICH MODE ---
                let raw = args.id_or_url.unwrap_or_default().trim().to_string();
                if raw.is_empty() {
                    let mut payload = serde_json::json!({
                        "ok": false,
                        "error": error_obj(ErrorCode::InvalidParams, "id_or_url must be non-empty", "Pass an arXiv ID like \"0805.3415\" or a full abs URL.")
                    });
                    add_envelope_fields(&mut payload, "arxiv", t0.elapsed().as_millis());
                    let md = arxiv_enrich_markdown(&payload);
                    return Ok(tool_result_markdown_with_json(payload, md));
                }
                let arxiv_id = if raw.contains("arxiv.org/") {
                    raw.split("/abs/")
                        .nth(1)
                        .unwrap_or(&raw)
                        .trim()
                        .trim_matches('/')
                        .to_string()
                } else {
                    raw.clone()
                };
                let abs_url = webpipe_local::arxiv::arxiv_abs_url(&arxiv_id);
                let paper = webpipe_local::arxiv::arxiv_lookup_by_id(
                    self.http.clone(),
                    arxiv_id.clone(),
                    args.timeout_ms.unwrap_or(20_000),
                )
                .await
                .map_err(|e| McpError::internal_error(e.to_string(), None))?;

                let mut payload = serde_json::json!({
                    "ok": true,
                    "arxiv_id": arxiv_id,
                    "abs_url": abs_url,
                    "paper": paper,
                });
                let include_discussions = args.include_discussions.unwrap_or(false);
                if include_discussions {
                    let max_discussion_urls = args.max_discussion_urls.unwrap_or(2).clamp(1, 5);
                    let q = format!("{} discussion", abs_url);
                    let r = self
                        .web_search(p(WebSearchArgs {
                            query: Some(q),
                            provider: Some("auto".to_string()),
                            auto_mode: Some("mab".to_string()),
                            max_results: Some(max_discussion_urls),
                            language: None,
                            country: None,
                            ..Default::default()
                        }))
                        .await?;
                    payload["discussion_search"] = payload_from_result(&r);
                }
                add_envelope_fields(&mut payload, "arxiv", t0.elapsed().as_millis());
                let md = arxiv_enrich_markdown(&payload);
                return Ok(tool_result_markdown_with_json(payload, md));
            }

            // --- SEARCH MODE ---
            let query = args.query.clone().unwrap_or_default().trim().to_string();
            if query.is_empty() {
                let mut payload = serde_json::json!({
                    "ok": false,
                    "query": args.query.unwrap_or_default(),
                    "error": error_obj(ErrorCode::InvalidParams, "provide query (search) or id_or_url (enrich)", "Pass query to search arXiv papers, or id_or_url to get details for a specific paper.")
                });
                add_envelope_fields(&mut payload, "arxiv", t0.elapsed().as_millis());
                let md = arxiv_search_markdown(&payload);
                return Ok(tool_result_markdown_with_json(payload, md));
            }

            let page = args.page.unwrap_or(1).max(1);
            let per_page = args
                .per_page
                .or(args.max_results)
                .unwrap_or(10)
                .clamp(1, 50);
            let timeout_ms = args.timeout_ms.unwrap_or(20_000);
            let categories = args.categories.unwrap_or_default();
            let years = args.years.unwrap_or_default();
            let semantic_rerank = args.semantic_rerank.unwrap_or(false);
            let semantic_top_k = args.semantic_top_k.unwrap_or(per_page).clamp(1, 50);

            let mut resp = webpipe_local::arxiv::arxiv_search(
                self.http.clone(),
                query.clone(),
                categories.clone(),
                years.clone(),
                page,
                per_page,
                timeout_ms,
            )
            .await
            .map_err(|e| McpError::internal_error(e.to_string(), None))?;

            let mut semantic: serde_json::Value = serde_json::Value::Null;
            if semantic_rerank {
                let cands: Vec<(usize, usize, String)> = resp
                    .papers
                    .iter()
                    .map(|p| {
                        let text =
                            format!("__ARXIV_ID__:{}\n{}\n\n{}", p.arxiv_id, p.title, p.summary);
                        (0, text.chars().count(), text)
                    })
                    .collect();
                let sem = self
                    .semantic_rerank_chunks_best(&query, &cands, semantic_top_k)
                    .await;
                if sem.ok {
                    let mut by_id = std::collections::BTreeMap::<
                        String,
                        webpipe_local::arxiv::ArxivPaper,
                    >::new();
                    for p in &resp.papers {
                        by_id.insert(p.arxiv_id.clone(), p.clone());
                    }
                    let mut out: Vec<webpipe_local::arxiv::ArxivPaper> = Vec::new();
                    let mut seen = std::collections::HashSet::<String>::new();
                    for ch in &sem.chunks {
                        let first = ch.text.lines().next().unwrap_or("");
                        let id = first.strip_prefix("__ARXIV_ID__:").unwrap_or("").trim();
                        if id.is_empty() {
                            continue;
                        }
                        if let Some(p) = by_id.get(id) {
                            if seen.insert(p.arxiv_id.clone()) {
                                out.push(p.clone());
                            }
                        }
                        if out.len() >= semantic_top_k {
                            break;
                        }
                    }
                    if !out.is_empty() {
                        resp.papers = out;
                    } else {
                        resp.warnings.push("semantic_rerank_no_matches");
                    }
                } else {
                    resp.warnings.extend(sem.warnings.iter().copied());
                }
                semantic = serde_json::to_value(sem).unwrap_or(serde_json::Value::Null);
            }

            let mut payload = serde_json::json!({
                "ok": resp.ok,
                "query": resp.query,
                "page": resp.page,
                "per_page": resp.per_page,
                "total_results": resp.total_results,
                "papers": resp.papers,
                "warnings": resp.warnings,
                "request": {
                    "query": query,
                    "categories": categories,
                    "years": years,
                    "page": page,
                    "per_page": per_page,
                    "timeout_ms": timeout_ms,
                    "semantic_rerank": semantic_rerank,
                    "semantic_top_k": semantic_top_k
                }
            });
            if !semantic.is_null() {
                payload["request"]["semantic"] = semantic;
            }
            add_envelope_fields(&mut payload, "arxiv", t0.elapsed().as_millis());
            let md = arxiv_search_markdown(&payload);
            Ok(tool_result_markdown_with_json(payload, md))
        }

        #[tool(
            description = "Local crawl: fetch+extract starting from start_url, follow on-page links (bounded), and return merged top chunks (cache-aware; JSON output)",
            input_schema = Arc::new(tool_input_schema_draft07::<WebExploreExtractArgs>()),
            annotations(
                title = "Explore+extract",
                read_only_hint = true,
                open_world_hint = true
            )
        )]
        async fn web_explore_extract(
            &self,
            params: Parameters<Option<WebExploreExtractArgs>>,
        ) -> Result<CallToolResult, McpError> {
            let args = params.0.unwrap_or_default();
            self.stats_inc_tool("web_explore_extract");
            let t0 = std::time::Instant::now();

            let start_url = args.start_url.trim().to_string();
            if start_url.is_empty() {
                let mut payload = serde_json::json!({
                    "ok": false,
                    "error": error_obj(ErrorCode::InvalidParams, "start_url must be non-empty", "Pass a valid URL like https://example.com/docs/.")
                });
                add_envelope_fields(
                    &mut payload,
                    "web_explore_extract",
                    t0.elapsed().as_millis(),
                );
                return Ok(tool_result(payload));
            }
            let base = match reqwest::Url::parse(&start_url) {
                Ok(u) => u,
                Err(_) => {
                    let mut payload = serde_json::json!({
                        "ok": false,
                        "error": error_obj(ErrorCode::InvalidUrl, "invalid start_url", "Pass a valid absolute URL (including scheme).")
                    });
                    add_envelope_fields(
                        &mut payload,
                        "web_explore_extract",
                        t0.elapsed().as_millis(),
                    );
                    return Ok(tool_result(payload));
                }
            };
            let base_host = base.host_str().unwrap_or("").to_string();

            let query = args.query.unwrap_or_default();
            let qkey = Self::query_key(&query).unwrap_or_default();
            let q_toks: Vec<&str> = qkey
                .split(|ch: char| !ch.is_alphanumeric())
                .filter(|t| t.len() >= 2)
                .collect();

            let max_pages = args.max_pages.unwrap_or(10).clamp(1, 50);
            let max_depth = args.max_depth.unwrap_or(2).clamp(0, 5);
            let same_host_only = args.same_host_only.unwrap_or(true);
            let max_links_per_page = args.max_links_per_page.unwrap_or(50).min(500);

            let timeout_ms = args.timeout_ms.unwrap_or(20_000).min(60_000);
            let max_bytes = args.max_bytes.unwrap_or(5_000_000);
            let width = args.width.unwrap_or(100).clamp(20, 240);
            let max_chars = args.max_chars.unwrap_or(20_000).min(200_000);
            let top_chunks = args.top_chunks.unwrap_or(5).min(50);
            let max_chunk_chars = args.max_chunk_chars.unwrap_or(500).min(5_000);

            let no_network = args.no_network.unwrap_or(false);
            let cache_read = args.cache_read.unwrap_or(true);
            let cache_write = args.cache_write.unwrap_or(true);
            let cache_ttl_s = args.cache_ttl_s;
            let include_links = args.include_links.unwrap_or(false);

            // Anonymous mode: require proxy for non-localhost crawl unless cache-only.
            if privacy_mode_from_env() == PrivacyMode::Anonymous
                && !no_network
                && anon_proxy_from_env().is_none()
                && !is_localhost_url(&start_url)
            {
                let mut payload = serde_json::json!({
                    "ok": false,
                    "error": error_obj(
                        ErrorCode::NotConfigured,
                        "anonymous mode requires a proxy",
                        "Set WEBPIPE_ANON_PROXY (recommended for Tor: socks5h://127.0.0.1:9050), or set no_network=true for cache-only runs."
                    ),
                    "request": {
                        "privacy_mode": "anonymous",
                        "anon_proxy_configured": false,
                        "no_network": no_network
                    }
                });
                add_envelope_fields(
                    &mut payload,
                    "web_explore_extract",
                    t0.elapsed().as_millis(),
                );
                return Ok(tool_result(payload));
            }

            fn score_for_query(q_toks: &[&str], url: &str, label: &str) -> u64 {
                if q_toks.is_empty() {
                    return 0;
                }
                let key = webpipe_local::textprep::scrub(&format!("{url} {label}"));
                let mut s = 0u64;
                for t in q_toks {
                    if key.contains(t) {
                        s += 1;
                    }
                }
                s
            }

            #[derive(Clone)]
            struct FrontierItem {
                depth: usize,
                url: String,
                score: u64,
                ord: u64,
            }

            let mut frontier: Vec<FrontierItem> = vec![FrontierItem {
                depth: 0,
                url: start_url.clone(),
                score: score_for_query(&q_toks, &start_url, ""),
                ord: 0,
            }];
            let mut ord = 1u64;
            let mut seen: std::collections::BTreeSet<String> = std::collections::BTreeSet::new();

            let mut pages_out: Vec<serde_json::Value> = Vec::new();
            let mut merged_chunks: Vec<serde_json::Value> = Vec::new();

            while !frontier.is_empty() && pages_out.len() < max_pages {
                // Stable pick: max score, then lowest insertion order.
                let mut best_i = 0usize;
                for i in 1..frontier.len() {
                    let a = &frontier[i];
                    let b = &frontier[best_i];
                    if a.score > b.score || (a.score == b.score && a.ord < b.ord) {
                        best_i = i;
                    }
                }
                let item = frontier.swap_remove(best_i);
                if item.depth > max_depth {
                    continue;
                }
                let canon = item
                    .url
                    .split('#')
                    .next()
                    .unwrap_or(item.url.as_str())
                    .trim()
                    .to_string();
                if canon.is_empty() || !seen.insert(canon.clone()) {
                    continue;
                }

                // Apply bounded rewrites before fetch.
                let (u0, arxiv_attempts) = self.maybe_rewrite_arxiv_abs_url(&item.url);
                let (u0b, gh_issue_attempts, gh_issue_rewrote) =
                    self.maybe_rewrite_github_issue_url(&u0);
                let (u0c, gh_release_attempts, gh_release_rewrote) =
                    self.maybe_rewrite_github_release_url(&u0b);
                let (u1, gh_patch_attempts, gh_patch_warn) =
                    self.maybe_rewrite_github_pr_or_commit_url(&u0c);
                let (u2, github_blob_attempts) = self.maybe_rewrite_github_blob_url(&u1);
                let (u3, gist_attempts) = self.maybe_rewrite_gist_url(&u2);
                let (fetch_url, github_repo_attempts) = self
                    .maybe_rewrite_github_repo_url(
                        &u3,
                        timeout_ms,
                        max_bytes,
                        no_network,
                        cache_read,
                        cache_write,
                        cache_ttl_s,
                    )
                    .await;
                let arxiv_rewrote = u0 != item.url;
                let github_patch_rewrote = u1 != u0;
                let github_blob_rewrote = u2 != u1;
                let gist_rewrote = u3 != u2;
                let github_repo_rewrote = fetch_url != u3;
                let mut attempts_map = serde_json::Map::new();
                if let Some(a) = arxiv_attempts {
                    attempts_map.insert("arxiv_rewrite".to_string(), a);
                }
                if let Some(a) = gh_issue_attempts {
                    attempts_map.insert("github_issue_rewrite".to_string(), a);
                }
                if let Some(a) = gh_release_attempts {
                    attempts_map.insert("github_release_rewrite".to_string(), a);
                }
                if let Some(a) = gh_patch_attempts {
                    attempts_map.insert("github_patch_rewrite".to_string(), a);
                }
                if let Some(a) = github_blob_attempts {
                    attempts_map.insert("github_blob_rewrite".to_string(), a);
                }
                if let Some(a) = gist_attempts {
                    attempts_map.insert("gist_rewrite".to_string(), a);
                }
                if let Some(a) = github_repo_attempts {
                    attempts_map.insert("github_repo_rewrite".to_string(), a);
                }

                let req = FetchRequest {
                    url: fetch_url.clone(),
                    timeout_ms: Some(timeout_ms),
                    max_bytes: Some(max_bytes),
                    headers: BTreeMap::new(),
                    cache: FetchCachePolicy {
                        read: cache_read || no_network,
                        write: if no_network { false } else { cache_write },
                        ttl_s: cache_ttl_s,
                    },
                };

                let per_t0 = std::time::Instant::now();
                let fetched = if no_network && !url_is_localhost(&req.url) {
                    self.fetcher.cache_get(&req).ok().flatten()
                } else {
                    self.fetcher.fetch(&req).await.ok()
                };

                let Some(resp) = fetched else {
                    let mut warnings: Vec<&'static str> = Vec::new();
                    if no_network && !url_is_localhost(&req.url) {
                        warnings.push("cache_only");
                        warnings.push("no_network_may_require_warm_cache");
                    }
                    if arxiv_rewrote {
                        warnings.push("arxiv_abs_rewritten_to_pdf");
                    }
                    if gh_issue_rewrote {
                        warnings.push("github_issue_rewritten_to_api");
                    }
                    if gh_release_rewrote {
                        warnings.push("github_release_rewritten_to_api");
                    }
                    if github_patch_rewrote {
                        if let Some(w) = gh_patch_warn {
                            warnings.push(w);
                        }
                    }
                    if github_blob_rewrote {
                        warnings.push("github_blob_rewritten_to_raw");
                    }
                    if gist_rewrote {
                        warnings.push("gist_rewritten_to_raw");
                    }
                    if github_repo_rewrote {
                        warnings.push("github_repo_rewritten_to_raw_readme");
                    }
                    let mut one = serde_json::json!({
                        "ok": false,
                        "url": item.url,
                        "fetched_url": fetch_url,
                        "error": error_obj(ErrorCode::FetchFailed, "fetch failed", "Fetch failed (or cache miss in no_network mode)."),
                        "elapsed_ms": per_t0.elapsed().as_millis()
                    });
                    if !warnings.is_empty() {
                        one["warnings"] = serde_json::json!(warnings);
                        let codes = warning_codes_from(&warnings);
                        one["warning_codes"] = serde_json::json!(codes.clone());
                        one["warning_hints"] = warning_hints_from(&codes);
                    }
                    one["attempts"] = if attempts_map.is_empty() {
                        serde_json::Value::Null
                    } else {
                        serde_json::Value::Object(attempts_map)
                    };
                    pages_out.push(one);
                    continue;
                };

                let resp_bytes = resp.bytes.clone();
                let resp_ct = resp.content_type.clone();
                let resp_final_url = resp.final_url.clone();
                let resp_status = resp.status;

                let bytes2 = resp_bytes.clone();
                let ct2 = resp_ct.clone();
                let fu2 = resp_final_url.clone();
                let query0 = if query.trim().is_empty() {
                    None
                } else {
                    Some(query.clone())
                };
                // Extraction can involve HTML parsing; keep it bounded so agentic loops can't hang.
                let extract_timeout_ms = std::env::var("WEBPIPE_EXTRACT_PIPELINE_TIMEOUT_MS")
                    .ok()
                    .and_then(|s| s.trim().parse::<u64>().ok())
                    .unwrap_or(12_000);
                let pipeline = if extract_timeout_ms == 0 {
                    // Deterministic: 0ms means “skip extraction immediately”.
                    let cfg = webpipe_local::extract::ExtractPipelineCfg {
                        query: None,
                        width,
                        max_chars,
                        top_chunks,
                        max_chunk_chars,
                        include_structure: false,
                        max_outline_items: 0,
                        max_blocks: 0,
                        max_block_chars: 0,
                    };
                    let mut p =
                        webpipe_local::extract::extract_pipeline_from_bytes(&[], None, "", cfg);
                    p.extracted.warnings.push("extract_pipeline_timeout");
                    p
                } else {
                    let handle = tokio::task::spawn_blocking(move || {
                        let cfg = webpipe_local::extract::ExtractPipelineCfg {
                            query: query0.as_deref(),
                            width,
                            max_chars,
                            top_chunks,
                            max_chunk_chars,
                            include_structure: false,
                            max_outline_items: 0,
                            max_blocks: 0,
                            max_block_chars: 0,
                        };
                        webpipe_local::extract::extract_pipeline_from_bytes(
                            bytes2.as_ref(),
                            ct2.as_deref(),
                            fu2.as_str(),
                            cfg,
                        )
                    });
                    match tokio::time::timeout(
                        std::time::Duration::from_millis(extract_timeout_ms),
                        handle,
                    )
                    .await
                    {
                        Ok(join) => join.unwrap_or_else(|_| {
                            let cfg = webpipe_local::extract::ExtractPipelineCfg {
                                query: None,
                                width,
                                max_chars,
                                top_chunks,
                                max_chunk_chars,
                                include_structure: false,
                                max_outline_items: 0,
                                max_blocks: 0,
                                max_block_chars: 0,
                            };
                            webpipe_local::extract::extract_pipeline_from_bytes(&[], None, "", cfg)
                        }),
                        Err(_) => {
                            let cfg = webpipe_local::extract::ExtractPipelineCfg {
                                query: None,
                                width,
                                max_chars,
                                top_chunks,
                                max_chunk_chars,
                                include_structure: false,
                                max_outline_items: 0,
                                max_blocks: 0,
                                max_block_chars: 0,
                            };
                            let mut p = webpipe_local::extract::extract_pipeline_from_bytes(
                                &[],
                                None,
                                "",
                                cfg,
                            );
                            p.extracted.warnings.push("extract_pipeline_timeout");
                            p
                        }
                    }
                };

                let extracted = pipeline.extracted;
                let mut warnings: Vec<&'static str> = Vec::new();
                if resp.truncated {
                    warnings.push("body_truncated_by_max_bytes");
                }
                if resp.timings_ms.contains_key("cache_get_timeout")
                    || resp.timings_ms.contains_key("cache_put_timeout")
                {
                    warnings.push("cache_io_timeout");
                }
                if arxiv_rewrote {
                    warnings.push("arxiv_abs_rewritten_to_pdf");
                }
                if gh_issue_rewrote {
                    warnings.push("github_issue_rewritten_to_api");
                }
                if gh_release_rewrote {
                    warnings.push("github_release_rewritten_to_api");
                }
                if github_patch_rewrote {
                    if let Some(w) = gh_patch_warn {
                        warnings.push(w);
                    }
                }
                if github_blob_rewrote {
                    warnings.push("github_blob_rewritten_to_raw");
                }
                if gist_rewrote {
                    warnings.push("gist_rewritten_to_raw");
                }
                if github_repo_rewrote {
                    warnings.push("github_repo_rewritten_to_raw_readme");
                }
                for w in &extracted.warnings {
                    warnings.push(*w);
                }
                if no_network && !url_is_localhost(&req.url) {
                    warnings.push("cache_only");
                }

                // Optional: extract link candidates for frontier expansion (HTML only).
                let mut link_cands: Vec<webpipe_local::links::LinkCandidate> = Vec::new();
                let is_html = webpipe_local::extract::bytes_look_like_html(resp_bytes.as_ref())
                    || resp_ct
                        .as_deref()
                        .unwrap_or("")
                        .to_ascii_lowercase()
                        .starts_with("text/html");
                if is_html && max_links_per_page > 0 {
                    let links_timeout_ms = std::env::var("WEBPIPE_LINKS_TIMEOUT_MS")
                        .ok()
                        .and_then(|s| s.trim().parse::<u64>().ok())
                        .unwrap_or(1_500);
                    if links_timeout_ms == 0 {
                        warnings.push("links_timeout");
                    } else {
                        let bytes = resp_bytes.clone();
                        let base_url = resp_final_url.clone();
                        let max_links = max_links_per_page;
                        let handle = tokio::task::spawn_blocking(move || {
                            // Link candidates are usually in the head/nav; avoid parsing huge bodies.
                            let cap_bytes = 750_000usize;
                            let b: &[u8] = bytes.as_slice();
                            let b = if b.len() > cap_bytes {
                                &b[..cap_bytes]
                            } else {
                                b
                            };
                            let html = String::from_utf8_lossy(b).to_string();
                            webpipe_local::links::extract_link_candidates(
                                &html,
                                Some(base_url.as_str()),
                                max_links,
                            )
                        });
                        match tokio::time::timeout(
                            std::time::Duration::from_millis(links_timeout_ms),
                            handle,
                        )
                        .await
                        {
                            Ok(join) => {
                                link_cands = join.unwrap_or_else(|_| Vec::new());
                            }
                            Err(_) => {
                                warnings.push("links_timeout");
                            }
                        }
                    }
                }

                // Expand frontier.
                if item.depth < max_depth {
                    for lc in link_cands.iter().take(max_links_per_page) {
                        if frontier.len() >= 500 {
                            break;
                        }
                        if same_host_only {
                            if let Ok(u) = reqwest::Url::parse(&lc.url) {
                                if u.host_str().unwrap_or("") != base_host {
                                    continue;
                                }
                            }
                        }
                        let s = score_for_query(&q_toks, &lc.url, &lc.text);
                        frontier.push(FrontierItem {
                            depth: item.depth + 1,
                            url: lc.url.clone(),
                            score: s,
                            ord,
                        });
                        ord = ord.saturating_add(1);
                    }
                }

                // Merge chunks (annotate with page URL).
                for c in pipeline.chunks.clone().into_iter() {
                    let mut v = serde_json::to_value(&c).unwrap_or_else(|_| serde_json::json!({}));
                    v["page_url"] = serde_json::json!(&item.url);
                    v["depth"] = serde_json::json!(item.depth);
                    merged_chunks.push(v);
                }

                let mut one = serde_json::json!({
                    "ok": true,
                    "url": item.url,
                    "fetched_url": resp.url,
                    "final_url": resp.final_url,
                    "status": resp_status,
                    "content_type": resp_ct,
                    "bytes": resp_bytes.len(),
                    "truncated": resp.truncated,
                    "extract": {
                        "engine": extracted.engine,
                        "width": width,
                        "max_chars": max_chars,
                        "top_chunks": top_chunks,
                        "max_chunk_chars": max_chunk_chars,
                        "chunks": pipeline.chunks
                    },
                    "elapsed_ms": per_t0.elapsed().as_millis()
                });
                if include_links {
                    one["links"] = serde_json::json!(link_cands
                        .iter()
                        .map(|c| c.url.clone())
                        .collect::<Vec<_>>());
                }
                if !warnings.is_empty() {
                    one["warnings"] = serde_json::json!(warnings);
                    let codes = warning_codes_from(&warnings);
                    one["warning_codes"] = serde_json::json!(codes.clone());
                    one["warning_hints"] = warning_hints_from(&codes);
                }
                one["attempts"] = if attempts_map.is_empty() {
                    serde_json::Value::Null
                } else {
                    serde_json::Value::Object(attempts_map)
                };
                pages_out.push(one);
            }

            // Sort merged chunks by score desc (stable-ish by insertion order).
            merged_chunks.sort_by(|a, b| {
                let ascore = a.get("score").and_then(|x| x.as_u64()).unwrap_or(0);
                let bscore = b.get("score").and_then(|x| x.as_u64()).unwrap_or(0);
                bscore.cmp(&ascore)
            });
            let merged_top = merged_chunks
                .into_iter()
                .take((max_pages * top_chunks).min(50))
                .collect::<Vec<_>>();

            let mut payload = serde_json::json!({
                "ok": true,
                "start_url": start_url,
                "query": if query.trim().is_empty() { serde_json::Value::Null } else { serde_json::json!(query) },
                "request": {
                    "max_pages": max_pages,
                    "max_depth": max_depth,
                    "same_host_only": same_host_only,
                    "max_links_per_page": max_links_per_page,
                    "timeout_ms": timeout_ms,
                    "max_bytes": max_bytes,
                    "width": width,
                    "max_chars": max_chars,
                    "top_chunks": top_chunks,
                    "max_chunk_chars": max_chunk_chars,
                    "no_network": no_network,
                    "cache_read": cache_read,
                    "cache_write": cache_write,
                    "cache_ttl_s": cache_ttl_s,
                    "include_links": include_links
                },
                "pages": pages_out,
                "top_chunks": merged_top,
                "timings_ms": { "total": t0.elapsed().as_millis() }
            });
            add_envelope_fields(
                &mut payload,
                "web_explore_extract",
                t0.elapsed().as_millis(),
            );
            Ok(tool_result(payload))
        }

        #[tool(
            description = "Repo ingest (gitingest-like): list a GitHub repo tree and fetch a bounded set of files as a combined text corpus (bounded; cache-aware; JSON output)",
            input_schema = Arc::new(tool_input_schema_draft07::<RepoIngestArgs>()),
            annotations(title = "Repo ingest", read_only_hint = true, open_world_hint = true)
        )]
        async fn repo_ingest(
            &self,
            params: Parameters<Option<RepoIngestArgs>>,
        ) -> Result<CallToolResult, McpError> {
            let args = params.0.unwrap_or_default();
            self.stats_inc_tool("repo_ingest");
            let t0 = std::time::Instant::now();

            let repo_url = args.repo_url.trim().to_string();
            if repo_url.is_empty() {
                let mut payload = serde_json::json!({
                    "ok": false,
                    "error": error_obj(ErrorCode::InvalidParams, "repo_url must be non-empty", "Pass a repo URL like https://github.com/owner/repo.")
                });
                add_envelope_fields(&mut payload, "repo_ingest", t0.elapsed().as_millis());
                return Ok(tool_result(payload));
            }
            let u = match reqwest::Url::parse(&repo_url) {
                Ok(u) => u,
                Err(_) => {
                    let mut payload = serde_json::json!({
                        "ok": false,
                        "error": error_obj(ErrorCode::InvalidUrl, "invalid repo_url", "Pass a valid absolute URL (including scheme).")
                    });
                    add_envelope_fields(&mut payload, "repo_ingest", t0.elapsed().as_millis());
                    return Ok(tool_result(payload));
                }
            };
            let scheme = u.scheme().to_string();
            let parts: Vec<&str> = u.path().trim_matches('/').split('/').collect();
            if parts.len() < 2 {
                let mut payload = serde_json::json!({
                    "ok": false,
                    "error": error_obj(ErrorCode::InvalidParams, "repo_url path must be /<owner>/<repo>", "Pass a GitHub-style repo URL.")
                });
                add_envelope_fields(&mut payload, "repo_ingest", t0.elapsed().as_millis());
                return Ok(tool_result(payload));
            }
            let owner = parts[0].trim().to_string();
            let repo = parts[1].trim().trim_end_matches(".git").to_string();
            if owner.is_empty() || repo.is_empty() {
                let mut payload = serde_json::json!({
                    "ok": false,
                    "error": error_obj(ErrorCode::InvalidParams, "repo_url missing owner/repo", "Pass a GitHub-style repo URL like https://github.com/owner/repo.")
                });
                add_envelope_fields(&mut payload, "repo_ingest", t0.elapsed().as_millis());
                return Ok(tool_result(payload));
            }

            let max_files = args.max_files.unwrap_or(100).clamp(1, 500);
            let max_total_bytes = args.max_total_bytes.unwrap_or(1_000_000).min(10_000_000);
            let max_file_bytes = args.max_file_bytes.unwrap_or(200_000).min(1_000_000);
            let timeout_ms = args.timeout_ms.unwrap_or(20_000).min(60_000);
            let include_combined_text = args.include_combined_text.unwrap_or(true);
            let no_network = args.no_network.unwrap_or(false);
            let cache_read = args.cache_read.unwrap_or(true);
            let cache_write = args.cache_write.unwrap_or(true);
            let cache_ttl_s = args.cache_ttl_s;
            let include_patterns: Vec<String> = args
                .include_patterns
                .clone()
                .unwrap_or_default()
                .into_iter()
                .map(|s| s.trim().to_string())
                .filter(|s| !s.is_empty())
                .collect();
            let exclude_patterns: Vec<String> = args
                .exclude_patterns
                .clone()
                .unwrap_or_default()
                .into_iter()
                .map(|s| s.trim().to_string())
                .filter(|s| !s.is_empty())
                .collect();

            let api_base = std::env::var("WEBPIPE_GITHUB_API_BASE")
                .ok()
                .unwrap_or_else(|| "https://api.github.com".to_string())
                .trim()
                .trim_end_matches('/')
                .to_string();
            let raw_host = std::env::var("WEBPIPE_GITHUB_RAW_HOST")
                .ok()
                .unwrap_or_else(|| "raw.githubusercontent.com".to_string())
                .trim()
                .to_string();
            let github_token = Self::github_token_from_env();

            // Anonymous mode: repo_ingest uses the GitHub API + raw fetches; require proxy unless cache-only.
            if privacy_mode_from_env() == PrivacyMode::Anonymous
                && !no_network
                && anon_proxy_from_env().is_none()
                && !is_localhost_url(&repo_url)
            {
                let mut payload = serde_json::json!({
                    "ok": false,
                    "error": error_obj(
                        ErrorCode::NotConfigured,
                        "anonymous mode requires a proxy",
                        "Set WEBPIPE_ANON_PROXY (recommended for Tor: socks5h://127.0.0.1:9050), or set no_network=true for cache-only runs (note: repo listing still requires network)."
                    ),
                    "request": {
                        "repo_url": repo_url,
                        "privacy_mode": "anonymous",
                        "anon_proxy_configured": false,
                        "no_network": no_network
                    }
                });
                add_envelope_fields(&mut payload, "repo_ingest", t0.elapsed().as_millis());
                return Ok(tool_result(payload));
            }

            if no_network {
                let mut payload = serde_json::json!({
                    "ok": false,
                    "error": error_obj(
                        ErrorCode::NotSupported,
                        "repo_ingest requires network for repo listing",
                        "repo_ingest needs the GitHub API to list files. Run once with no_network=false to warm cache, then you can re-fetch raw files with no_network=true."
                    ),
                    "request": {
                        "repo_url": repo_url,
                        "no_network": true
                    }
                });
                add_envelope_fields(&mut payload, "repo_ingest", t0.elapsed().as_millis());
                return Ok(tool_result(payload));
            }

            // Resolve reference (default_branch) via API, then fall back.
            let mut reference = args.reference.clone().unwrap_or_default();
            let mut api_attempts: Vec<serde_json::Value> = Vec::new();
            if reference.trim().is_empty() {
                let url = format!("{api_base}/repos/{owner}/{repo}");
                let t1 = std::time::Instant::now();
                let mut req = self.http.get(url.clone()).header("user-agent", "webpipe");
                if let Some(tok) = github_token.as_ref() {
                    req = req.header("authorization", format!("Bearer {tok}"));
                }
                let r = req
                    .timeout(std::time::Duration::from_millis(timeout_ms))
                    .send()
                    .await;
                match r {
                    Ok(resp) => {
                        let status = resp.status().as_u16();
                        let v = resp
                            .json::<serde_json::Value>()
                            .await
                            .unwrap_or_else(|_| serde_json::json!({}));
                        let br = v
                            .get("default_branch")
                            .and_then(|x| x.as_str())
                            .unwrap_or("")
                            .trim()
                            .to_string();
                        api_attempts.push(serde_json::json!({
                            "endpoint": "repos",
                            "url": url,
                            "status": status,
                            "auth": github_token.as_ref().is_some(),
                            "elapsed_ms": t1.elapsed().as_millis()
                        }));
                        if !br.is_empty() {
                            reference = br;
                        }
                    }
                    Err(_) => {
                        api_attempts.push(serde_json::json!({
                            "endpoint": "repos",
                            "url": url,
                            "error": "request_failed",
                            "auth": github_token.as_ref().is_some(),
                            "elapsed_ms": t1.elapsed().as_millis()
                        }));
                    }
                }
            }
            if reference.trim().is_empty() {
                reference = "main".to_string();
            }

            // List tree via API.
            let tree_url =
                format!("{api_base}/repos/{owner}/{repo}/git/trees/{reference}?recursive=1");
            let t2 = std::time::Instant::now();
            let mut tree_req = self
                .http
                .get(tree_url.clone())
                .header("user-agent", "webpipe");
            if let Some(tok) = github_token.as_ref() {
                tree_req = tree_req.header("authorization", format!("Bearer {tok}"));
            }
            let tree_resp = tree_req
                .timeout(std::time::Duration::from_millis(timeout_ms))
                .send()
                .await;
            let tree_v = match tree_resp {
                Ok(resp) => {
                    api_attempts.push(serde_json::json!({
                        "endpoint": "trees",
                        "url": tree_url,
                        "status": resp.status().as_u16(),
                        "auth": github_token.as_ref().is_some(),
                        "elapsed_ms": t2.elapsed().as_millis()
                    }));
                    resp.json::<serde_json::Value>()
                        .await
                        .unwrap_or_else(|_| serde_json::json!({}))
                }
                Err(_) => {
                    api_attempts.push(serde_json::json!({
                        "endpoint": "trees",
                        "url": tree_url,
                        "error": "request_failed",
                        "auth": github_token.as_ref().is_some(),
                        "elapsed_ms": t2.elapsed().as_millis()
                    }));
                    serde_json::json!({})
                }
            };
            let tree = tree_v
                .get("tree")
                .and_then(|x| x.as_array())
                .cloned()
                .unwrap_or_default();

            fn glob_match_simple(pat: &str, text: &str) -> bool {
                // Simple glob: `*` matches any sequence, `?` matches one char.
                // Bounded DP, char-based (safe for Unicode).
                let p: Vec<char> = pat.chars().collect();
                let t: Vec<char> = text.chars().collect();
                let mut dp = vec![vec![false; t.len() + 1]; p.len() + 1];
                dp[0][0] = true;
                for i in 1..=p.len() {
                    if p[i - 1] == '*' {
                        dp[i][0] = dp[i - 1][0];
                    }
                }
                for i in 1..=p.len() {
                    for j in 1..=t.len() {
                        dp[i][j] = match p[i - 1] {
                            '*' => dp[i - 1][j] || dp[i][j - 1],
                            '?' => dp[i - 1][j - 1],
                            c => dp[i - 1][j - 1] && c == t[j - 1],
                        };
                    }
                }
                dp[p.len()][t.len()]
            }

            fn include_path(
                path: &str,
                include_patterns: &[String],
                exclude_patterns: &[String],
            ) -> bool {
                let p = path.trim();
                if p.is_empty() {
                    return false;
                }
                let pl = p.to_ascii_lowercase();
                for bad in [
                    "/.git/",
                    "node_modules/",
                    "target/",
                    "dist/",
                    "build/",
                    "vendor/",
                    ".min.",
                ] {
                    if pl.contains(bad) {
                        return false;
                    }
                }
                let mut base = false;
                if pl.starts_with("readme") || pl == "readme.md" {
                    base = true;
                }
                if pl.starts_with("docs/") || pl.starts_with("doc/") {
                    base = true;
                }
                if pl.starts_with("src/") || pl.starts_with("crates/") {
                    base = true;
                }
                let exts = [
                    ".md", ".rst", ".txt", ".rs", ".py", ".js", ".ts", ".tsx", ".go", ".java",
                    ".kt", ".c", ".h", ".cpp", ".hpp", ".toml", ".yaml", ".yml", ".json",
                ];
                base = base || exts.iter().any(|e| pl.ends_with(e));

                let mut ok = base;
                if !include_patterns.is_empty() {
                    ok = include_patterns.iter().any(|pat| glob_match_simple(pat, p));
                }
                if !ok {
                    return false;
                }
                if !exclude_patterns.is_empty()
                    && exclude_patterns.iter().any(|pat| glob_match_simple(pat, p))
                {
                    return false;
                }
                true
            }

            let mut paths: Vec<String> = Vec::new();
            for node in tree {
                let typ = node.get("type").and_then(|x| x.as_str()).unwrap_or("");
                if typ != "blob" {
                    continue;
                }
                let path = node
                    .get("path")
                    .and_then(|x| x.as_str())
                    .unwrap_or("")
                    .to_string();
                if include_path(&path, &include_patterns, &exclude_patterns) {
                    paths.push(path);
                }
            }
            paths.sort();
            paths.dedup();
            paths.truncate(max_files);

            let mut files_out: Vec<serde_json::Value> = Vec::new();
            let mut total_fetched: u64 = 0;
            let mut combined = String::new();
            let mut files_text = String::new();
            let mut warnings: Vec<&'static str> = Vec::new();

            // Pre-compute a simple directory tree for the selected paths (for gitingest-like output).
            #[derive(Default)]
            struct DirNode {
                dirs: BTreeMap<String, DirNode>,
                files: Vec<String>,
            }
            fn insert_path(root: &mut DirNode, path: &str) {
                let parts: Vec<&str> = path.split('/').filter(|s| !s.is_empty()).collect();
                if parts.is_empty() {
                    return;
                }
                let mut cur = root;
                for (i, part) in parts.iter().enumerate() {
                    if i + 1 == parts.len() {
                        cur.files.push((*part).to_string());
                    } else {
                        cur = cur.dirs.entry((*part).to_string()).or_default();
                    }
                }
            }
            fn render_tree(
                out: &mut String,
                node: &DirNode,
                prefix: &str,
                is_last: bool,
                name: &str,
            ) {
                let branch = if prefix.is_empty() || is_last {
                    "└── "
                } else {
                    "├── "
                };
                out.push_str(prefix);
                out.push_str(branch);
                out.push_str(name);
                out.push('\n');

                let child_prefix = if prefix.is_empty() {
                    "    ".to_string()
                } else if is_last {
                    format!("{prefix}    ")
                } else {
                    format!("{prefix}│   ")
                };

                let mut dirs: Vec<(&String, &DirNode)> = node.dirs.iter().collect();
                dirs.sort_by(|a, b| a.0.cmp(b.0));
                let mut files = node.files.clone();
                files.sort();
                files.dedup();

                let total = dirs.len() + files.len();
                let mut idx = 0usize;
                for (dname, dnode) in dirs {
                    idx += 1;
                    let last = idx == total;
                    render_tree(out, dnode, &child_prefix, last, dname);
                }
                for fname in files {
                    idx += 1;
                    let last = idx == total;
                    let branch2 = if last { "└── " } else { "├── " };
                    out.push_str(&child_prefix);
                    out.push_str(branch2);
                    out.push_str(&fname);
                    out.push('\n');
                }
            }

            let mut tree_root = DirNode::default();
            for p in &paths {
                insert_path(&mut tree_root, p);
            }
            let root_name = format!("{owner}-{repo}/");
            let mut tree_text = String::new();
            tree_text.push_str("Directory structure:\n");
            render_tree(&mut tree_text, &tree_root, "", true, &root_name);

            let mut files_analyzed: u64 = 0;
            for path in paths {
                if total_fetched >= max_total_bytes {
                    warnings.push("repo_ingest_total_bytes_capped");
                    break;
                }
                let t = std::time::Instant::now();
                let mut attempts: Vec<serde_json::Value> = Vec::new();
                let mut best_url: Option<String> = None;

                // Attempt 1: raw host (fast path for public repos).
                let raw_url = format!("{scheme}://{raw_host}/{owner}/{repo}/{reference}/{path}");
                let raw_req = FetchRequest {
                    url: raw_url.clone(),
                    timeout_ms: Some(timeout_ms),
                    max_bytes: Some(max_file_bytes),
                    headers: BTreeMap::new(),
                    cache: FetchCachePolicy {
                        read: cache_read,
                        write: cache_write,
                        ttl_s: cache_ttl_s,
                    },
                };
                let mut best: Option<webpipe_core::FetchResponse> = None;
                match self.fetcher.fetch(&raw_req).await {
                    Ok(r) => {
                        attempts.push(serde_json::json!({
                            "kind": "raw_host",
                            "url": raw_url,
                            "status": r.status,
                            "bytes": r.bytes.len()
                        }));
                        // Only accept as "best" when it looks like a real success.
                        if r.status >= 200 && r.status < 300 && !r.bytes.is_empty() {
                            best = Some(r);
                            best_url = Some(raw_req.url.clone());
                        }
                    }
                    Err(_) => {
                        attempts.push(serde_json::json!({
                            "kind": "raw_host",
                            "url": raw_url,
                            "error": "request_failed"
                        }));
                    }
                }

                // Attempt 2 (bounded fallback): GitHub contents API raw media type.
                if best.is_none() && github_token.is_some() {
                    let mut base_u =
                        reqwest::Url::parse(&format!("{api_base}/repos/{owner}/{repo}/contents"))
                            .map_err(|_| {
                            McpError::internal_error("invalid WEBPIPE_GITHUB_API_BASE", None)
                        })?;
                    {
                        let mut segs = base_u.path_segments_mut().expect("path_segments");
                        for seg in path.split('/') {
                            if !seg.is_empty() {
                                segs.push(seg);
                            }
                        }
                    }
                    base_u.query_pairs_mut().append_pair("ref", &reference);
                    let contents_url = base_u.to_string();

                    let mut headers = BTreeMap::new();
                    headers.insert(
                        "accept".to_string(),
                        "application/vnd.github.raw+json".to_string(),
                    );
                    headers.insert("user-agent".to_string(), "webpipe".to_string());
                    if let Some(tok) = github_token.as_ref() {
                        headers.insert("authorization".to_string(), format!("Bearer {tok}"));
                    }

                    let api_req = FetchRequest {
                        url: contents_url.clone(),
                        timeout_ms: Some(timeout_ms),
                        max_bytes: Some(max_file_bytes),
                        headers,
                        cache: FetchCachePolicy {
                            read: cache_read,
                            write: cache_write,
                            ttl_s: cache_ttl_s,
                        },
                    };
                    match self.fetcher.fetch(&api_req).await {
                        Ok(r) => {
                            attempts.push(serde_json::json!({
                                "kind": "api_contents_raw",
                                "url": contents_url,
                                "status": r.status,
                                "bytes": r.bytes.len()
                            }));
                            if r.status >= 200 && r.status < 300 && !r.bytes.is_empty() {
                                best = Some(r);
                                best_url = Some(api_req.url.clone());
                            }
                        }
                        Err(_) => {
                            attempts.push(serde_json::json!({
                                "kind": "api_contents_raw",
                                "url": contents_url,
                                "error": "request_failed"
                            }));
                        }
                    }
                }

                let Some(r) = best else {
                    warnings.push("repo_ingest_fetch_failed");
                    files_out.push(serde_json::json!({
                        "path": path,
                        "url": best_url.unwrap_or_else(|| raw_req.url.clone()),
                        "ok": false,
                        "attempts": attempts,
                        "error": "fetch_failed",
                        "elapsed_ms": t.elapsed().as_millis()
                    }));
                    continue;
                };

                let bytes = r.bytes;
                let mut is_binary = bytes.contains(&0);
                // Heuristic: huge non-text blobs should be treated as binary even without NUL.
                if bytes.len() > 500_000 {
                    is_binary = true;
                }
                let truncated = r.truncated;
                total_fetched = total_fetched.saturating_add(bytes.len() as u64);
                let text = if is_binary {
                    warnings.push("repo_ingest_skipped_binary");
                    String::new()
                } else {
                    String::from_utf8_lossy(bytes.as_ref()).to_string()
                };
                let (excerpt, _n, clipped) = Self::truncate_to_chars(&text, 10_000);
                files_out.push(serde_json::json!({
                    "path": path,
                    "url": best_url.unwrap_or_else(|| raw_req.url.clone()),
                    "status": r.status,
                    "content_type": r.content_type,
                    "bytes": bytes.len(),
                    "truncated": truncated,
                    "binary": is_binary,
                    "excerpt_chars": excerpt.chars().count(),
                    "excerpt_truncated": clipped,
                    "attempts": attempts,
                    "elapsed_ms": t.elapsed().as_millis()
                }));
                if include_combined_text && !is_binary {
                    files_analyzed = files_analyzed.saturating_add(1);
                    // Gitingest-like delimiter.
                    let delim = "================================================";
                    files_text.push_str(delim);
                    files_text.push('\n');
                    files_text.push_str("FILE: ");
                    files_text.push_str(&path);
                    files_text.push('\n');
                    files_text.push_str(delim);
                    files_text.push('\n');
                    files_text.push_str(&text);
                    files_text.push_str("\n\n");
                }
            }

            // Bound combined output.
            let mut combined_out = serde_json::Value::Null;
            if include_combined_text {
                let summary_text =
                    format!("Repository: {owner}/{repo}\nFiles analyzed: {files_analyzed}\n");
                combined.clear();
                combined.push_str(&summary_text);
                combined.push('\n');
                combined.push_str(&tree_text);
                combined.push('\n');
                combined.push_str(&files_text);

                let (t, _n, clipped) = Self::truncate_to_chars(&combined, 200_000);
                let approx_tokens = (t.chars().count() as f64 / 4.0).ceil() as u64;
                combined_out = serde_json::json!(t);
                if clipped {
                    warnings.push("repo_ingest_combined_text_truncated");
                }

                // Additive: expose gitingest-style components.
                // Keep these bounded too.
                let (sum2, _n2, _) = Self::truncate_to_chars(&summary_text, 5_000);
                let (tree2, _n3, _) = Self::truncate_to_chars(&tree_text, 50_000);
                let (files2, _n4, files_clipped) = Self::truncate_to_chars(&files_text, 200_000);
                if files_clipped {
                    warnings.push("repo_ingest_files_text_truncated");
                }
                // We attach these after payload creation below.
                // (see payload building)
                // Store in locals for insertion.
                let summary_text_bounded = sum2;
                let tree_text_bounded = tree2;
                let files_text_bounded = files2;

                // Create payload below and insert these fields there.
                // We'll temporarily stash these in warnings via payload locals.
                // (Rust: we can't move after borrowing `warnings`, so compute here.)
                let mut payload = serde_json::json!({
                    "ok": true,
                    "repo": { "owner": owner, "name": repo },
                    "repo_url": repo_url,
                    "reference": reference,
                    "request": {
                        "max_files": max_files,
                        "max_total_bytes": max_total_bytes,
                        "max_file_bytes": max_file_bytes,
                        "timeout_ms": timeout_ms,
                        "include_combined_text": include_combined_text,
                        "include_patterns": include_patterns,
                        "exclude_patterns": exclude_patterns,
                        "github_token_configured": github_token.as_ref().is_some(),
                        "cache": { "read": cache_read, "write": cache_write, "ttl_s": cache_ttl_s },
                        "api_base": api_base,
                        "raw_host": raw_host
                    },
                    "api_attempts": api_attempts,
                    "files": files_out,
                    "total_bytes_fetched": total_fetched,
                    "combined_text": combined_out,
                    "summary_text": summary_text_bounded,
                    "tree_text": tree_text_bounded,
                    "files_text": files_text_bounded,
                    "estimated_tokens": approx_tokens,
                    "timings_ms": { "total": t0.elapsed().as_millis() }
                });
                if !warnings.is_empty() {
                    warnings.sort();
                    warnings.dedup();
                    payload["warnings"] = serde_json::json!(warnings);
                    let codes = warning_codes_from(&warnings);
                    payload["warning_codes"] = serde_json::json!(codes.clone());
                    payload["warning_hints"] = warning_hints_from(&codes);
                }
                add_envelope_fields(&mut payload, "repo_ingest", t0.elapsed().as_millis());
                return Ok(tool_result(payload));
            }

            let mut payload = serde_json::json!({
                "ok": true,
                "repo": { "owner": owner, "name": repo },
                "repo_url": repo_url,
                "reference": reference,
                "request": {
                    "max_files": max_files,
                    "max_total_bytes": max_total_bytes,
                    "max_file_bytes": max_file_bytes,
                    "timeout_ms": timeout_ms,
                    "include_combined_text": include_combined_text,
                    "include_patterns": include_patterns,
                    "exclude_patterns": exclude_patterns,
                    "github_token_configured": github_token.as_ref().is_some(),
                    "cache": { "read": cache_read, "write": cache_write, "ttl_s": cache_ttl_s },
                    "api_base": api_base,
                    "raw_host": raw_host
                },
                "api_attempts": api_attempts,
                "files": files_out,
                "total_bytes_fetched": total_fetched,
                "combined_text": combined_out,
                "timings_ms": { "total": t0.elapsed().as_millis() }
            });
            if !warnings.is_empty() {
                warnings.sort();
                warnings.dedup();
                payload["warnings"] = serde_json::json!(warnings);
                let codes = warning_codes_from(&warnings);
                payload["warning_codes"] = serde_json::json!(codes.clone());
                payload["warning_hints"] = warning_hints_from(&codes);
            }
            add_envelope_fields(&mut payload, "repo_ingest", t0.elapsed().as_millis());
            Ok(tool_result(payload))
        }

        #[tool(
            description = "Sitemap discovery: fetch robots.txt/sitemaps (bounded), extract URL list, and optionally extract+rank content for a query (bounded; cache-aware; JSON output)",
            input_schema = Arc::new(tool_input_schema_draft07::<WebSitemapExtractArgs>()),
            annotations(
                title = "Sitemap+extract",
                read_only_hint = true,
                open_world_hint = true
            )
        )]
        async fn web_sitemap_extract(
            &self,
            params: Parameters<Option<WebSitemapExtractArgs>>,
        ) -> Result<CallToolResult, McpError> {
            let args = params.0.unwrap_or_default();
            self.stats_inc_tool("web_sitemap_extract");
            let t0 = std::time::Instant::now();

            let site_url = args.site_url.trim().to_string();
            if site_url.is_empty() {
                let mut payload = serde_json::json!({
                    "ok": false,
                    "error": error_obj(ErrorCode::InvalidParams, "site_url must be non-empty", "Pass a site root URL like https://docs.example.com/.")
                });
                add_envelope_fields(
                    &mut payload,
                    "web_sitemap_extract",
                    t0.elapsed().as_millis(),
                );
                return Ok(tool_result(payload));
            }
            let base = match reqwest::Url::parse(&site_url) {
                Ok(u) => u,
                Err(_) => {
                    let mut payload = serde_json::json!({
                        "ok": false,
                        "error": error_obj(ErrorCode::InvalidUrl, "invalid site_url", "Pass a valid absolute URL (including scheme).")
                    });
                    add_envelope_fields(
                        &mut payload,
                        "web_sitemap_extract",
                        t0.elapsed().as_millis(),
                    );
                    return Ok(tool_result(payload));
                }
            };
            let restrict_prefix = args.restrict_prefix.unwrap_or(true);
            let try_default_sitemap = args.try_default_sitemap.unwrap_or(true);
            let max_sitemaps = args.max_sitemaps.unwrap_or(3).clamp(1, 10);
            let max_urls = args.max_urls.unwrap_or(200).clamp(1, 2000);
            let timeout_ms = args.timeout_ms.unwrap_or(20_000).min(60_000);
            let max_bytes = args.max_bytes.unwrap_or(2_000_000).min(10_000_000);
            let no_network = args.no_network.unwrap_or(false);
            let cache_read = args.cache_read.unwrap_or(true);
            let cache_write = args.cache_write.unwrap_or(true);
            let cache_ttl_s = args.cache_ttl_s;

            let query = args.query.unwrap_or_default();
            let do_extract = args.extract.unwrap_or_else(|| !query.trim().is_empty());

            // Anonymous mode: require proxy for non-localhost sitemap discovery unless cache-only.
            if privacy_mode_from_env() == PrivacyMode::Anonymous
                && !no_network
                && anon_proxy_from_env().is_none()
                && !is_localhost_url(&site_url)
            {
                let mut payload = serde_json::json!({
                    "ok": false,
                    "error": error_obj(
                        ErrorCode::NotConfigured,
                        "anonymous mode requires a proxy",
                        "Set WEBPIPE_ANON_PROXY (recommended for Tor: socks5h://127.0.0.1:9050), or set no_network=true for cache-only runs."
                    ),
                    "request": {
                        "site_url": site_url,
                        "privacy_mode": "anonymous",
                        "anon_proxy_configured": false,
                        "no_network": no_network
                    }
                });
                add_envelope_fields(
                    &mut payload,
                    "web_sitemap_extract",
                    t0.elapsed().as_millis(),
                );
                return Ok(tool_result(payload));
            }

            fn parse_sitemaps_from_robots(txt: &str) -> Vec<String> {
                let mut out = Vec::new();
                for line in txt.lines() {
                    let l = line.trim();
                    if l.len() < 8 {
                        continue;
                    }
                    let ll = l.to_ascii_lowercase();
                    if ll.starts_with("sitemap:") {
                        let rest = l.split_once(':').map(|x| x.1).unwrap_or("").trim();
                        if !rest.is_empty() {
                            out.push(rest.to_string());
                        }
                    }
                }
                out
            }

            fn maybe_gunzip(bytes: &[u8]) -> Option<Vec<u8>> {
                use std::io::Read as _;
                let mut out = Vec::new();
                let mut gz = flate2::read::GzDecoder::new(bytes);
                gz.read_to_end(&mut out).ok()?;
                Some(out)
            }

            fn parse_loc_urls(xml: &str, max: usize) -> Vec<String> {
                // Minimal sitemap parser: find <loc>...</loc> tags.
                // This avoids bringing in a heavy DOM parser and is deterministic.
                let mut out = Vec::new();
                let mut i = 0usize;
                while out.len() < max {
                    let Some(s) = xml[i..].find("<loc>") else {
                        break;
                    };
                    let start = i + s + "<loc>".len();
                    let Some(e) = xml[start..].find("</loc>") else {
                        break;
                    };
                    let end = start + e;
                    let u = xml[start..end].trim();
                    if !u.is_empty() {
                        out.push(u.to_string());
                    }
                    i = end + "</loc>".len();
                }
                out
            }

            fn looks_like_sitemap_index(xml: &str) -> bool {
                let x = xml.to_ascii_lowercase();
                x.contains("<sitemapindex") && x.contains("<sitemap")
            }

            let mut attempts: Vec<serde_json::Value> = Vec::new();
            let mut sitemap_urls: Vec<String> = Vec::new();

            // robots.txt (optional)
            let robots_url = base.join("robots.txt").ok().map(|u| u.to_string());
            if let Some(robots_url) = robots_url {
                let req = FetchRequest {
                    url: robots_url.clone(),
                    timeout_ms: Some(timeout_ms),
                    max_bytes: Some(max_bytes.min(500_000)),
                    headers: BTreeMap::new(),
                    cache: FetchCachePolicy {
                        read: cache_read || no_network,
                        write: if no_network { false } else { cache_write },
                        ttl_s: cache_ttl_s,
                    },
                };
                let t = std::time::Instant::now();
                let fetched = if no_network && !url_is_localhost(&req.url) {
                    self.fetcher.cache_get(&req).ok().flatten()
                } else {
                    self.fetcher.fetch(&req).await.ok()
                };
                if let Some(r) = fetched {
                    let txt = String::from_utf8_lossy(r.bytes.as_ref()).to_string();
                    let mut s = parse_sitemaps_from_robots(&txt);
                    sitemap_urls.append(&mut s);
                    attempts.push(serde_json::json!({
                        "kind": "robots",
                        "url": robots_url,
                        "ok": true,
                        "status": r.status,
                        "elapsed_ms": t.elapsed().as_millis()
                    }));
                } else {
                    attempts.push(serde_json::json!({
                        "kind": "robots",
                        "url": robots_url,
                        "ok": false,
                        "elapsed_ms": t.elapsed().as_millis()
                    }));
                }
            }

            if try_default_sitemap {
                // Common defaults:
                // - relative to provided site_url (e.g. https://docs.example.com/sitemap.xml)
                // - at origin root (e.g. https://docs.example.com/sitemap.xml when site_url is https://docs.example.com/docs/)
                if let Some(host) = base.host_str() {
                    let mut root = base.clone();
                    root.set_path("/");
                    root.set_query(None);
                    root.set_fragment(None);
                    if let Ok(u) = root.join("sitemap.xml") {
                        sitemap_urls.push(u.to_string());
                    } else {
                        // Fall back: construct manually when join fails (should be rare).
                        sitemap_urls.push(format!("{}://{host}/sitemap.xml", root.scheme()));
                    }
                }
                if let Ok(u) = base.join("sitemap.xml") {
                    sitemap_urls.push(u.to_string());
                }
            }
            // Preserve insertion order (robots-derived first), but dedup deterministically.
            let mut seen = std::collections::BTreeSet::<String>::new();
            let mut s2: Vec<String> = Vec::new();
            for u in sitemap_urls.into_iter() {
                let k = u.trim().to_string();
                if k.is_empty() {
                    continue;
                }
                if seen.insert(k.clone()) {
                    s2.push(k);
                }
            }
            sitemap_urls = s2;
            sitemap_urls.truncate(max_sitemaps);

            let mut discovered: Vec<String> = Vec::new();
            // BFS over sitemap URLs (handles sitemapindex indirection).
            let mut queue: std::collections::VecDeque<String> =
                sitemap_urls.iter().cloned().collect();
            let mut seen_sitemaps = std::collections::BTreeSet::<String>::new();
            let mut fetched_sitemaps = 0usize;
            while let Some(su) = queue.pop_front() {
                if fetched_sitemaps >= max_sitemaps {
                    break;
                }
                let su0 = su.trim().to_string();
                if su0.is_empty() || !seen_sitemaps.insert(su0.clone()) {
                    continue;
                }
                let req = FetchRequest {
                    url: su0.clone(),
                    timeout_ms: Some(timeout_ms),
                    max_bytes: Some(max_bytes),
                    headers: BTreeMap::new(),
                    cache: FetchCachePolicy {
                        read: cache_read || no_network,
                        write: if no_network { false } else { cache_write },
                        ttl_s: cache_ttl_s,
                    },
                };
                let t = std::time::Instant::now();
                let fetched = if no_network && !url_is_localhost(&req.url) {
                    self.fetcher.cache_get(&req).ok().flatten()
                } else {
                    self.fetcher.fetch(&req).await.ok()
                };
                if let Some(r) = fetched {
                    let mut bytes = r.bytes.clone();
                    let ct = r.content_type.as_deref().unwrap_or("").to_ascii_lowercase();
                    let is_gz = su0.to_ascii_lowercase().ends_with(".gz")
                        || ct.contains("gzip")
                        || ct.contains("application/x-gzip");
                    if is_gz {
                        if let Some(b) = maybe_gunzip(&bytes) {
                            bytes = b;
                        }
                    }
                    let xml = String::from_utf8_lossy(bytes.as_ref()).to_string();
                    if looks_like_sitemap_index(&xml) {
                        // Extract sitemap URLs from <loc> and enqueue them.
                        for u in parse_loc_urls(&xml, 2000) {
                            queue.push_back(u);
                        }
                    } else {
                        let mut locs = parse_loc_urls(&xml, max_urls.saturating_mul(2).min(20_000));
                        discovered.append(&mut locs);
                    }
                    attempts.push(serde_json::json!({
                        "kind": "sitemap",
                        "url": su0,
                        "ok": true,
                        "status": r.status,
                        "elapsed_ms": t.elapsed().as_millis()
                    }));
                    fetched_sitemaps += 1;
                } else {
                    attempts.push(serde_json::json!({
                        "kind": "sitemap",
                        "url": su0,
                        "ok": false,
                        "elapsed_ms": t.elapsed().as_millis()
                    }));
                    fetched_sitemaps += 1;
                }
                if discovered.len() >= max_urls {
                    break;
                }
            }

            // Normalize + restrict
            let prefix = if restrict_prefix {
                site_url.clone()
            } else {
                String::new()
            };
            let mut urls: Vec<String> = Vec::new();
            for u in discovered {
                if urls.len() >= max_urls {
                    break;
                }
                let uu = u.trim();
                if uu.is_empty() {
                    continue;
                }
                if restrict_prefix && !uu.starts_with(prefix.as_str()) {
                    continue;
                }
                urls.push(uu.to_string());
            }
            urls.sort();
            urls.dedup();
            urls.truncate(max_urls);

            let mut payload = serde_json::json!({
                "ok": true,
                "site_url": site_url,
                "request": {
                    "max_sitemaps": max_sitemaps,
                    "max_urls": max_urls,
                    "restrict_prefix": restrict_prefix,
                    "try_default_sitemap": try_default_sitemap,
                    "timeout_ms": timeout_ms,
                    "max_bytes": max_bytes,
                    "no_network": no_network,
                    "cache": { "read": cache_read, "write": cache_write, "ttl_s": cache_ttl_s },
                    "extract": do_extract
                },
                "attempts": attempts,
                "sitemaps": sitemap_urls,
                "urls": urls
            });

            if do_extract && !urls.is_empty() {
                let fetch_backend = args.fetch_backend.unwrap_or_else(|| "local".to_string());
                let max_results = args.max_results.unwrap_or(0);
                let width = args.width.unwrap_or(100);
                let max_chars = args.max_chars.unwrap_or(20_000);
                let top_chunks = args.top_chunks.unwrap_or(5);
                let max_chunk_chars = args.max_chunk_chars.unwrap_or(500);

                let r = self
                    .web_search_extract(p(WebSearchExtractArgs {
                        query: if query.trim().is_empty() {
                            None
                        } else {
                            Some(query.clone())
                        },
                        urls: Some(urls.clone()),
                        url_selection_mode: Some("query_rank".to_string()),
                        provider: Some("auto".to_string()),
                        auto_mode: Some("fallback".to_string()),
                        selection_mode: Some("pareto".to_string()),
                        fetch_backend: Some(fetch_backend),
                        no_network: Some(no_network),
                        firecrawl_fallback_on_empty_extraction: Some(false),
                        firecrawl_fallback_on_low_signal: Some(false),
                        render_fallback_on_empty_extraction: Some(false),
                        render_fallback_on_low_signal: Some(false),
                        max_results: if max_results == 0 {
                            None
                        } else {
                            Some(max_results)
                        },
                        max_urls: Some(10),
                        timeout_ms: Some(timeout_ms),
                        max_bytes: Some(max_bytes.min(5_000_000)),
                        retry_on_truncation: Some(false),
                        truncation_retry_max_bytes: None,
                        width: Some(width),
                        max_chars: Some(max_chars),
                        top_chunks: Some(top_chunks),
                        max_chunk_chars: Some(max_chunk_chars),
                        include_links: Some(false),
                        include_structure: Some(false),
                        max_outline_items: None,
                        max_blocks: None,
                        max_block_chars: None,
                        semantic_rerank: Some(false),
                        semantic_top_k: None,
                        max_links: None,
                        include_text: Some(false),
                        cache_read: Some(cache_read),
                        cache_write: Some(cache_write),
                        cache_ttl_s,
                        exploration: Some("wide".to_string()),
                        agentic: Some(false),
                        agentic_selector: None,
                        agentic_max_search_rounds: None,
                        agentic_frontier_max: None,
                        planner_max_calls: Some(0),
                        compact: Some(true),
                        ..Default::default()
                    }))
                    .await?;
                payload["extract"] = payload_from_result(&r);
            }

            add_envelope_fields(
                &mut payload,
                "web_sitemap_extract",
                t0.elapsed().as_millis(),
            );
            Ok(tool_result(payload))
        }

        #[tool(
            description = "Best for: any question requiring web evidence — research, current info, library docs, papers, or any query where you don't know the exact URL. Not this when you already have the URL — use web_extract. Not this for raw bytes/status — use web_fetch.\n\nOutput (full by default): top_chunks[] + results[] + request + search.steps + warning_codes. Set minimal_output=true for a compact response (~10x smaller): top_chunks + warning_codes only.\n\nModes:\n- urls-mode: pass urls=[...] — NO API KEY REQUIRED, works out of the box\n- search-mode: pass query=... — requires a search provider key (WEBPIPE_BRAVE_API_KEY etc.)\n- cache-corpus: pass query=... + no_network=true to search local cache only\n\nPresets (exploration param): balanced (default) | deep (agentic discovery) | smart (balanced+agentic).\nFor full page text: set include_text=true (bounded by max_chars).\nFor JS-heavy pages: set fetch_backend=render (requires Playwright) or fetch_backend=firecrawl.",
            input_schema = Arc::new(tool_input_schema_draft07::<WebSearchExtractArgs>()),
            annotations(title = "Evidence pack", read_only_hint = true, open_world_hint = true)
        )]
        async fn search_evidence(
            &self,
            params: Parameters<Option<WebSearchExtractArgs>>,
        ) -> Result<CallToolResult, McpError> {
            self.web_search_extract(params).await
        }

        #[tool(
            description = "LEGACY NAME (debug mode). Prefer `search_evidence`.\n\nSearch (or use urls) → fetch → extract → rank chunks (bounded; cache-aware; Markdown + structured JSON output). Supports fetch_backend=render for JS-heavy pages (requires Playwright).",
            input_schema = Arc::new(tool_input_schema_draft07::<WebSearchExtractArgs>()),
            annotations(
                title = "Search+extract (legacy)",
                read_only_hint = true,
                open_world_hint = true
            )
        )]
        pub(crate) async fn _web_search_extract_debug(
            &self,
            params: Parameters<Option<WebSearchExtractArgs>>,
        ) -> Result<CallToolResult, McpError> {
            self.web_search_extract(params).await
        }

        pub(crate) async fn web_search_extract(
            &self,
            params: Parameters<Option<WebSearchExtractArgs>>,
        ) -> Result<CallToolResult, McpError> {
            let args = params.0.unwrap_or_default();
            self.stats_inc_tool("web_search_extract");
            let t0 = std::time::Instant::now();

            // Cursor UX guardrail: when the user provides `urls=[...]`, agentic discovery should not
            // silently wander to unrelated hosts (e.g. Wikipedia language variants) and replace the
            // user's intended sources. We allow discovery *within* the provided hosts.
            let url_scope_hosts: Option<std::collections::BTreeSet<String>> = args
                .urls
                .as_ref()
                .map(|us| {
                    let mut hs = std::collections::BTreeSet::<String>::new();
                    for u in us {
                        if let Ok(p) = reqwest::Url::parse(u.trim()) {
                            if let Some(h) = p.host_str() {
                                let h = h.trim().to_ascii_lowercase();
                                if !h.is_empty() {
                                    hs.insert(h);
                                }
                            }
                        }
                    }
                    hs
                })
                .and_then(|hs| if hs.is_empty() { None } else { Some(hs) });
            let user_urls_provided = args.urls.is_some();
            let user_urls_len = args.urls.as_ref().map(|u| u.len()).unwrap_or(0);

            // Track which knobs were explicitly provided so presets can fill only defaults.
            let max_results_user = args.max_results.is_some();
            let max_urls_user = args.max_urls.is_some();
            let max_chars_user = args.max_chars.is_some();
            let top_chunks_user = args.top_chunks.is_some();
            let max_chunk_chars_user = args.max_chunk_chars.is_some();
            let include_structure_user = args.include_structure.is_some();

            let mut max_results = args.max_results.unwrap_or(5).clamp(1, 20);
            let mut max_urls = args.max_urls.unwrap_or(3).clamp(1, 10);
            let timeout_ms = args.timeout_ms.unwrap_or(20_000);
            let max_bytes = args.max_bytes.unwrap_or(5_000_000);
            let width = args.width.unwrap_or(100).clamp(20, 240);
            let deadline_ms_raw = args
                .deadline_ms
                .unwrap_or_else(|| timeout_ms.saturating_mul((max_urls as u64).saturating_add(2)));
            let deadline_ms = if args.deadline_ms.is_some() {
                // Respect explicit caller deadlines, including small values (useful for quick “bail out”
                // workflows and tests). Still cap the worst case.
                deadline_ms_raw.min(300_000)
            } else {
                // Default overall budget should not be too tiny, or the tool will almost always return
                // partial results even for healthy URLs.
                deadline_ms_raw.clamp(5_000, 300_000)
            };
            let deadline = t0 + std::time::Duration::from_millis(deadline_ms);

            let domains_allow: Vec<String> = args
                .domains_allow
                .clone()
                .unwrap_or_default()
                .into_iter()
                .filter_map(|s| normalize_domain_rule(&s))
                .collect();
            let domains_deny: Vec<String> = args
                .domains_deny
                .clone()
                .unwrap_or_default()
                .into_iter()
                .filter_map(|s| normalize_domain_rule(&s))
                .collect();
            // Defaults tuned for research use cases (more content, more chunks).
            let mut max_chars = args.max_chars.unwrap_or(30_000).min(200_000);
            let mut top_chunks = args.top_chunks.unwrap_or(8).min(50);
            let mut max_chunk_chars = args.max_chunk_chars.unwrap_or(800).min(5_000);
            let include_links = args.include_links.unwrap_or(false);
            let max_links = args.max_links.unwrap_or(25).min(500);
            let include_text = args.include_text.unwrap_or(false);
            // Default to structure output: it improves chunk selection and debuggability,
            // and it is bounded by max_outline_items/max_blocks/max_block_chars.
            let include_structure = args.include_structure.unwrap_or(true);
            let max_outline_items = args.max_outline_items.unwrap_or(25).min(200);
            let max_blocks = args.max_blocks.unwrap_or(40).min(200);
            let max_block_chars = args.max_block_chars.unwrap_or(400).min(2000);
            let semantic_rerank = args.semantic_rerank.unwrap_or(false);
            let semantic_top_k = args.semantic_top_k.unwrap_or(5).min(50);
            let compact = args.compact.unwrap_or(true);
            let minimal_output = args.minimal_output.unwrap_or(false);
            let retry_on_truncation = args.retry_on_truncation.unwrap_or(false);
            let truncation_retry_max_bytes = args.truncation_retry_max_bytes;
            // Default to agentic loop only when we're discovering URLs (search-mode).
            //
            // When the caller provides `urls=[...]`, “agentic discovery” is surprising: it can
            // legitimately wander to generic portal pages within the same host (e.g. /models) and
            // bury the explicitly provided URL. If callers want that behavior, they can opt in.
            let agentic_default = args.urls.is_none();
            let mut agentic = args.agentic.unwrap_or(agentic_default);
            let agentic_selector = args.agentic_selector.unwrap_or_else(|| "auto".to_string());

            let cache_read = args.cache_read.unwrap_or(true);
            let cache_write = args.cache_write.unwrap_or(true);
            let cache_ttl_s = args.cache_ttl_s;

            let requested_provider = args.provider.unwrap_or_else(|| "auto".to_string());
            let requested_auto_mode = args.auto_mode.unwrap_or_else(|| "fallback".to_string());
            // Prefer Pareto by default: tends to produce cleaner evidence packs.
            let selection_mode = args.selection_mode.unwrap_or_else(|| "pareto".to_string());
            let fetch_backend = args.fetch_backend.unwrap_or_else(|| "local".to_string());
            let no_network = args.no_network.unwrap_or(false);
            let cache_read_effective = cache_read || no_network;
            let cache_write_effective = if no_network { false } else { cache_write };

            // Deterministic fail-fast: allow disabling the render backend without relying on runtime
            // Node/Playwright detection (useful for tests and environments where Node isn't installed).
            if fetch_backend == "render"
                && matches!(
                    std::env::var("WEBPIPE_RENDER_DISABLE")
                        .unwrap_or_default()
                        .trim()
                        .to_ascii_lowercase()
                        .as_str(),
                    "1" | "true" | "yes" | "on"
                )
            {
                let mut payload = serde_json::json!({
                    "ok": false,
                    "error": error_obj(
                        ErrorCode::NotConfigured,
                        "render backend disabled (WEBPIPE_RENDER_DISABLE)",
                        "Unset WEBPIPE_RENDER_DISABLE to use fetch_backend=\"render\"."
                    ),
                    "request": { "fetch_backend": fetch_backend, "no_network": no_network }
                });
                add_envelope_fields(&mut payload, "web_search_extract", t0.elapsed().as_millis());
                let md = web_search_extract_markdown(&payload);
                return Ok(tool_result_markdown_with_json(payload, md));
            }

            // Privacy guardrails (fail closed for anonymous mode).
            let privacy = privacy_mode_from_env();
            if privacy == PrivacyMode::Offline {
                // Offline privacy mode: no network egress (except localhost). This is stronger than
                // the tool-level `no_network` knob; privacy_mode overrides it.
                if fetch_backend == "firecrawl" || fetch_backend == "render" {
                    let mut payload = serde_json::json!({
                        "ok": false,
                        "error": error_obj(
                            ErrorCode::NotSupported,
                            format!("privacy_mode=offline disables fetch_backend=\"{fetch_backend}\""),
                            "Use fetch_backend=\"local\" with urls pointing to localhost, or set WEBPIPE_PRIVACY_MODE=normal/anonymous."
                        ),
                        "request": {
                            "privacy_mode": "offline",
                            "fetch_backend": fetch_backend,
                            "no_network": no_network
                        }
                    });
                    add_envelope_fields(
                        &mut payload,
                        "web_search_extract",
                        t0.elapsed().as_millis(),
                    );
                    let md = web_search_extract_markdown(&payload);
                    return Ok(tool_result_markdown_with_json(payload, md));
                }
                if let Some(urls) = args.urls.as_ref() {
                    let needs_outbound = urls.iter().any(|u| !is_localhost_url(u));
                    if needs_outbound {
                        // Cache-only runs are allowed even for non-localhost URLs.
                        if no_network {
                            // ok: cache-only
                        } else {
                            let mut payload = serde_json::json!({
                                "ok": false,
                                "error": error_obj(
                                    ErrorCode::NotSupported,
                                    "privacy_mode=offline blocks network fetches to non-localhost URLs",
                                    "Use localhost URLs, or warm the cache in normal mode and then use no_network=true with fetch_backend=\"local\"."
                                ),
                                "request": { "privacy_mode": "offline", "no_network": no_network }
                            });
                            add_envelope_fields(
                                &mut payload,
                                "web_search_extract",
                                t0.elapsed().as_millis(),
                            );
                            let md = web_search_extract_markdown(&payload);
                            return Ok(tool_result_markdown_with_json(payload, md));
                        }
                    }
                } else {
                    // urls omitted:
                    // - if no_network=true, we can still run cache-corpus mode (handled later).
                    // - otherwise query mode would require web_search (network-only), which is disabled.
                    if !no_network {
                        let mut payload = serde_json::json!({
                            "ok": false,
                            "error": error_obj(
                                ErrorCode::NotSupported,
                                "privacy_mode=offline disables web_search-based discovery",
                                "Provide urls=[...] pointing to localhost, or set WEBPIPE_PRIVACY_MODE=normal/anonymous."
                            ),
                            "request": { "privacy_mode": "offline", "no_network": no_network }
                        });
                        add_envelope_fields(
                            &mut payload,
                            "web_search_extract",
                            t0.elapsed().as_millis(),
                        );
                        let md = web_search_extract_markdown(&payload);
                        return Ok(tool_result_markdown_with_json(payload, md));
                    }
                }
            }
            if privacy == PrivacyMode::Anonymous {
                if fetch_backend == "firecrawl" {
                    let mut payload = serde_json::json!({
                        "ok": false,
                        "error": error_obj(
                            ErrorCode::NotSupported,
                            "anonymous mode disables fetch_backend=\"firecrawl\"",
                            "Use fetch_backend=\"local\" (routed via WEBPIPE_ANON_PROXY) or set WEBPIPE_PRIVACY_MODE=normal."
                        ),
                        "request": {
                            "privacy_mode": "anonymous",
                            "fetch_backend": fetch_backend,
                            "no_network": no_network
                        }
                    });
                    add_envelope_fields(
                        &mut payload,
                        "web_search_extract",
                        t0.elapsed().as_millis(),
                    );
                    let md = web_search_extract_markdown(&payload);
                    return Ok(tool_result_markdown_with_json(payload, md));
                }
                if !no_network && anon_proxy_from_env().is_none() {
                    // If urls are provided, we will fetch them; require a proxy for any non-localhost URL.
                    if let Some(urls) = args.urls.as_ref() {
                        let needs_outbound = urls.iter().any(|u| !is_localhost_url(u));
                        if needs_outbound {
                            let mut payload = serde_json::json!({
                                "ok": false,
                                "error": error_obj(
                                    ErrorCode::NotConfigured,
                                    "anonymous mode requires a proxy",
                                    if fetch_backend == "render" {
                                        "Set WEBPIPE_ANON_PROXY to an HTTP proxy (Tor users often run Privoxy), or set WEBPIPE_PRIVACY_MODE=normal."
                                    } else {
                                        "Set WEBPIPE_ANON_PROXY (recommended for Tor: socks5h://127.0.0.1:9050) or set WEBPIPE_PRIVACY_MODE=normal."
                                    }
                                ),
                                "request": {
                                    "privacy_mode": "anonymous",
                                    "anon_proxy_configured": false,
                                    "no_network": no_network
                                }
                            });
                            add_envelope_fields(
                                &mut payload,
                                "web_search_extract",
                                t0.elapsed().as_millis(),
                            );
                            let md = web_search_extract_markdown(&payload);
                            return Ok(tool_result_markdown_with_json(payload, md));
                        }
                    } else {
                        // No urls => would require web_search; disallow in anonymous mode.
                        let mut payload = serde_json::json!({
                            "ok": false,
                            "error": error_obj(
                                ErrorCode::NotSupported,
                                "anonymous mode disables web_search-based discovery",
                                "Provide urls=[...] (fetched via WEBPIPE_ANON_PROXY) or use cached/offline workflows, or set WEBPIPE_PRIVACY_MODE=normal."
                            ),
                            "request": {
                                "privacy_mode": "anonymous",
                                "no_network": no_network
                            }
                        });
                        add_envelope_fields(
                            &mut payload,
                            "web_search_extract",
                            t0.elapsed().as_millis(),
                        );
                        let md = web_search_extract_markdown(&payload);
                        return Ok(tool_result_markdown_with_json(payload, md));
                    }
                }
            }
            // Prefer auto_plus by default when we have a query: bounded hint prepass to avoid portals.
            let url_selection_mode = args
                .url_selection_mode
                .unwrap_or_else(|| "auto_plus".to_string());
            let firecrawl_fallback_on_empty_extraction = if no_network {
                false
            } else {
                let configured =
                    has_env("WEBPIPE_FIRECRAWL_API_KEY") || has_env("FIRECRAWL_API_KEY");
                args.firecrawl_fallback_on_empty_extraction
                    .unwrap_or(configured) // Default to true if configured
            };
            let firecrawl_fallback_on_low_signal = if no_network {
                false
            } else {
                let configured =
                    has_env("WEBPIPE_FIRECRAWL_API_KEY") || has_env("FIRECRAWL_API_KEY");
                args.firecrawl_fallback_on_low_signal.unwrap_or(configured) // Default to true if configured, regardless of backend
            };
            // Render fallback is always opt-in (never default-on); it can be expensive.
            let render_fallback_on_empty_extraction = if no_network {
                false
            } else {
                args.render_fallback_on_empty_extraction.unwrap_or(false)
                    && fetch_backend == "local"
            };
            // Default render_fallback_on_low_signal to true for better JS-heavy page handling.
            let render_fallback_on_low_signal = if no_network {
                false
            } else {
                args.render_fallback_on_low_signal.unwrap_or(true) && fetch_backend == "local"
            };

            // Width vs depth preset: only fills in defaults when those fields were not explicitly set.
            let exploration = args
                .exploration
                .clone()
                .unwrap_or_else(|| "balanced".to_string());
            let mut preset_max_search_rounds: Option<usize> = None;
            match exploration.as_str() {
                "balanced" => {}
                "wide" => {
                    if !max_results_user {
                        max_results = 10;
                    }
                    if !max_urls_user {
                        max_urls = 5;
                    }
                    if !max_chars_user {
                        max_chars = 10_000;
                    }
                    if !top_chunks_user {
                        top_chunks = 3;
                    }
                    if !max_chunk_chars_user {
                        max_chunk_chars = 400;
                    }
                }
                "deep" => {
                    if !max_results_user {
                        max_results = 5;
                    }
                    if !max_urls_user {
                        max_urls = 8; // Bumped from 3 to allow deeper exploration
                    }
                    if !max_chars_user {
                        max_chars = 60_000;
                    }
                    if !top_chunks_user {
                        top_chunks = 10;
                    }
                    if !max_chunk_chars_user {
                        max_chunk_chars = 800;
                    }
                    // Deep mode implies agentic discovery unless explicitly disabled.
                    if args.agentic.is_none() {
                        agentic = true;
                    }
                    if args.agentic_max_search_rounds.is_none() {
                        preset_max_search_rounds = Some(2);
                    }
                }
                "smart" => {
                    // "Smart" preset: balanced but agentic.
                    if args.agentic.is_none() {
                        agentic = true;
                    }
                    // Ensure we have enough budget to actually expand the frontier.
                    if !max_urls_user && max_urls < 5 {
                        max_urls = 5;
                    }
                }
                _ => {
                    let mut payload = serde_json::json!({
                        "ok": false,
                        "provider": requested_provider,
                        "exploration": exploration,
                        "query": args.query.clone().unwrap_or_default(),
                        "error": error_obj(
                            ErrorCode::InvalidParams,
                            "unknown exploration preset",
                            "Allowed exploration values: balanced, wide, deep, smart"
                        ),
                    });
                    add_envelope_fields(
                        &mut payload,
                        "web_search_extract",
                        t0.elapsed().as_millis(),
                    );
                    let md = web_search_extract_markdown(&payload);
                    return Ok(tool_result_markdown_with_json(payload, md));
                }
            }
            if url_selection_mode.as_str() != "auto"
                && url_selection_mode.as_str() != "auto_plus"
                && url_selection_mode.as_str() != "preserve"
                && url_selection_mode.as_str() != "query_rank"
            {
                let mut payload = serde_json::json!({
                    "ok": false,
                    "provider": requested_provider,
                    "auto_mode": requested_auto_mode,
                    "url_selection_mode": url_selection_mode,
                    "fetch_backend": fetch_backend,
                    "query": args.query.clone().unwrap_or_default(),
                    "error": error_obj(
                        ErrorCode::InvalidParams,
                        "unknown url_selection_mode",
                        "Allowed url_selection_mode values: auto, auto_plus, preserve, query_rank"
                    ),
                });
                add_envelope_fields(&mut payload, "web_search_extract", t0.elapsed().as_millis());
                let md = web_search_extract_markdown(&payload);
                return Ok(tool_result_markdown_with_json(payload, md));
            }
            if selection_mode.as_str() != "score" && selection_mode.as_str() != "pareto" {
                let mut payload = serde_json::json!({
                    "ok": false,
                    "provider": requested_provider,
                    "auto_mode": requested_auto_mode,
                    "selection_mode": selection_mode,
                    "fetch_backend": fetch_backend,
                    "query": args.query.clone().unwrap_or_default(),
                    "error": error_obj(
                        ErrorCode::InvalidParams,
                        "unknown selection_mode",
                        "Allowed selection_mode values: score, pareto"
                    ),
                });
                add_envelope_fields(&mut payload, "web_search_extract", t0.elapsed().as_millis());
                let md = web_search_extract_markdown(&payload);
                return Ok(tool_result_markdown_with_json(payload, md));
            }
            if fetch_backend.as_str() != "local"
                && fetch_backend.as_str() != "firecrawl"
                && fetch_backend.as_str() != "render"
            {
                let mut payload = serde_json::json!({
                    "ok": false,
                    "provider": requested_provider,
                    "fetch_backend": fetch_backend,
                    "query": args.query.clone().unwrap_or_default(),
                    "error": error_obj(
                        ErrorCode::InvalidParams,
                        "unknown fetch_backend",
                        "Allowed fetch_backends: local, firecrawl, render"
                    ),
                });
                add_envelope_fields(&mut payload, "web_search_extract", t0.elapsed().as_millis());
                let md = web_search_extract_markdown(&payload);
                return Ok(tool_result_markdown_with_json(payload, md));
            }
            if no_network && fetch_backend == "render" {
                let mut payload = serde_json::json!({
                    "ok": false,
                    "provider": requested_provider,
                    "fetch_backend": fetch_backend,
                    "no_network": true,
                    "query": args.query.clone().unwrap_or_default(),
                    "error": error_obj(
                        ErrorCode::NotSupported,
                        "no_network=true cannot be used with fetch_backend=\"render\"",
                        "Render mode uses a headless browser and performs network requests. Use fetch_backend=\"local\" with a warmed WEBPIPE_CACHE_DIR, or set no_network=false."
                    )
                });
                add_envelope_fields(&mut payload, "web_search_extract", t0.elapsed().as_millis());
                let md = web_search_extract_markdown(&payload);
                return Ok(tool_result_markdown_with_json(payload, md));
            }
            if agentic_selector.as_str() != "auto"
                && agentic_selector.as_str() != "lexical"
                && agentic_selector.as_str() != "llm"
            {
                let mut payload = serde_json::json!({
                    "ok": false,
                    "provider": requested_provider,
                    "fetch_backend": fetch_backend,
                    "query": args.query.clone().unwrap_or_default(),
                    "error": error_obj(
                        ErrorCode::InvalidParams,
                        "unknown agentic_selector",
                        "Allowed agentic_selector values: auto, lexical, llm"
                    ),
                });
                add_envelope_fields(&mut payload, "web_search_extract", t0.elapsed().as_millis());
                let md = web_search_extract_markdown(&payload);
                return Ok(tool_result_markdown_with_json(payload, md));
            }
            // `fetch_backend` selects the primary backend. Separately, we may use a bounded Firecrawl
            // fallback in local mode when extraction yields empty or low-signal output (opt-in flags).
            let firecrawl_primary = if fetch_backend == "firecrawl" {
                match webpipe_local::firecrawl::FirecrawlClient::from_env(self.http.clone()) {
                    Ok(c) => Some(c),
                    Err(e) => {
                        let mut payload = serde_json::json!({
                            "ok": false,
                            "provider": requested_provider,
                            "fetch_backend": fetch_backend,
                            "query": args.query.clone().unwrap_or_default(),
                            "error": error_obj(
                                ErrorCode::NotConfigured,
                                e.to_string(),
                                "Set WEBPIPE_FIRECRAWL_API_KEY (or FIRECRAWL_API_KEY) to use fetch_backend=\"firecrawl\"."
                            ),
                        });
                        add_envelope_fields(
                            &mut payload,
                            "web_search_extract",
                            t0.elapsed().as_millis(),
                        );
                        return Ok(tool_result(payload));
                    }
                }
            } else {
                None
            };
            let firecrawl_fallback = if !no_network
                && fetch_backend == "local"
                && (firecrawl_fallback_on_empty_extraction || firecrawl_fallback_on_low_signal)
            {
                webpipe_local::firecrawl::FirecrawlClient::from_env(self.http.clone()).ok()
            } else {
                None
            };

            let mut search_steps: Vec<serde_json::Value> = Vec::new();
            let mut search_rounds: usize = 0;
            let mut search_backend_provider: Option<String> = None;

            let (query, urls): (String, Vec<String>) = if let Some(urls) = args.urls {
                let q = args.query.unwrap_or_default();
                (q, urls)
            } else {
                if no_network {
                    let q = args.query.unwrap_or_default().trim().to_string();
                    if q.is_empty() {
                        let mut payload = serde_json::json!({
                            "ok": false,
                            "mode": "cache",
                            "provider": "cache",
                            "query": "",
                            "error": error_obj(
                                ErrorCode::InvalidParams,
                                "no_network=true requires either urls=[...] or a non-empty query",
                                "Provide urls=[...] for cache-only hydration, or provide query=\"...\" to search the cache corpus."
                            ),
                            "request": { "no_network": true }
                        });
                        add_envelope_fields(
                            &mut payload,
                            "web_search_extract",
                            t0.elapsed().as_millis(),
                        );
                        let md = web_search_extract_markdown(&payload);
                        return Ok(tool_result_markdown_with_json(payload, md));
                    }

                    // Cache-corpus mode: prefer an explicit cache dir, but fall back to a
                    // persistent default so "warm cache + no_network" works out-of-the-box.
                    let cache_dir = cache_dir_from_env().unwrap_or_else(default_cache_dir);

                    // Cache-corpus search: bounded, deterministic, offline.
                    let cache_dir2 = cache_dir.clone();
                    let q2 = q.clone();
                    // Cache-corpus mode is for "offline inspiration" and should stay fast.
                    // Keep these conservative so the default deadline_ms is usually respected.
                    let max_docs = 30usize;
                    let max_scan_entries = 800usize;
                    let max_chars2 = max_chars;
                    let max_bytes2 = max_bytes;
                    let width2 = width;
                    let top_chunks2 = top_chunks;
                    let max_chunk_chars2 = max_chunk_chars;
                    // Cache-corpus mode should stay fast by default. Structure can be useful
                    // for debugging, but it's expensive when scanning many cached docs.
                    let include_structure2 = if include_structure_user {
                        include_structure
                    } else {
                        false
                    };
                    let max_outline_items2 = max_outline_items;
                    let max_blocks2 = max_blocks;
                    let max_block_chars2 = max_block_chars;

                    let now = std::time::Instant::now();
                    let remaining = deadline.saturating_duration_since(now);
                    let handle = tokio::task::spawn_blocking(move || {
                        webpipe_local::cache_search::cache_search_extract(
                            &cache_dir2,
                            &q2,
                            max_docs,
                            max_chars2,
                            max_bytes2,
                            width2,
                            top_chunks2,
                            max_chunk_chars2,
                            include_structure2,
                            max_outline_items2,
                            max_blocks2,
                            max_block_chars2,
                            include_text,
                            max_scan_entries,
                        )
                    });
                    let r = match tokio::time::timeout(remaining, handle).await {
                        Ok(join) => join.unwrap_or_else(|_| {
                            webpipe_local::cache_search::CacheSearchResult {
                                ok: false,
                                scanned_entries: 0,
                                selected_docs: 0,
                                results: vec![],
                                warnings: vec!["cache_search_task_failed"],
                            }
                        }),
                        Err(_) => {
                            let mut payload = serde_json::json!({
                                "ok": false,
                                "mode": "cache",
                                "provider": "cache",
                                "query": q,
                                "warning_codes": ["deadline_exceeded_partial"],
                                "warning_hints": warning_hints_from(&["deadline_exceeded_partial"]),
                                "error": error_obj(
                                    ErrorCode::CacheError,
                                    "cache corpus search timed out",
                                    format!("Increase deadline_ms (current value: {deadline_ms}ms) or reduce cache size; or pass urls=[...] to avoid scanning the whole cache corpus.")
                                ),
                                "request": { "no_network": true, "cache_dir": cache_dir.to_string_lossy().to_string(), "deadline_ms": deadline_ms }
                            });
                            add_envelope_fields(
                                &mut payload,
                                "web_search_extract",
                                t0.elapsed().as_millis(),
                            );
                            let md = web_search_extract_markdown(&payload);
                            return Ok(tool_result_markdown_with_json(payload, md));
                        }
                    };

                    if !r.ok {
                        let mut payload = serde_json::json!({
                            "ok": false,
                            "mode": "cache",
                            "provider": "cache",
                            "query": q,
                            "error": error_obj(
                                ErrorCode::CacheError,
                                "cache corpus search failed",
                                format!("Ensure WEBPIPE_CACHE_DIR points to a valid webpipe cache dir (or warm the default cache dir at {}).", cache_dir.display())
                            ),
                            "request": { "no_network": true, "cache_dir": cache_dir.to_string_lossy().to_string() }
                        });
                        add_envelope_fields(
                            &mut payload,
                            "web_search_extract",
                            t0.elapsed().as_millis(),
                        );
                        let md = web_search_extract_markdown(&payload);
                        return Ok(tool_result_markdown_with_json(payload, md));
                    }

                    let mut per_url: Vec<serde_json::Value> = Vec::new();
                    let mut all_chunks: Vec<ChunkCandidate> = Vec::new();

                    for hit in r.results {
                        let warns = hit.warnings.clone();
                        let codes = warning_codes_from(&warns);
                        let warn_pen = Self::warning_penalty(&warns);

                        // Add chunks into the global pool.
                        for c in &hit.chunks {
                            all_chunks.push(ChunkCandidate {
                                url: hit.final_url.clone(),
                                score: c.score,
                                start_char: c.start_char,
                                end_char: c.end_char,
                                text: c.text.clone(),
                                warning_penalty: warn_pen,
                                cache_hit: true,
                            });
                        }

                        per_url.push(serde_json::json!({
                            "ok": true,
                            "url": hit.url,
                            "final_url": hit.final_url,
                            "status": hit.status,
                            "content_type": hit.content_type,
                            "bytes": hit.bytes,
                            "fetch_backend": "cache",
                            "attempts": serde_json::Value::Null,
                            "extract": {
                                "engine": hit.extraction_engine,
                                "text_chars": hit.text_chars,
                                "text_truncated": hit.text_truncated,
                                "quality": serde_json::Value::Null
                            },
                            "warnings": warns,
                            "warning_codes": codes.clone(),
                            "warning_hints": warning_hints_from(&codes),
                            "elapsed_ms": 0
                        }));
                    }

                    let selected =
                        Self::select_top_chunks(all_chunks, top_chunks, selection_mode.as_str());
                    let top_chunks_out: Vec<serde_json::Value> = selected
                        .into_iter()
                        .map(|c| {
                            serde_json::json!({
                                "url": c.url,
                                "score": c.score,
                                "start_char": c.start_char,
                                "end_char": c.end_char,
                                "text": c.text
                            })
                        })
                        .collect();

                    let mut payload = serde_json::json!({
                        "ok": true,
                        "mode": "cache",
                        "provider": "cache",
                        "backend_provider": serde_json::Value::Null,
                        "query": q,
                        "query_key": Self::query_key(&q),
                        "request": {
                            "mode": "cache",
                            "no_network": true,
                            "fetch_backend": "local",
                            "selection_mode": selection_mode,
                            "max_chars": max_chars,
                            "top_chunks": top_chunks,
                            "max_chunk_chars": max_chunk_chars,
                            "include_text": include_text,
                            "include_links": include_links,
                            "include_structure": include_structure,
                            "max_outline_items": max_outline_items2,
                            "max_blocks": max_blocks2,
                            "max_block_chars": max_block_chars2,
                            "cache": { "read": true, "write": false, "ttl_s": cache_ttl_s },
                            "compact": compact
                        },
                        "url_count_in": 0,
                        "url_count_used": per_url.len(),
                        "results": per_url,
                        "top_chunks": top_chunks_out
                    });
                    add_envelope_fields(
                        &mut payload,
                        "web_search_extract",
                        t0.elapsed().as_millis(),
                    );
                    if minimal_output {
                        strip_minimal_output(&mut payload);
                    }
                    let md = web_search_extract_markdown(&payload);
                    return Ok(tool_result_markdown_with_json(payload, md));
                }
                let q = match args.query {
                    Some(q) if !q.trim().is_empty() => q,
                    _ => {
                        let mut payload = serde_json::json!({
                            "ok": false,
                            "provider": requested_provider,
                            "query": args.query.unwrap_or_default(),
                            "error": error_obj(
                                ErrorCode::InvalidParams,
                                "either `urls` must be provided, or `query` must be a non-empty string",
                                "Pass urls=[...] for offline mode, or query=\"...\" to run a search first."
                            ),
                            "request": {
                                "provider": requested_provider,
                                "max_results": max_results,
                                "max_urls": max_urls
                            }
                        });
                        add_envelope_fields(
                            &mut payload,
                            "web_search_extract",
                            t0.elapsed().as_millis(),
                        );
                        return Ok(tool_result(payload));
                    }
                };

                // Use our existing web_search tool logic to keep routing/fallback consistent.
                let sr = self
                    .web_search(p(WebSearchArgs {
                        query: Some(q.clone()),
                        provider: Some(requested_provider.clone()),
                        auto_mode: Some(requested_auto_mode.clone()),
                        max_results: Some(max_results),
                        language: None,
                        country: None,
                        timeout_ms: Some(timeout_ms),
                    }))
                    .await?;
                let sv = payload_from_result(&sr);
                if sv.get("ok").and_then(|v| v.as_bool()) != Some(true) {
                    // Bubble the search failure, but keep this tool's kind.
                    let mut payload = serde_json::json!({
                        "ok": false,
                        "provider": requested_provider,
                        "query": q,
                        "search": sv,
                        "error": error_obj(
                            ErrorCode::SearchFailed,
                            "search failed",
                            "No search provider is configured or all providers failed. Either: (1) set a key — WEBPIPE_BRAVE_API_KEY, WEBPIPE_TAVILY_API_KEY, or WEBPIPE_SEARXNG_ENDPOINT — or (2) skip search entirely by passing urls=[...] directly."
                        )
                    });
                    add_envelope_fields(
                        &mut payload,
                        "web_search_extract",
                        t0.elapsed().as_millis(),
                    );
                    let md = web_search_extract_markdown(&payload);
                    return Ok(tool_result_markdown_with_json(payload, md));
                }

                search_backend_provider = sv
                    .get("backend_provider")
                    .and_then(|v| v.as_str())
                    .map(|s| s.to_string())
                    .or_else(|| {
                        sv.get("provider")
                            .and_then(|v| v.as_str())
                            .map(|s| s.to_string())
                    });

                let mut out_all = Vec::new();
                if let Some(rs) = sv.get("results").and_then(|v| v.as_array()) {
                    for r in rs {
                        if let Some(u) = r.get("url").and_then(|v| v.as_str()) {
                            out_all.push(u.to_string());
                            if out_all.len() >= max_results {
                                break;
                            }
                        }
                    }
                }
                // Search is only for *seed URLs*; the actual exploration happens via bounded
                // fetch/extract/link expansion. Keep the seed set small and high-signal.
                let cap = max_urls;
                let mut out: Vec<String> = Vec::new();
                let mut dropped_auth = 0usize;
                let mut dropped_promo = 0usize;
                let mut dropped_home = 0usize;
                let mut dropped_domain = 0usize;
                // Prefer non-auth-wall URLs when we have enough candidates.
                for u in &out_all {
                    if out.len() >= cap {
                        break;
                    }
                    if !url_allowed_by_domain_filters(u, &domains_allow, &domains_deny) {
                        dropped_domain += 1;
                        continue;
                    }
                    if !url_looks_like_auth_or_challenge(u)
                        && !url_looks_like_promo_or_tracking(u)
                        && !url_looks_like_low_value_homepage(u)
                    {
                        out.push(u.clone());
                    } else if url_looks_like_auth_or_challenge(u) {
                        dropped_auth += 1;
                    } else if url_looks_like_promo_or_tracking(u) {
                        dropped_promo += 1;
                    } else if url_looks_like_low_value_homepage(u) {
                        dropped_home += 1;
                    }
                }
                // If we still need URLs, allow promo/tracking/homepage URLs (but still avoid auth walls if possible).
                if out.len() < cap {
                    for u in &out_all {
                        if out.len() >= cap {
                            break;
                        }
                        if out.iter().any(|x| x == u) {
                            continue;
                        }
                        if !url_allowed_by_domain_filters(u, &domains_allow, &domains_deny) {
                            dropped_domain += 1;
                            continue;
                        }
                        if url_looks_like_auth_or_challenge(u) {
                            continue;
                        }
                        // Prefer returning fewer seeds over adding obvious soft-junk.
                        // Agentic exploration can expand from good seeds; promo/homepage URLs tend to waste budget.
                        if !out.is_empty()
                            && (url_looks_like_promo_or_tracking(u)
                                || url_looks_like_low_value_homepage(u))
                        {
                            continue;
                        }
                        // Avoid adding a low-value homepage when we already have a non-homepage URL
                        // from the same host.
                        if url_looks_like_low_value_homepage(u) {
                            let Ok(pu) = reqwest::Url::parse(u.trim()) else {
                                continue;
                            };
                            let Some(h) = pu.host_str().map(|s| s.to_ascii_lowercase()) else {
                                continue;
                            };
                            let mut already_have_same_host = false;
                            for e in &out {
                                if let Ok(pe) = reqwest::Url::parse(e.trim()) {
                                    if pe.host_str().map(|s| s.to_ascii_lowercase())
                                        == Some(h.clone())
                                        && !url_looks_like_low_value_homepage(e)
                                    {
                                        already_have_same_host = true;
                                        break;
                                    }
                                }
                            }
                            if already_have_same_host {
                                continue;
                            }
                        }
                        out.push(u.clone());
                    }
                }
                // If we still need URLs, allow auth-wall URLs as a fallback (better than returning nothing).
                if out.len() < cap {
                    for u in &out_all {
                        if out.len() >= cap {
                            break;
                        }
                        if out.iter().any(|x| x == u) {
                            continue;
                        }
                        if !url_allowed_by_domain_filters(u, &domains_allow, &domains_deny) {
                            dropped_domain += 1;
                            continue;
                        }
                        out.push(u.clone());
                    }
                }

                // Keep a compact, stable summary of the search step for agent provenance.
                // (Do not duplicate results; we already emit per-URL fetch/extract outputs.)
                // NOTE: `provider` is the requested provider; `backend_provider` may be "merge" or a concrete provider.
                search_steps.push(serde_json::json!({
                    "ok": true,
                    "requested_provider": requested_provider,
                    "auto_mode": requested_auto_mode,
                    "backend_provider": sv.get("backend_provider").cloned().unwrap_or(serde_json::Value::Null),
                    "result_count": sv.get("results").and_then(|x| x.as_array()).map(|a| a.len()).unwrap_or(0),
                    "warnings": sv.get("warnings").cloned().unwrap_or(serde_json::Value::Null),
                    "providers": sv.get("providers").cloned().unwrap_or(serde_json::Value::Null),
                    "selection": sv.get("selection").cloned().unwrap_or(serde_json::Value::Null),
                    "auth_wall_urls_skipped": dropped_auth,
                    "promo_or_tracking_urls_skipped": dropped_promo,
                    "homepage_urls_skipped": dropped_home,
                    "domain_urls_skipped": dropped_domain,
                }));
                search_rounds = 1;

                (q, out)
            };

            // Dedup URLs while preserving the original order. This keeps `max_urls`
            // semantics predictable and avoids doing redundant work.
            let urls = {
                let mut seen = std::collections::HashSet::<String>::new();
                let mut out = Vec::new();
                for u in urls {
                    let us = u.trim();
                    if us.is_empty() {
                        continue;
                    }
                    let s = us.to_string();
                    if !url_allowed_by_domain_filters(&s, &domains_allow, &domains_deny) {
                        continue;
                    }
                    if seen.insert(s.clone()) {
                        out.push(s);
                    }
                }
                out
            };
            if urls.is_empty() {
                let mut payload = serde_json::json!({
                    "ok": false,
                    "provider": requested_provider,
                    "query": query,
                    "error": error_obj(
                        ErrorCode::InvalidParams,
                        "all candidate URLs were filtered out by domains_allow/domains_deny",
                        "Relax domains_allow/domains_deny, or provide urls=[...] that match the allowed domains."
                    ),
                    "request": {
                        "domains_allow": domains_allow.clone(),
                        "domains_deny": domains_deny.clone()
                    }
                });
                add_envelope_fields(&mut payload, "web_search_extract", t0.elapsed().as_millis());
                let md = web_search_extract_markdown(&payload);
                return Ok(tool_result_markdown_with_json(payload, md));
            }

            // URL selection under max_urls. For the "auto_plus" mode, we do a bounded pre-pass:
            // fetch small bytes (cache-first) and rank by title/h1 hints + URL tokens.
            let selected_urls: Vec<String> = if url_selection_mode.as_str() == "auto_plus" {
                // If we don't have a query, auto_plus degenerates to preserve.
                let qkey = Self::query_key(&query).unwrap_or_default();
                if qkey.is_empty() {
                    urls.iter().take(max_urls).cloned().collect()
                } else {
                    let mut q_toks: Vec<String> = qkey
                        .split(|ch: char| !ch.is_alphanumeric())
                        .filter_map(|t| {
                            let t = t.trim();
                            (t.len() >= 2).then_some(t.to_string())
                        })
                        .collect();
                    // De-noise: drop generic stopwords and a few domain-generic tokens that
                    // otherwise swamp hint/title matching.
                    {
                        let stop: std::collections::HashSet<&'static str> =
                            textprep::stopwords::ENGLISH.iter().copied().collect();
                        let domain_stop: std::collections::HashSet<&'static str> = [
                            "api",
                            "docs",
                            "doc",
                            "documentation",
                            "reference",
                            "guide",
                            "page",
                            "latest",
                        ]
                        .into_iter()
                        .collect();
                        q_toks.retain(|t| {
                            !stop.contains(t.as_str()) && !domain_stop.contains(t.as_str())
                        });
                        q_toks.sort();
                        q_toks.dedup();
                    }

                    // Tiny, explicit query expansion for common MCP stdio failure terms.
                    // This is intentionally small + deterministic (no LLM), and helps map
                    // “TransportClosed/ConnectionClosed” error queries to the transports spec.
                    let q_join = q_toks.join(" ");
                    if q_join.contains("transportclosed") || q_join.contains("connectionclosed") {
                        q_toks.extend(
                            ["transport", "transports", "stdio", "spec", "specification"]
                                .into_iter()
                                .map(|s| s.to_string()),
                        );
                        q_toks.sort();
                        q_toks.dedup();
                    }

                    if q_toks.is_empty() {
                        urls.iter().take(max_urls).cloned().collect()
                    } else {
                        // Token rarity weights (cheap DF over URL strings).
                        let token_weights: std::collections::HashMap<String, u64> = {
                            let mut df: std::collections::HashMap<String, u64> =
                                q_toks.iter().map(|t| (t.clone(), 0u64)).collect();
                            for u in &urls {
                                let u_scrub = textprep::scrub(u);
                                let toks: std::collections::BTreeSet<&str> =
                                    u_scrub.split_whitespace().collect();
                                for t in &q_toks {
                                    if toks.contains(t.as_str()) {
                                        if let Some(v) = df.get_mut(t) {
                                            *v += 1;
                                        }
                                    }
                                }
                            }
                            // Weight is inverse document frequency-ish. Keep integer arithmetic.
                            // Higher weight for rarer tokens so “firecrawl” beats generic words like “field”.
                            df.into_iter()
                                .map(|(t, c)| {
                                    let w = 1_000u64 / (1 + c);
                                    (t, w.max(1))
                                })
                                .collect()
                        };

                        // First pass over ALL URLs using URL-string token match (cheap, no IO).
                        let mut by_url_score: Vec<(usize, u64, String)> = urls
                            .iter()
                            .enumerate()
                            .map(|(i, u)| {
                                let u_scrub = textprep::scrub(u);
                                let toks: std::collections::BTreeSet<&str> =
                                    u_scrub.split_whitespace().collect();
                                let mut s = 0u64;
                                for t in &q_toks {
                                    if toks.contains(t.as_str()) {
                                        s = s.saturating_add(*token_weights.get(t).unwrap_or(&1));
                                    }
                                }
                                (i, s, u.clone())
                            })
                            .collect();
                        by_url_score.sort_by(|a, b| b.1.cmp(&a.1).then_with(|| a.0.cmp(&b.0)));

                        // Bound prefetch work: refine only the top-N by URL score, capped.
                        // This keeps IO predictable while still considering URLs anywhere in the list.
                        let probe_n = std::cmp::min(
                            urls.len(),
                            std::cmp::min(max_urls.saturating_mul(12), 60),
                        );
                        let probe: Vec<(usize, u64, String)> =
                            by_url_score.iter().take(probe_n).cloned().collect();

                        let mut scored: Vec<(usize, u64, u64, bool, String)> = Vec::new();
                        for (i, url_score, u) in probe {
                            // Cache-first, bounded fetch of small bytes for hint extraction.
                            let (hint_score, cache_hit) = match self
                                .fetcher
                                .fetch(&webpipe_core::FetchRequest {
                                    url: u.clone(),
                                    timeout_ms: Some(timeout_ms.min(5_000)),
                                    max_bytes: Some(max_bytes.min(200_000)),
                                    headers: BTreeMap::new(),
                                    cache: webpipe_core::FetchCachePolicy {
                                        read: true,
                                        write: cache_write,
                                        ttl_s: cache_ttl_s,
                                    },
                                })
                                .await
                            {
                                Ok(fr) => {
                                    let cache_hit = fr.source == webpipe_core::FetchSource::Cache;
                                    let html = fr.text_lossy();
                                    let hint = webpipe_local::extract::html_hint_text(&html, 400);
                                    let hint_scrub = textprep::scrub(&hint);
                                    let hint_toks: std::collections::BTreeSet<&str> =
                                        hint_scrub.split_whitespace().collect();
                                    let mut s = 0u64;
                                    for t in &q_toks {
                                        if hint_toks.contains(t.as_str()) {
                                            s = s.saturating_add(
                                                *token_weights.get(t).unwrap_or(&1),
                                            );
                                        }
                                    }
                                    (s, cache_hit)
                                }
                                Err(_) => (0u64, false),
                            };

                            let mut hint_score2 = hint_score;
                            let mut url_score2 = url_score;
                            if url_looks_like_auth_or_challenge(&u) {
                                hint_score2 /= 4;
                                url_score2 /= 4;
                            }
                            // If we’re doing local fetching, downrank hosts that often trip JS challenges.
                            if fetch_backend == "local"
                                && url_looks_like_js_challenge_prone_host(&u)
                            {
                                hint_score2 /= 4;
                                url_score2 /= 4;
                            }
                            scored.push((i, hint_score2, url_score2, cache_hit, u));
                        }

                        // Sort: hint_score, then url_score, then cache_hit, then original order.
                        scored.sort_by(|a, b| {
                            b.1.cmp(&a.1)
                                .then_with(|| b.2.cmp(&a.2))
                                .then_with(|| (b.3 as u8).cmp(&(a.3 as u8)))
                                .then_with(|| a.0.cmp(&b.0))
                        });
                        // Prefer URLs with *any* signal. If we have enough, drop zero-signal tail URLs
                        // rather than wasting fetch budget on clearly irrelevant pages (common with query
                        // token collisions like "loss"→"mloss").
                        let min_urls = max_urls.min(3);
                        let mut picked: Vec<String> = Vec::new();
                        for (_i, hint_s, url_s, _cache_hit, u) in &scored {
                            if *hint_s > 0 || *url_s > 0 {
                                picked.push(u.clone());
                                if picked.len() >= max_urls {
                                    break;
                                }
                            }
                        }
                        if picked.len() < min_urls {
                            picked = scored
                                .into_iter()
                                .take(max_urls)
                                .map(|(_, _, _, _, u)| u)
                                .collect();
                        }

                        // Only backfill up to a small minimum. If we already have enough signal-bearing
                        // URLs, prefer returning fewer URLs over including obvious junk.
                        if picked.len() < min_urls {
                            let mut seen = std::collections::HashSet::<String>::new();
                            for u in &picked {
                                seen.insert(u.clone());
                            }
                            for (_i, _s, u) in by_url_score {
                                if picked.len() >= min_urls {
                                    break;
                                }
                                if seen.insert(u.clone()) {
                                    picked.push(u);
                                }
                            }
                        }

                        picked
                    }
                }
            } else {
                Self::select_urls_for_hydration(
                    urls.clone(),
                    max_urls,
                    &query,
                    url_selection_mode.as_str(),
                )
            };

            let mut per_url = Vec::new();
            let mut all_chunks: Vec<ChunkCandidate> = Vec::new();
            let mut total_urls_ok: usize = 0;
            let mut hard_junk_urls: usize = 0;
            let mut soft_junk_urls: usize = 0;

            let query_key = Self::query_key(&query);

            let mut agentic_trace: Vec<serde_json::Value> = Vec::new();
            fn canonicalize_url_no_frag(url: &str) -> Option<String> {
                let s = url.trim();
                if s.is_empty() {
                    return None;
                }
                if let Ok(mut u) = reqwest::Url::parse(s) {
                    u.set_fragment(None);
                    Some(u.to_string())
                } else {
                    None
                }
            }

            // Agentic loop: frontier can expand via link discovery and can replace weak initial picks.
            // Bounded by `max_urls` total fetches.
            let mut frontier: Vec<String> = if agentic {
                urls.clone()
            } else {
                selected_urls.clone()
            };
            let mut seen_frontier = std::collections::HashSet::<String>::new();
            frontier.retain(|u| {
                if !url_allowed_by_domain_filters(u, &domains_allow, &domains_deny) {
                    return false;
                }
                canonicalize_url_no_frag(u)
                    .map(|k| seen_frontier.insert(k))
                    .unwrap_or(false)
            });

            // Query tokens used for deterministic frontier ranking (no special-cased domains).
            let qkey = Self::query_key(&query).unwrap_or_default();
            let mut q_toks: Vec<String> = qkey
                .split(|ch: char| !ch.is_alphanumeric())
                .filter_map(|t| {
                    let t = t.trim();
                    (t.len() >= 2).then_some(t.to_string())
                })
                .collect();
            {
                let stop: std::collections::HashSet<&'static str> =
                    textprep::stopwords::ENGLISH.iter().copied().collect();
                let domain_stop: std::collections::HashSet<&'static str> = [
                    "api",
                    "docs",
                    "doc",
                    "documentation",
                    "reference",
                    "guide",
                    "page",
                    "latest",
                ]
                .into_iter()
                .collect();
                q_toks.retain(|t| !stop.contains(t.as_str()) && !domain_stop.contains(t.as_str()));
                q_toks.sort();
                q_toks.dedup();
            }
            let q_join = q_toks.join(" ");
            if q_join.contains("transportclosed") || q_join.contains("connectionclosed") {
                q_toks.extend(
                    ["transport", "transports", "stdio", "spec", "specification"]
                        .into_iter()
                        .map(|s| s.to_string()),
                );
                q_toks.sort();
                q_toks.dedup();
            }

            // Per-URL “prior” relevance propagated from fetched pages to their discovered links.
            // Keyed by canonical URL (fragmentless).
            let mut priors = std::collections::HashMap::<String, u64>::new();
            // Best-effort label per discovered URL (anchor text), to help the planner pick.
            let mut link_labels = std::collections::HashMap::<String, String>::new();

            // If Firecrawl is configured, allow the agentic selector to opt into it for a single URL.
            // This is bounded (at most `max_urls` total fetches) and opt-in per-URL.
            let firecrawl_agentic = if !no_network
                && (has_env("WEBPIPE_FIRECRAWL_API_KEY") || has_env("FIRECRAWL_API_KEY"))
            {
                webpipe_local::firecrawl::FirecrawlClient::from_env(self.http.clone()).ok()
            } else {
                None
            };
            let mut firecrawl_disabled = false;
            let mut agentic_force_firecrawl_next: bool = false;
            let planner_max_calls = args
                .planner_max_calls
                .unwrap_or_else(Self::planner_max_calls_from_env)
                .min(10);

            // Optional LLM planner for agentic URL selection (bounded).
            //
            // We intentionally keep the contract tiny: the planner may only choose among a bounded
            // candidate set (derived deterministically), and may optionally request a per-URL Firecrawl
            // fetch (also bounded and opt-in).
            let planner_requested =
                agentic_selector.as_str() == "llm" || agentic_selector.as_str() == "auto";
            let planner_client: Option<webpipe_local::openai_compat::OpenAiCompatClient> =
                if planner_requested && planner_max_calls > 0 && !no_network {
                    webpipe_local::openai_compat::OpenAiCompatClient::from_env(
                        self.http.clone(),
                        None,
                    )
                    .ok()
                } else {
                    None
                };
            let mut planner_calls: usize = 0;
            let mut max_search_rounds = args
                .agentic_max_search_rounds
                .or_else(|| {
                    std::env::var("WEBPIPE_AGENTIC_MAX_SEARCH_ROUNDS")
                        .ok()
                        .and_then(|v| v.trim().parse::<usize>().ok())
                })
                .unwrap_or(1)
                .clamp(1, 5);
            if args.agentic_max_search_rounds.is_none() {
                if let Some(v) = preset_max_search_rounds {
                    max_search_rounds = v.clamp(1, 5);
                }
            }
            let frontier_max = args.agentic_frontier_max.unwrap_or(200).clamp(50, 2_000);
            let mut stuck_streak: usize = 0;

            let mut deadline_exceeded_partial: bool = false;
            let remaining_ms = || -> u64 {
                deadline
                    .checked_duration_since(std::time::Instant::now())
                    .map(|d| d.as_millis() as u64)
                    .unwrap_or(0)
            };
            let max_parallel_urls = args
                .max_parallel_urls
                .unwrap_or(3)
                .clamp(1, 10)
                .min(max_urls);

            while per_url.len() < max_urls {
                // Hard-stop the loop when we ran out of wall clock budget. Return partial results.
                let rem0 = remaining_ms();
                if rem0 < 800 {
                    deadline_exceeded_partial = true;
                    break;
                }
                // Bound per-request time to the remaining budget (leave a small margin for processing).
                let timeout_ms_eff = timeout_ms.min(rem0.saturating_sub(200).max(1_000));

                // Wide parallel hydration: run a small batch concurrently.
                //
                // This reduces tail latency for multi-URL evidence packs while preserving deterministic ordering.
                //
                // We enable this in:
                // - non-agentic runs (straight urls-mode hydration), and
                // - a safe agentic subset: user-provided urls=[...] with len>1, agentic_selector="lexical".
                //   In that case, "agentic expansion" is disabled anyway, and URL ordering depends only on the
                //   deterministic lexical selector (no fetched-content feedback), so we can pre-pick a small batch.
                let agentic_parallel_ok = agentic
                    && user_urls_provided
                    && user_urls_len > 1
                    && agentic_selector.as_str() == "lexical";
                if (agentic_parallel_ok || !agentic) && max_parallel_urls > 1 {
                    if frontier.is_empty() {
                        break;
                    }

                    // Pull a deterministic batch from the frontier.
                    let want = (max_urls.saturating_sub(per_url.len()))
                        .min(max_parallel_urls)
                        .min(frontier.len());
                    let mut batch_urls: Vec<String> = Vec::new();
                    if agentic_parallel_ok {
                        // Simulate the lexical agentic selector (without fetching) for `want` picks,
                        // recomputing token weights each time to preserve semantics.
                        for _ in 0..want {
                            if frontier.is_empty() {
                                break;
                            }

                            // Token rarity weights over current frontier (cheap DF over URL strings).
                            let token_weights: std::collections::HashMap<String, u64> = {
                                let mut df: std::collections::HashMap<String, u64> =
                                    q_toks.iter().map(|t| (t.clone(), 0u64)).collect();
                                for u in &frontier {
                                    let u_scrub = textprep::scrub(u);
                                    let toks: std::collections::BTreeSet<&str> =
                                        u_scrub.split_whitespace().collect();
                                    for t in &q_toks {
                                        if toks.contains(t.as_str()) {
                                            if let Some(v) = df.get_mut(t) {
                                                *v += 1;
                                            }
                                        }
                                    }
                                }
                                df.into_iter()
                                    .map(|(t, c)| {
                                        let w = 1_000u64 / (1 + c);
                                        (t, w.max(1))
                                    })
                                    .collect()
                            };

                            // Compute (url_score, prior_score) for each frontier URL, then pick a Pareto-optimal
                            // candidate and break ties deterministically.
                            let mut url_scores: Vec<u64> = Vec::with_capacity(frontier.len());
                            let mut prior_scores: Vec<u64> = Vec::with_capacity(frontier.len());
                            let mut canon: Vec<String> = Vec::with_capacity(frontier.len());
                            for u in &frontier {
                                let u_scrub = textprep::scrub(u);
                                let toks: std::collections::BTreeSet<&str> =
                                    u_scrub.split_whitespace().collect();
                                let mut s = 0u64;
                                for t in &q_toks {
                                    if toks.contains(t.as_str()) {
                                        s = s.saturating_add(*token_weights.get(t).unwrap_or(&1));
                                    }
                                }
                                if url_looks_like_auth_or_challenge(u) {
                                    s /= 4;
                                }
                                let k = canonicalize_url_no_frag(u).unwrap_or_else(|| u.clone());
                                let p = *priors.get(&k).unwrap_or(&0);
                                url_scores.push(s);
                                prior_scores.push(p);
                                canon.push(k);
                            }

                            let metrics: Vec<Vec<f32>> = (0..frontier.len())
                                .map(|i| vec![url_scores[i] as f32, prior_scores[i] as f32])
                                .collect();
                            let mut idxs = pare::pareto_indices(&metrics)
                                .unwrap_or_else(|| (0..frontier.len()).collect());
                            // Prefer higher prior (content-backed), then higher url_score, then stable order.
                            idxs.sort_by(|&ia, &ib| {
                                prior_scores[ib]
                                    .cmp(&prior_scores[ia])
                                    .then_with(|| url_scores[ib].cmp(&url_scores[ia]))
                                    .then_with(|| ia.cmp(&ib))
                            });
                            let mut best_i = idxs.first().copied().unwrap_or(0);
                            if best_i >= frontier.len() {
                                best_i = 0;
                            }

                            let picked = frontier.swap_remove(best_i);
                            let best_s = url_scores.get(best_i).copied().unwrap_or(0);
                            let trace_obj = serde_json::json!({
                                "picked_url": canonicalize_url_no_frag(&picked).unwrap_or_else(|| picked.clone()),
                                "score": best_s,
                                "prior": *priors.get(&canonicalize_url_no_frag(&picked).unwrap_or_else(|| picked.clone())).unwrap_or(&0),
                                "selector": "lexical",
                                "planner_action": "pick_url",
                                "planner_reason": serde_json::Value::Null,
                                "planner_calls": planner_calls,
                                "force_firecrawl": false,
                                "frontier_len_before": frontier.len() + 1
                            });
                            agentic_trace.push(trace_obj);
                            batch_urls.push(picked);
                        }
                    } else {
                        for _ in 0..want {
                            if frontier.is_empty() {
                                break;
                            }
                            batch_urls.push(frontier.remove(0));
                        }
                    }

                    // Helper: warning penalty for dynamic warning strings.
                    fn warning_penalty_strs(warnings: &[String]) -> i64 {
                        let mut p: i64 = 0;
                        for w in warnings {
                            match normalize_warning_code(w.as_str()) {
                                "blocked_by_js_challenge" => p += 100,
                                "empty_extraction" => p += 80,
                                "http_status_error" => p += 60,
                                "http_rate_limited" => p += 60,
                                "main_content_low_signal" => p += 25,
                                "chunks_filtered_low_signal" => p += 15,
                                "body_truncated_by_max_bytes" => p += 12,
                                "text_truncated_by_max_chars" => p += 8,
                                "retried_due_to_truncation" => p += 3,
                                "truncation_retry_used" => p += 3,
                                "truncation_retry_failed" => p += 8,
                                "pdf_extract_failed" => p += 15,
                                "pdf_extract_panicked" => p += 20,
                                "pdf_strings_fallback_used" => p += 10,
                                "pdf_shellout_used" => p += 2,
                                "pdf_shellout_unavailable" => p += 15,
                                "arxiv_pdf_fallback_to_html" => p += 0,
                                "openreview_pdf_fallback_to_forum" => p += 0,
                                "openreview_pdf_fallback_to_api" => p += 0,
                                "pandoc_failed" => p += 12,
                                "image_no_text_extraction" => p += 30,
                                "media_no_text_extraction" => p += 30,
                                "cache_error" => p += 10,
                                "fetch_failed" => p += 12,
                                _ => {}
                            }
                        }
                        p
                    }

                    #[derive(Debug)]
                    struct OneOut {
                        one: serde_json::Value,
                        candidates: Vec<ChunkCandidate>,
                        ok: bool,
                        hard_junk: bool,
                        soft_junk: bool,
                    }

                    use futures::future::join_all;
                    let fetch_backend0 = fetch_backend.clone();
                    let query0 = query.clone();
                    let futs = batch_urls.iter().cloned().map(|url0| {
                        let fetch_backend1 = fetch_backend0.clone();
                        let query1 = query0.clone();
                        async move {
                        let per_t0 = std::time::Instant::now();
                        let r = self
                            .web_extract(p(WebExtractArgs {
                                url: Some(url0.clone()),
                                fetch_backend: Some(fetch_backend1.clone()),
                                no_network: Some(no_network),
                                timeout_ms: Some(timeout_ms_eff),
                                max_bytes: Some(max_bytes),
                                cache_read: Some(cache_read_effective),
                                cache_write: Some(cache_write_effective),
                                cache_ttl_s,
                                width: Some(width),
                                max_chars: Some(max_chars),
                                query: Some(query1.clone()).filter(|s| !s.trim().is_empty()),
                                top_chunks: Some(top_chunks),
                                max_chunk_chars: Some(max_chunk_chars),
                                include_text: Some(include_text),
                                include_links: Some(include_links),
                                max_links: Some(max_links),
                                include_structure: Some(include_structure),
                                max_outline_items: Some(max_outline_items),
                                max_blocks: Some(max_blocks),
                                max_block_chars: Some(max_block_chars),
                                semantic_rerank: Some(semantic_rerank),
                                semantic_auto_fallback: Some(false),
                                semantic_top_k: Some(semantic_top_k),
                                retry_on_truncation: Some(retry_on_truncation),
                                truncation_retry_max_bytes,
                            }))
                            .await?;
                        let v = payload_from_result(&r);
                        let ok = v.get("ok").and_then(|x| x.as_bool()).unwrap_or(false);

                        // Normalize warnings to Vec<String>.
                        let mut warnings: Vec<String> = v
                            .get("warnings")
                            .and_then(|x| x.as_array())
                            .into_iter()
                            .flatten()
                            .filter_map(|x| x.as_str().map(|s| s.to_string()))
                            .collect();
                        let status_u = v.get("status").and_then(|x| x.as_u64()).unwrap_or(200);
                        let status_bad = status_u >= 400;
                        if status_bad && !warnings.iter().any(|w| w == "http_status_error") {
                            warnings.push("http_status_error".to_string());
                        }
                        if status_u == 429 && !warnings.iter().any(|w| w == "http_rate_limited") {
                            warnings.push("http_rate_limited".to_string());
                        }
                        let hard_junk = warnings
                            .iter()
                            .any(|w| normalize_warning_code(w.as_str()) == "blocked_by_js_challenge");
                        let soft_junk = warnings.iter().any(|w| {
                            matches!(
                                normalize_warning_code(w.as_str()),
                                "empty_extraction" | "main_content_low_signal" | "chunks_filtered_low_signal"
                            )
                        });

                        // Collect chunk candidates from extract.chunks.
                        let wp = warning_penalty_strs(&warnings);
                        let cache_hit = v
                            .get("fetch_source")
                            .and_then(|x| x.as_str())
                            .unwrap_or("")
                            == "cache";
                        let mut candidates: Vec<ChunkCandidate> = Vec::new();
                        if ok && !status_bad {
                            if let Some(chs) = v
                                .get("extract")
                                .and_then(|e| e.get("chunks"))
                                .and_then(|x| x.as_array())
                            {
                                for c in chs {
                                    let score = c
                                        .get("score")
                                        .and_then(|x| x.as_u64())
                                        .unwrap_or(0);
                                    let start_char = c
                                        .get("start_char")
                                        .and_then(|x| x.as_u64())
                                        .unwrap_or(0) as usize;
                                    let end_char = c
                                        .get("end_char")
                                        .and_then(|x| x.as_u64())
                                        .unwrap_or(0) as usize;
                                    let text = c
                                        .get("text")
                                        .and_then(|x| x.as_str())
                                        .unwrap_or("")
                                        .to_string();
                                    candidates.push(ChunkCandidate {
                                        url: url0.clone(),
                                        score,
                                        start_char,
                                        end_char,
                                        text,
                                        warning_penalty: wp,
                                        cache_hit,
                                    });
                                }
                            }
                        }

                        // Build per-URL object consistent with search_evidence output.
                        let mut warning_codes: Vec<String> = warnings
                            .iter()
                            .map(|w| normalize_warning_code(w.as_str()).to_string())
                            .collect();
                        warning_codes.sort();
                        warning_codes.dedup();
                        let warning_code_refs: Vec<&str> =
                            warning_codes.iter().map(|s| s.as_str()).collect();
                        let mut one = serde_json::json!({
                            "url": url0,
                            "ok": ok,
                            "fetch_backend": fetch_backend1,
                            "final_url": v.get("final_url").cloned().unwrap_or(serde_json::Value::Null),
                            "status": v.get("status").cloned().unwrap_or(serde_json::Value::Null),
                            "content_type": v.get("content_type").cloned().unwrap_or(serde_json::Value::Null),
                            "bytes": v.get("bytes").cloned().unwrap_or(serde_json::Value::Null),
                            "truncated": v.get("truncated").cloned().unwrap_or(serde_json::Value::Null),
                            "fetch_source": v.get("fetch_source").cloned().unwrap_or(serde_json::Value::Null),
                            "attempts": serde_json::Value::Null,
                            "extract": v.get("extract").cloned().unwrap_or(serde_json::Value::Null),
                            "warnings": serde_json::json!(warnings),
                            "warning_codes": serde_json::json!(warning_codes),
                            "warning_hints": warning_hints_from(&warning_code_refs),
                            "elapsed_ms": per_t0.elapsed().as_millis(),
                        });
                        if ok && status_bad {
                            // 4xx/5xx pages are usually error shells; include them as per-URL results
                            // but do not allow them to dominate evidence selection.
                            let code = "http_status_error";
                            match one.get_mut("warnings") {
                                Some(serde_json::Value::Array(a)) => {
                                    if !a.iter().any(|v| v.as_str() == Some(code)) {
                                        a.push(serde_json::Value::String(code.to_string()));
                                    }
                                }
                                _ => {
                                    one["warnings"] = serde_json::json!([code]);
                                }
                            }
                            match one.get_mut("warning_codes") {
                                Some(serde_json::Value::Array(a)) => {
                                    if !a.iter().any(|v| v.as_str() == Some(code)) {
                                        a.push(serde_json::Value::String(code.to_string()));
                                    }
                                }
                                _ => {
                                    one["warning_codes"] = serde_json::json!([code]);
                                }
                            }
                            match one.get_mut("warning_hints") {
                                Some(serde_json::Value::Object(m)) => {
                                    if !m.contains_key(code) {
                                        if let serde_json::Value::Object(h) =
                                            warning_hints_from(&[code])
                                        {
                                            if let Some(v) = h.get(code) {
                                                m.insert(code.to_string(), v.clone());
                                            }
                                        }
                                    }
                                }
                                _ => {
                                    one["warning_hints"] = warning_hints_from(&[code]);
                                }
                            }
                        }
                        let cache_io_disabled = std::env::var("WEBPIPE_CACHE_IO_TIMEOUT_MS")
                            .ok()
                            .and_then(|s| s.trim().parse::<u64>().ok())
                            .unwrap_or(2_000)
                            == 0;
                        if cache_io_disabled {
                            // Surface cache IO disablement deterministically for agentic callers:
                            // if cache IO is disabled (timeout_ms=0), warn even if we did no cache ops.
                            let code = "cache_io_timeout";
                            match one.get_mut("warnings") {
                                Some(serde_json::Value::Array(a)) => {
                                    if !a.iter().any(|v| v.as_str() == Some(code)) {
                                        a.push(serde_json::Value::String(code.to_string()));
                                    }
                                }
                                _ => {
                                    one["warnings"] = serde_json::json!([code]);
                                }
                            }
                            match one.get_mut("warning_codes") {
                                Some(serde_json::Value::Array(a)) => {
                                    if !a.iter().any(|v| v.as_str() == Some(code)) {
                                        a.push(serde_json::Value::String(code.to_string()));
                                    }
                                }
                                _ => {
                                    one["warning_codes"] = serde_json::json!([code]);
                                }
                            }
                            match one.get_mut("warning_hints") {
                                Some(serde_json::Value::Object(m)) => {
                                    if !m.contains_key(code) {
                                        if let serde_json::Value::Object(h) =
                                            warning_hints_from(&[code])
                                        {
                                            if let Some(v) = h.get(code) {
                                                m.insert(code.to_string(), v.clone());
                                            }
                                        }
                                    }
                                }
                                _ => {
                                    one["warning_hints"] = warning_hints_from(&[code]);
                                }
                            }
                        }
                        if !ok {
                            if let Some(e) = v.get("error") {
                                one["error"] = e.clone();
                            }
                        }

                        // Apply compact shaping consistent with the sequential path.
                        if compact {
                            if let Some(ex) = one.get_mut("extract").and_then(|v| v.as_object_mut()) {
                                ex.remove("chunks");
                                ex.remove("width");
                                ex.remove("max_chars");
                                ex.remove("top_chunks");
                                ex.remove("max_chunk_chars");
                            }
                            let mut out = serde_json::Map::new();
                            for k in [
                                "url",
                                "final_url",
                                "ok",
                                "fetch_backend",
                                "attempts",
                                "status",
                                "content_type",
                                "bytes",
                                "elapsed_ms",
                                "warnings",
                                "warning_codes",
                                "warning_hints",
                                "extract",
                            ] {
                                if let Some(v) = one.get(k) {
                                    out.insert(k.to_string(), v.clone());
                                }
                            }
                            one = serde_json::Value::Object(out);
                        }

                        Ok::<OneOut, McpError>(OneOut {
                            one,
                            candidates,
                            ok,
                            hard_junk,
                            soft_junk,
                        })
                    }
                    });

                    let outs: Vec<OneOut> = join_all(futs)
                        .await
                        .into_iter()
                        .collect::<Result<Vec<_>, _>>()?;
                    for o in outs {
                        if o.ok {
                            total_urls_ok = total_urls_ok.saturating_add(1);
                            if o.hard_junk {
                                hard_junk_urls = hard_junk_urls.saturating_add(1);
                            } else if o.soft_junk {
                                soft_junk_urls = soft_junk_urls.saturating_add(1);
                            }
                        }
                        for c in o.candidates {
                            all_chunks.push(c);
                        }
                        per_url.push(o.one);
                    }
                    continue;
                }
                if frontier.is_empty() {
                    // Agentic escape hatch: do another bounded search round when we ran out of frontier.
                    // This is cost-aware: default max_search_rounds=1 unless explicitly enabled.
                    if !agentic
                        || no_network
                        || query.trim().is_empty()
                        || search_rounds >= max_search_rounds
                    {
                        break;
                    }

                    // Try to broaden search results. Keep this bounded and mostly deterministic.
                    // If the user requested a concrete provider, we try the other configured provider once.
                    let brave_ok =
                        has_env("WEBPIPE_BRAVE_API_KEY") || has_env("BRAVE_SEARCH_API_KEY");
                    let tavily_ok = has_env("WEBPIPE_TAVILY_API_KEY") || has_env("TAVILY_API_KEY");
                    let (prov2, auto2) = if requested_provider.as_str() == "auto" {
                        ("auto".to_string(), "merge".to_string())
                    } else if requested_provider.as_str() == "brave" && tavily_ok {
                        ("tavily".to_string(), requested_auto_mode.clone())
                    } else if requested_provider.as_str() == "tavily" && brave_ok {
                        ("brave".to_string(), requested_auto_mode.clone())
                    } else {
                        // Fall back to auto routing (it will return not_configured if neither is set).
                        ("auto".to_string(), "merge".to_string())
                    };

                    search_rounds = search_rounds.saturating_add(1);
                    let query2 = query.trim().to_string();
                    let sr2 = self
                        .web_search(p(WebSearchArgs {
                            query: Some(query2.clone()),
                            provider: Some(prov2.clone()),
                            auto_mode: Some(auto2.clone()),
                            max_results: Some(max_results),
                            language: None,
                            country: None,
                            timeout_ms: Some(timeout_ms_eff),
                        }))
                        .await?;
                    let sv2 = payload_from_result(&sr2);
                    search_steps.push(serde_json::json!({
                        "ok": sv2.get("ok").and_then(|v| v.as_bool()).unwrap_or(false),
                        "requested_provider": prov2,
                        "auto_mode": auto2,
                        "query_rewrite": "",
                        "backend_provider": sv2.get("backend_provider").cloned().unwrap_or(serde_json::Value::Null),
                        "result_count": sv2.get("results").and_then(|x| x.as_array()).map(|a| a.len()).unwrap_or(0),
                        "warnings": sv2.get("warnings").cloned().unwrap_or(serde_json::Value::Null),
                        "providers": sv2.get("providers").cloned().unwrap_or(serde_json::Value::Null),
                        "selection": sv2.get("selection").cloned().unwrap_or(serde_json::Value::Null),
                    }));

                    if sv2.get("ok").and_then(|v| v.as_bool()) != Some(true) {
                        break;
                    }

                    let mut added = 0usize;
                    if let Some(rs) = sv2.get("results").and_then(|v| v.as_array()) {
                        for r in rs {
                            if let Some(u) = r.get("url").and_then(|v| v.as_str()) {
                                if !url_allowed_by_domain_filters(u, &domains_allow, &domains_deny)
                                {
                                    continue;
                                }
                                if let Some(k) = canonicalize_url_no_frag(u) {
                                    if seen_frontier.insert(k) {
                                        frontier.push(u.to_string());
                                        added += 1;
                                        if frontier.len() >= frontier_max {
                                            break;
                                        }
                                    }
                                }
                            }
                        }
                    }
                    agentic_trace.push(serde_json::json!({
                        "search_more": true,
                        "round": search_rounds,
                        "frontier_added": added,
                        "frontier_len_after": frontier.len(),
                    }));

                    // Continue to selection now that we have a frontier again.
                    continue;
                }

                let next_url: String = if agentic {
                    // Token rarity weights over current frontier (cheap DF over URL strings).
                    let token_weights: std::collections::HashMap<String, u64> = {
                        let mut df: std::collections::HashMap<String, u64> =
                            q_toks.iter().map(|t| (t.clone(), 0u64)).collect();
                        for u in &frontier {
                            let u_scrub = textprep::scrub(u);
                            let toks: std::collections::BTreeSet<&str> =
                                u_scrub.split_whitespace().collect();
                            for t in &q_toks {
                                if toks.contains(t.as_str()) {
                                    if let Some(v) = df.get_mut(t) {
                                        *v += 1;
                                    }
                                }
                            }
                        }
                        df.into_iter()
                            .map(|(t, c)| {
                                let w = 1_000u64 / (1 + c);
                                (t, w.max(1))
                            })
                            .collect()
                    };

                    // Compute (url_score, prior_score) for each frontier URL, then pick a Pareto-optimal
                    // candidate and break ties deterministically. Optionally, ask an LLM to choose
                    // among a bounded candidate set (falls back to lexical if unavailable).
                    let mut url_scores: Vec<u64> = Vec::with_capacity(frontier.len());
                    let mut prior_scores: Vec<u64> = Vec::with_capacity(frontier.len());
                    let mut canon: Vec<String> = Vec::with_capacity(frontier.len());
                    for u in &frontier {
                        let u_scrub = textprep::scrub(u);
                        let toks: std::collections::BTreeSet<&str> =
                            u_scrub.split_whitespace().collect();
                        let mut s = 0u64;
                        for t in &q_toks {
                            if toks.contains(t.as_str()) {
                                s = s.saturating_add(*token_weights.get(t).unwrap_or(&1));
                            }
                        }
                        // Generic penalty: avoid auth/challenge/consent URLs unless they are clearly relevant.
                        if url_looks_like_auth_or_challenge(u) {
                            s /= 4;
                        }
                        let k = canonicalize_url_no_frag(u).unwrap_or_else(|| u.clone());
                        let p = *priors.get(&k).unwrap_or(&0);
                        url_scores.push(s);
                        prior_scores.push(p);
                        canon.push(k);
                    }

                    let metrics: Vec<Vec<f32>> = (0..frontier.len())
                        .map(|i| vec![url_scores[i] as f32, prior_scores[i] as f32])
                        .collect();
                    let mut idxs = pare::pareto_indices(&metrics)
                        .unwrap_or_else(|| (0..frontier.len()).collect());
                    // Prefer higher prior (content-backed), then higher url_score, then stable order.
                    idxs.sort_by(|&ia, &ib| {
                        prior_scores[ib]
                            .cmp(&prior_scores[ia])
                            .then_with(|| url_scores[ib].cmp(&url_scores[ia]))
                            .then_with(|| ia.cmp(&ib))
                    });
                    let mut best_i = idxs.first().copied().unwrap_or(0);

                    if best_i >= frontier.len() {
                        best_i = idxs.first().copied().unwrap_or(0);
                    }

                    let mut selector_used: &'static str = "lexical";

                    // Optional planner: choose among a bounded candidate set.
                    //
                    // Contract:
                    // - Must choose a URL from `candidates`.
                    // - May optionally request Firecrawl for the chosen URL (if configured).
                    // - May request "search_more" (clears frontier to trigger bounded extra search round).
                    let mut planner_reason: Option<String> = None;
                    let mut planner_force_firecrawl: bool = false;
                    let mut planner_action: String = "pick_url".to_string();
                    // Agentic planner chat history: keep stable system + short assistant notes.
                    // This helps provider prompt caching across multi-step selection loops.
                    let mut planner_history: Vec<webpipe_local::openai_compat::ChatMessage> =
                        Vec::new();
                    if let Some(client) = planner_client.as_ref() {
                        if planner_calls < planner_max_calls
                            && agentic_selector.as_str() != "lexical"
                            && !query.trim().is_empty()
                        {
                            planner_calls = planner_calls.saturating_add(1);

                            let top_k = 7usize.min(idxs.len());
                            let cand_idxs = idxs.iter().copied().take(top_k).collect::<Vec<_>>();
                            let mut cand_urls: Vec<String> = Vec::new();
                            let mut candidates: Vec<serde_json::Value> = Vec::new();
                            for &i in &cand_idxs {
                                if i >= frontier.len() {
                                    continue;
                                }
                                let url = frontier[i].clone();
                                let canon_url =
                                    canon.get(i).cloned().unwrap_or_else(|| url.clone());
                                cand_urls.push(canon_url.clone());
                                let label =
                                    link_labels.get(&canon_url).cloned().unwrap_or_default();
                                candidates.push(serde_json::json!({
                                    "url": canon_url,
                                    "label": label,
                                    "url_score": url_scores.get(i).copied().unwrap_or(0),
                                    "prior_score": prior_scores.get(i).copied().unwrap_or(0),
                                    "auth_wall": url_looks_like_auth_or_challenge(&url),
                                }));
                            }

                            let allow_firecrawl =
                                firecrawl_agentic.is_some() && !firecrawl_disabled;
                            let system = r#"You are a URL selector for a bounded web search→fetch→extract loop.

You MUST return JSON only, matching this shape:
{
  "action": "pick_url" | "search_more" | "stop",
  "picked_url": "https://…",              // required when action="pick_url"; must be one of the candidates
  "force_fetch_backend": "firecrawl" | null,
  "reason": "short string"
}

Rules:
- Never invent URLs; if action="pick_url", picked_url must exactly match one candidate URL.
- Prefer URLs likely to contain substantive content for the query.
- Avoid auth/challenge/consent walls unless unavoidable.
- Use force_fetch_backend="firecrawl" only when the page is likely JS-blocked or the content is likely hidden.
"#;
                            let user = serde_json::to_string(&serde_json::json!({
                                "query": query,
                                "candidates": candidates,
                                "allow": {
                                    "search_more": true,
                                    "force_fetch_backend": if allow_firecrawl { ["firecrawl"].as_slice() } else { [].as_slice() }
                                }
                            }))
                            .unwrap_or_else(|_| "{}".to_string());

                            if planner_history.is_empty() {
                                planner_history.push(
                                    webpipe_local::openai_compat::ChatMessage::system(system),
                                );
                            }
                            let mut msgs = planner_history.clone();
                            // Do not retain this large user payload in history.
                            msgs.push(webpipe_local::openai_compat::ChatMessage::user(&user));

                            let resp = client
                                .chat_messages_with_options(
                                    msgs,
                                    timeout_ms_eff,
                                    Some(300),
                                    Some(0.0),
                                    Some(1.0),
                                    true,
                                    webpipe_local::openai_compat::ChatOptions::default(),
                                )
                                .await;
                            match resp {
                                Ok(text) => {
                                    if let Ok(v) = serde_json::from_str::<serde_json::Value>(&text)
                                    {
                                        let action = v
                                            .get("action")
                                            .and_then(|x| x.as_str())
                                            .unwrap_or("pick_url");
                                        planner_action = action.to_string();

                                        let force = v
                                            .get("force_fetch_backend")
                                            .and_then(|x| x.as_str())
                                            .unwrap_or("");
                                        planner_force_firecrawl =
                                            allow_firecrawl && force == "firecrawl";

                                        planner_reason = v
                                            .get("reason")
                                            .and_then(|x| x.as_str())
                                            .map(|s| s.to_string());

                                        // Add short state to history for subsequent calls (no huge payloads).
                                        let picked = v
                                            .get("picked_url")
                                            .and_then(|x| x.as_str())
                                            .unwrap_or("");
                                        let note = format!(
                                            "Planner state: action={}, picked_url={}, force_firecrawl={}, reason={}",
                                            planner_action,
                                            picked,
                                            planner_force_firecrawl,
                                            planner_reason.clone().unwrap_or_default()
                                        );
                                        planner_history.push(
                                            webpipe_local::openai_compat::ChatMessage::assistant(
                                                &note,
                                            ),
                                        );

                                        if action == "search_more" {
                                            // Trigger bounded extra search round (handled at top-of-loop).
                                            frontier.clear();
                                            agentic_trace.push(serde_json::json!({
                                                "planner": true,
                                                "action": "search_more",
                                                "reason": planner_reason,
                                            }));
                                            continue;
                                        }
                                        if action == "stop" {
                                            agentic_trace.push(serde_json::json!({
                                                "planner": true,
                                                "action": "stop",
                                                "reason": planner_reason,
                                            }));
                                            break;
                                        }

                                        if let Some(picked_url) =
                                            v.get("picked_url").and_then(|x| x.as_str())
                                        {
                                            if cand_urls.iter().any(|u| u == picked_url) {
                                                // Replace lexical pick with the planner's choice.
                                                if let Some(pos) = frontier.iter().position(|u| {
                                                    canonicalize_url_no_frag(u)
                                                        .map(|k| k == picked_url)
                                                        .unwrap_or(false)
                                                }) {
                                                    best_i = pos;
                                                    selector_used = "llm";
                                                }
                                            }
                                        }
                                    }
                                }
                                Err(_) => {
                                    // Planner failures fall back to lexical.
                                }
                            }
                        }
                    }

                    if planner_force_firecrawl {
                        agentic_force_firecrawl_next = true;
                    }

                    let picked = frontier.swap_remove(best_i);
                    let best_s = url_scores.get(best_i).copied().unwrap_or(0);
                    let trace_obj = serde_json::json!({
                        "picked_url": canonicalize_url_no_frag(&picked).unwrap_or_else(|| picked.clone()),
                        "score": best_s,
                        "prior": *priors.get(&canonicalize_url_no_frag(&picked).unwrap_or_else(|| picked.clone())).unwrap_or(&0),
                        "selector": selector_used,
                        "planner_action": planner_action,
                        "planner_reason": planner_reason,
                        "planner_calls": planner_calls,
                        "force_firecrawl": planner_force_firecrawl,
                        "frontier_len_before": frontier.len() + 1
                    });
                    agentic_trace.push(trace_obj);
                    picked
                } else {
                    frontier.remove(0)
                };

                let url_owned = next_url;
                let url = &url_owned;
                let per_t0 = std::time::Instant::now();
                let mut attempts: serde_json::Value = serde_json::Value::Null;
                let use_firecrawl_agentic = !firecrawl_disabled
                    && agentic_force_firecrawl_next
                    && firecrawl_agentic.is_some();
                if agentic_force_firecrawl_next {
                    agentic_force_firecrawl_next = false;
                }
                let mut used_firecrawl_agentic = false;
                let (
                    raw_text,
                    raw_bytes,
                    final_url,
                    status,
                    content_type,
                    bytes_len,
                    truncated,
                    fetch_source,
                    used_render_fallback,
                    used_firecrawl_fallback,
                    extracted_obj,
                    cache_io_timed_out,
                ) = if let Some(fc) = if use_firecrawl_agentic {
                    firecrawl_agentic.as_ref()
                } else {
                    firecrawl_primary.as_ref()
                } {
                    let max_age_ms = cache_ttl_s.map(|s| s.saturating_mul(1000));
                    let r = match fc.fetch_markdown(url, timeout_ms, max_age_ms).await {
                        Ok(r) => r,
                        Err(e) => {
                            let msg = e.to_string();
                            self.stats_record_fetch_backend(
                                "firecrawl",
                                false,
                                per_t0.elapsed().as_millis() as u64,
                                Some(msg.as_str()),
                            );
                            firecrawl_disabled = true;
                            per_url.push(serde_json::json!({
                                    "url": url,
                                    "ok": false,
                                    "stage": "fetch",
                                    "fetch_backend": "firecrawl",
                                    "error": error_obj(ErrorCode::FetchFailed, msg, "Firecrawl fetch failed for this URL."),
                                    "elapsed_ms": per_t0.elapsed().as_millis()
                                }));
                            continue;
                        }
                    };
                    let md = r.markdown;
                    let md_bytes = md.as_bytes().to_vec();
                    let bytes_len = Self::approx_bytes_len(&md);
                    used_firecrawl_agentic = use_firecrawl_agentic;
                    attempts = serde_json::json!({
                        (if use_firecrawl_agentic { "firecrawl_agentic" } else { "firecrawl" }): {
                            "ok": true,
                            "elapsed_ms": r.elapsed_ms,
                            "bytes": bytes_len
                        }
                    });
                    self.stats_record_fetch_backend(
                        "firecrawl",
                        true,
                        per_t0.elapsed().as_millis() as u64,
                        None,
                    );
                    (
                        md.clone(),
                        md_bytes,
                        url.clone(),
                        200u16,
                        Some("text/markdown".to_string()),
                        bytes_len,
                        false,
                        "network",
                        false,
                        false,
                        webpipe_local::extract::ExtractedText {
                            engine: "firecrawl",
                            text: md,
                            warnings: Vec::new(),
                        },
                        false,
                    )
                } else {
                    let (url0, arxiv_attempts) = self.maybe_rewrite_arxiv_abs_url(url);
                    let (url1, gh_patch_attempts, _gh_patch_warn) =
                        self.maybe_rewrite_github_pr_or_commit_url(&url0);
                    let (url2, github_blob_attempts) = self.maybe_rewrite_github_blob_url(&url1);
                    let (url3, gist_attempts) = self.maybe_rewrite_gist_url(&url2);
                    let (fetch_url, github_repo_attempts) = self
                        .maybe_rewrite_github_repo_url(
                            &url3,
                            timeout_ms_eff,
                            max_bytes,
                            no_network,
                            cache_read,
                            cache_write,
                            cache_ttl_s,
                        )
                        .await;

                    let req = FetchRequest {
                        url: fetch_url.clone(),
                        timeout_ms: Some(timeout_ms_eff),
                        max_bytes: Some(max_bytes),
                        headers: BTreeMap::new(),
                        cache: FetchCachePolicy {
                            read: cache_read || no_network,
                            write: if no_network { false } else { cache_write },
                            ttl_s: cache_ttl_s,
                        },
                    };
                    let github_repo_attempts0 = github_repo_attempts;
                    let github_blob_attempts0 = github_blob_attempts;
                    let gh_patch_attempts0 = gh_patch_attempts;
                    let gist_attempts0 = gist_attempts;
                    let arxiv_attempts0 = arxiv_attempts;
                    let fetch_url0 = fetch_url.clone();

                    let mut fetched = if fetch_backend == "render" {
                        // Render mode: Playwright/Chromium render-to-HTML, then treat as text/html.
                        //
                        // In anonymous mode, require a configured proxy for non-localhost URLs.
                        // Playwright's proxy settings do not support socks5h://; require an HTTP proxy.
                        let pm = privacy_mode_from_env();
                        let anon_proxy = anon_proxy_from_env();
                        let proxy_for_render = if matches!(pm, PrivacyMode::Anonymous)
                            && !is_localhost_url(fetch_url0.as_str())
                        {
                            let Some(p) = anon_proxy.as_deref() else {
                                self.stats_record_fetch_backend(
                                    "render",
                                    false,
                                    per_t0.elapsed().as_millis() as u64,
                                    Some("missing_anon_proxy"),
                                );
                                per_url.push(serde_json::json!({
                                    "url": url,
                                    "ok": false,
                                    "stage": "fetch",
                                    "fetch_backend": "render",
                                    "error": error_obj(
                                        ErrorCode::NotConfigured,
                                        "anonymous mode requires a proxy for fetch_backend=\"render\"",
                                        "Set WEBPIPE_ANON_PROXY to an HTTP proxy endpoint (Tor users often run Privoxy)."
                                    ),
                                    "elapsed_ms": per_t0.elapsed().as_millis()
                                }));
                                continue;
                            };
                            let pl = p.trim().to_ascii_lowercase();
                            if pl.starts_with("socks5h://") {
                                self.stats_record_fetch_backend(
                                    "render",
                                    false,
                                    per_t0.elapsed().as_millis() as u64,
                                    Some("socks5h_not_supported_by_playwright"),
                                );
                                per_url.push(serde_json::json!({
                                    "url": url,
                                    "ok": false,
                                    "stage": "fetch",
                                    "fetch_backend": "render",
                                    "error": error_obj(
                                        ErrorCode::NotSupported,
                                        "socks5h:// proxies are not supported by Playwright proxy settings",
                                        "Use an HTTP proxy endpoint, or convert Tor SOCKS to HTTP (e.g. via Privoxy), then set WEBPIPE_ANON_PROXY to that HTTP proxy URL."
                                    ),
                                    "elapsed_ms": per_t0.elapsed().as_millis()
                                }));
                                continue;
                            }
                            Some(p)
                        } else {
                            None
                        };

                        let pr = match webpipe_local::render_playwright::render_html_playwright(
                            fetch_url0.as_str(),
                            timeout_ms_eff,
                            proxy_for_render,
                        )
                        .await
                        {
                            Ok(r) => r,
                            Err(e) => {
                                let msg = e.to_string();
                                self.stats_record_fetch_backend(
                                    "render",
                                    false,
                                    per_t0.elapsed().as_millis() as u64,
                                    Some(msg.as_str()),
                                );
                                let (code, hint) = match &e {
                                    WebpipeError::NotConfigured(_) => (
                                        ErrorCode::NotConfigured,
                                        "Install Playwright (Node) + browsers, or set fetch_backend=\"local\" for non-JS pages.",
                                    ),
                                    WebpipeError::InvalidUrl(_) => (
                                        ErrorCode::InvalidUrl,
                                        "Check that the URL is absolute (includes http/https) and is well-formed.",
                                    ),
                                    WebpipeError::NotSupported(_) => (
                                        ErrorCode::NotSupported,
                                        "This render operation is not supported by the current privacy/proxy configuration.",
                                    ),
                                    _ => (
                                        ErrorCode::FetchFailed,
                                        "Playwright render failed for this URL. Try a longer timeout_ms or set fetch_backend=\"local\".",
                                    ),
                                };
                                per_url.push(serde_json::json!({
                                    "url": url,
                                    "ok": false,
                                    "stage": "fetch",
                                    "fetch_backend": "render",
                                    "error": error_obj(
                                        code,
                                        msg,
                                        hint
                                    ),
                                    "elapsed_ms": per_t0.elapsed().as_millis()
                                }));
                                continue;
                            }
                        };

                        self.stats_record_fetch_backend(
                            "render",
                            true,
                            per_t0.elapsed().as_millis() as u64,
                            None,
                        );
                        attempts = serde_json::json!({
                            "render": {
                                "ok": true,
                                "backend": "playwright",
                                "mode": pr.mode,
                                "elapsed_ms": pr.elapsed_ms,
                                "console_error_count": pr.console_error_count
                            }
                        });
                        webpipe_core::FetchResponse {
                            url: fetch_url0.clone(),
                            final_url: pr.final_url,
                            status: pr.status.unwrap_or(200),
                            content_type: Some("text/html".to_string()),
                            headers: BTreeMap::new(),
                            bytes: pr.html.into_bytes(),
                            truncated: false,
                            source: webpipe_core::FetchSource::Network,
                            timings_ms: {
                                let mut m = BTreeMap::new();
                                m.insert("playwright_render".to_string(), pr.elapsed_ms as u128);
                                m
                            },
                        }
                    } else if no_network && !url_is_localhost(&req.url) {
                        match self.fetcher.cache_get(&req) {
                            Ok(Some(r)) => r,
                            Ok(None) => {
                                self.stats_record_fetch_backend(
                                    "local",
                                    false,
                                    per_t0.elapsed().as_millis() as u64,
                                    Some("cache_miss_no_network"),
                                );
                                let warns: Vec<&'static str> =
                                    vec!["no_network_may_require_warm_cache"];
                                let codes = warning_codes_from(&warns);
                                per_url.push(serde_json::json!({
                                        "url": url,
                                        "ok": false,
                                        "stage": "fetch",
                                        "fetch_backend": "local",
                                        "no_network": true,
                                        "error": error_obj(
                                            ErrorCode::FetchFailed,
                                            "cache miss in no_network mode",
                                            "Warm the cache first (run without no_network), or set no_network=false."
                                        ),
                                        "warnings": warns,
                                        "warning_codes": codes.clone(),
                                        "warning_hints": warning_hints_from(&codes),
                                        "elapsed_ms": per_t0.elapsed().as_millis()
                                    }));
                                continue;
                            }
                            Err(e) => {
                                let msg = e.to_string();
                                self.stats_record_fetch_backend(
                                    "local",
                                    false,
                                    per_t0.elapsed().as_millis() as u64,
                                    Some(msg.as_str()),
                                );
                                per_url.push(serde_json::json!({
                                        "url": url,
                                        "ok": false,
                                        "stage": "fetch",
                                        "fetch_backend": "local",
                                        "no_network": true,
                                        "error": error_obj(ErrorCode::CacheError, msg, "Cache read failed in no_network mode."),
                                        "elapsed_ms": per_t0.elapsed().as_millis()
                                    }));
                                continue;
                            }
                        }
                    } else {
                        match self.fetcher.fetch(&req).await {
                            Ok(r) => r,
                            Err(e) => {
                                let msg = e.to_string();
                                self.stats_record_fetch_backend(
                                    "local",
                                    false,
                                    per_t0.elapsed().as_millis() as u64,
                                    Some(msg.as_str()),
                                );
                                per_url.push(serde_json::json!({
                                        "url": url,
                                        "ok": false,
                                        "stage": "fetch",
                                        "fetch_backend": "local",
                                        "error": error_obj(ErrorCode::FetchFailed, msg, "Fetch failed for this URL."),
                                        "elapsed_ms": per_t0.elapsed().as_millis()
                                    }));
                                continue;
                            }
                        }
                    };

                    // Optional truncation retry (local-only, opt-in).
                    //
                    // If the response was truncated by max_bytes, try again with a larger cap so
                    // we can recover content that lives beyond the initial prefix (common on JS-heavy docs).
                    //
                    // IMPORTANT: only do this when explicitly requested; callers sometimes set small max_bytes
                    // intentionally to limit bandwidth/latency.
                    let mut local_attempt_obj: Option<serde_json::Value> = None;
                    let mut local_retry_obj: Option<serde_json::Value> = None;
                    if fetch_backend == "local"
                        && !no_network
                        && retry_on_truncation
                        && fetched.truncated
                        && max_bytes > 0
                    {
                        let retry_cap_default = max_bytes.saturating_mul(2);
                        let retry_cap = truncation_retry_max_bytes
                            .unwrap_or(retry_cap_default)
                            .max(max_bytes.saturating_add(1))
                            .min(20_000_000);
                        if retry_cap > max_bytes {
                            // Snapshot first attempt for diagnostics.
                            local_attempt_obj = Some(serde_json::json!({
                                "ok": true,
                                "final_url": fetched.final_url.clone(),
                                "status": fetched.status,
                                "content_type": fetched.content_type.clone(),
                                "bytes": fetched.bytes.len(),
                                "truncated": true,
                                "source": match fetched.source { FetchSource::Cache => "cache", FetchSource::Network => "network" },
                                "max_bytes": max_bytes
                            }));
                            let req2 = FetchRequest {
                                url: fetch_url0.clone(),
                                timeout_ms: Some(timeout_ms_eff),
                                max_bytes: Some(retry_cap),
                                headers: BTreeMap::new(),
                                cache: FetchCachePolicy {
                                    read: cache_read,
                                    write: cache_write,
                                    ttl_s: cache_ttl_s,
                                },
                            };
                            if let Ok(r2) = self.fetcher.fetch(&req2).await {
                                local_retry_obj = Some(serde_json::json!({
                                    "ok": true,
                                    "final_url": r2.final_url.clone(),
                                    "status": r2.status,
                                    "content_type": r2.content_type.clone(),
                                    "bytes": r2.bytes.len(),
                                    "truncated": r2.truncated,
                                    "source": match r2.source { FetchSource::Cache => "cache", FetchSource::Network => "network" },
                                    "max_bytes": retry_cap
                                }));
                                fetched = r2;
                            } else {
                                local_retry_obj = Some(serde_json::json!({
                                    "ok": false,
                                    "error": "retry_fetch_failed",
                                    "max_bytes": retry_cap
                                }));
                            }
                        }
                    }
                    if local_retry_obj.is_some() {
                        let mut a = serde_json::Map::new();
                        if let Some(g) = arxiv_attempts0.clone() {
                            a.insert("arxiv_rewrite".to_string(), g);
                        }
                        if let Some(g) = gh_patch_attempts0.clone() {
                            a.insert("github_patch_rewrite".to_string(), g);
                        }
                        if let Some(g) = github_blob_attempts0.clone() {
                            a.insert("github_blob_rewrite".to_string(), g);
                        }
                        if let Some(g) = gist_attempts0.clone() {
                            a.insert("gist_rewrite".to_string(), g);
                        }
                        if let Some(g) = github_repo_attempts0.clone() {
                            a.insert("github_repo_rewrite".to_string(), g);
                        }
                        if let Some(obj) = local_attempt_obj.take() {
                            a.insert("local".to_string(), obj);
                        }
                        if let Some(obj) = local_retry_obj.take() {
                            a.insert("local_retry".to_string(), obj);
                        }
                        attempts = serde_json::Value::Object(a);
                    } else if arxiv_attempts0.is_some()
                        || gh_patch_attempts0.is_some()
                        || github_blob_attempts0.is_some()
                        || gist_attempts0.is_some()
                        || github_repo_attempts0.is_some()
                    {
                        let mut a = serde_json::Map::new();
                        if let Some(g) = arxiv_attempts0.clone() {
                            a.insert("arxiv_rewrite".to_string(), g);
                        }
                        if let Some(g) = gh_patch_attempts0.clone() {
                            a.insert("github_patch_rewrite".to_string(), g);
                        }
                        if let Some(g) = github_blob_attempts0.clone() {
                            a.insert("github_blob_rewrite".to_string(), g);
                        }
                        if let Some(g) = gist_attempts0.clone() {
                            a.insert("gist_rewrite".to_string(), g);
                        }
                        if let Some(g) = github_repo_attempts0.clone() {
                            a.insert("github_repo_rewrite".to_string(), g);
                        }
                        attempts = serde_json::Value::Object(a);
                    }
                    self.stats_record_fetch_backend(
                        "local",
                        true,
                        per_t0.elapsed().as_millis() as u64,
                        None,
                    );
                    let mut local_final_url = fetched.final_url.clone();
                    let mut local_status = fetched.status;
                    let mut local_content_type = fetched.content_type.clone();
                    let mut local_bytes_len = fetched.bytes.len();
                    let mut local_truncated = fetched.truncated;
                    let mut local_source = match fetched.source {
                        FetchSource::Cache => "cache",
                        FetchSource::Network => "network",
                    };
                    let cache_io_disabled = std::env::var("WEBPIPE_CACHE_IO_TIMEOUT_MS")
                        .ok()
                        .and_then(|s| s.trim().parse::<u64>().ok())
                        .unwrap_or(2_000)
                        == 0;
                    let mut cache_io_timed_out = cache_io_disabled
                        || fetched.timings_ms.contains_key("cache_get_timeout")
                        || fetched.timings_ms.contains_key("cache_put_timeout");

                    // Attempt local extraction first; if it's empty and fallback is enabled + configured,
                    // retry *just this URL* via Firecrawl.
                    let mut local_pdf_like =
                        Self::content_type_is_pdf(local_content_type.as_deref())
                            || Self::url_looks_like_pdf(&local_final_url)
                            || webpipe_local::extract::bytes_look_like_pdf(&fetched.bytes);
                    let mut local_raw_text = if local_pdf_like {
                        String::new()
                    } else {
                        fetched.text_lossy()
                    };
                    let mut local_extracted_obj =
                        webpipe_local::extract::best_effort_text_from_bytes(
                            &fetched.bytes,
                            local_content_type.as_deref(),
                            &local_final_url,
                            width,
                            500,
                        );

                    // Content-first: if PDF extraction is degraded (common for malformed/compressed
                    // PDFs), fall back to a higher-signal HTML artifact when a conservative rewrite
                    // is available (e.g. arXiv→ar5iv, OpenReview PDF→forum metadata page).
                    // and use that higher-signal content for chunk selection/evidence.
                    //
                    // Note: this mirrors the behavior of the `web_extract` tool (which already
                    // implements the same fallback), but the single-URL agentic hydration path
                    // in `web_search_extract` does its own fetch+extract.
                    if fetch_backend == "local" && !no_network && local_pdf_like {
                        let pdf_extraction_degraded = local_extracted_obj.text.trim().is_empty()
                            || local_extracted_obj.warnings.iter().any(|&w| {
                                matches!(
                                    normalize_warning_code(w),
                                    "pdf_extract_failed"
                                        | "pdf_extract_panicked"
                                        | "pdf_strings_fallback_used"
                                )
                            });
                        if pdf_extraction_degraded {
                            #[derive(Clone)]
                            struct PdfHtmlFallbackSpec {
                                attempt_key: &'static str,
                                kind: &'static str,
                                warning_code: &'static str,
                                url: String,
                            }
                            let mut fb_specs: Vec<PdfHtmlFallbackSpec> = Vec::new();
                            if let Some(cands) = webpipe_local::rewrite::arxiv_pdf_html_candidates(
                                local_final_url.as_str(),
                            ) {
                                if let Some(u) = cands.into_iter().next() {
                                    fb_specs.push(PdfHtmlFallbackSpec {
                                        attempt_key: "arxiv_pdf_html_fallback",
                                        kind: "arxiv_pdf_to_html_fallback",
                                        warning_code: "arxiv_pdf_fallback_to_html",
                                        url: u,
                                    });
                                }
                            }
                            if let Some(cands) =
                                webpipe_local::rewrite::openreview_pdf_api_candidates(
                                    local_final_url.as_str(),
                                )
                            {
                                if let Some(u) = cands.into_iter().next() {
                                    fb_specs.push(PdfHtmlFallbackSpec {
                                        attempt_key: "openreview_pdf_api_fallback",
                                        kind: "openreview_pdf_to_api_fallback",
                                        warning_code: "openreview_pdf_fallback_to_api",
                                        url: u,
                                    });
                                }
                            }
                            if let Some(cands) =
                                webpipe_local::rewrite::openreview_pdf_forum_candidates(
                                    local_final_url.as_str(),
                                )
                            {
                                if let Some(u) = cands.into_iter().next() {
                                    fb_specs.push(PdfHtmlFallbackSpec {
                                        attempt_key: "openreview_pdf_forum_fallback",
                                        kind: "openreview_pdf_to_forum_fallback",
                                        warning_code: "openreview_pdf_fallback_to_forum",
                                        url: u,
                                    });
                                }
                            }

                            for fb in fb_specs {
                                let fallback_url = fb.url.clone();
                                let pdf_from_url = local_final_url.clone();
                                let pdf_engine0 = local_extracted_obj.engine;
                                let (_t0, pdf_text_chars0, _clip0) =
                                    Self::truncate_to_chars(&local_extracted_obj.text, max_chars);
                                let pdf_warn0: Vec<&'static str> =
                                    local_extracted_obj.warnings.clone();

                                let t_fb0 = std::time::Instant::now();
                                let fb_req = FetchRequest {
                                    url: fallback_url.clone(),
                                    timeout_ms: Some(timeout_ms_eff),
                                    max_bytes: Some(max_bytes),
                                    headers: BTreeMap::new(),
                                    cache: FetchCachePolicy {
                                        read: cache_read,
                                        write: cache_write,
                                        ttl_s: cache_ttl_s,
                                    },
                                };
                                match self.fetcher.fetch(&fb_req).await {
                                    Ok(r2) => {
                                        let fb_final_url = r2.final_url.clone();
                                        let fb_status = r2.status;
                                        let fb_content_type = r2.content_type.clone();

                                        let fb_pdf_like =
                                            Self::content_type_is_pdf(fb_content_type.as_deref())
                                                || Self::url_looks_like_pdf(&fb_final_url)
                                                || webpipe_local::extract::bytes_look_like_pdf(
                                                    &r2.bytes,
                                                );
                                        let fb_raw_text = if fb_pdf_like {
                                            String::new()
                                        } else {
                                            r2.text_lossy()
                                        };
                                        let mut fb_extracted_obj =
                                            webpipe_local::extract::best_effort_text_from_bytes(
                                                &r2.bytes,
                                                fb_content_type.as_deref(),
                                                &fb_final_url,
                                                width,
                                                500,
                                            );
                                        let (_fb_t, fb_text_chars, _fb_clip) =
                                            Self::truncate_to_chars(
                                                &fb_extracted_obj.text,
                                                max_chars,
                                            );
                                        let fb_nonempty = fb_status < 400
                                            && (fb_text_chars >= 80
                                                || !fb_extracted_obj.text.trim().is_empty());
                                        if fb_nonempty {
                                            fb_extracted_obj.warnings.push(fb.warning_code);

                                            let mut a =
                                                attempts.as_object().cloned().unwrap_or_default();
                                            a.insert(
                                                fb.attempt_key.to_string(),
                                                serde_json::json!({
                                                    "kind": fb.kind,
                                                    "ok": true,
                                                    "from": pdf_from_url,
                                                    "to": fb_final_url,
                                                    "status": fb_status,
                                                    "elapsed_ms": t_fb0.elapsed().as_millis(),
                                                    "pdf": {
                                                        "engine": pdf_engine0,
                                                        "text_chars": pdf_text_chars0,
                                                        "warning_codes": pdf_warn0
                                                    }
                                                }),
                                            );
                                            attempts = serde_json::Value::Object(a);

                                            // Replace the local response + extraction with the HTML fallback.
                                            fetched = r2;
                                            local_final_url = fetched.final_url.clone();
                                            local_status = fetched.status;
                                            local_content_type = fetched.content_type.clone();
                                            local_bytes_len = fetched.bytes.len();
                                            local_truncated = fetched.truncated;
                                            local_source = match fetched.source {
                                                FetchSource::Cache => "cache",
                                                FetchSource::Network => "network",
                                            };
                                            local_pdf_like = fb_pdf_like;
                                            local_raw_text = fb_raw_text;
                                            local_extracted_obj = fb_extracted_obj;
                                            cache_io_timed_out = cache_io_disabled
                                                || fetched
                                                    .timings_ms
                                                    .contains_key("cache_get_timeout")
                                                || fetched
                                                    .timings_ms
                                                    .contains_key("cache_put_timeout");
                                            break;
                                        } else {
                                            let mut a =
                                                attempts.as_object().cloned().unwrap_or_default();
                                            a.insert(
                                                fb.attempt_key.to_string(),
                                                serde_json::json!({
                                                    "kind": fb.kind,
                                                    "ok": false,
                                                    "from": pdf_from_url,
                                                    "to": fb_final_url,
                                                    "status": fb_status,
                                                    "elapsed_ms": t_fb0.elapsed().as_millis(),
                                                    "error": "fallback_not_better"
                                                }),
                                            );
                                            attempts = serde_json::Value::Object(a);
                                        }
                                    }
                                    Err(e) => {
                                        let mut a =
                                            attempts.as_object().cloned().unwrap_or_default();
                                        a.insert(
                                            fb.attempt_key.to_string(),
                                            serde_json::json!({
                                                "kind": fb.kind,
                                                "ok": false,
                                                "from": pdf_from_url,
                                                "to": fallback_url,
                                                "elapsed_ms": t_fb0.elapsed().as_millis(),
                                                "error": e.to_string()
                                            }),
                                        );
                                        attempts = serde_json::Value::Object(a);
                                    }
                                }
                            }
                        }
                    }

                    // Handle client-side redirects (meta refresh / JS): enqueue the target if found.
                    if local_extracted_obj.engine == "redirect" {
                        let target = local_extracted_obj.text.trim();
                        if !target.is_empty() {
                            let next = if let Ok(u) = reqwest::Url::parse(target) {
                                Some(u)
                            } else if let Ok(base) = reqwest::Url::parse(&local_final_url) {
                                base.join(target).ok()
                            } else {
                                None
                            };
                            if let Some(u) = next {
                                if let Some(k) = canonicalize_url_no_frag(u.as_str()) {
                                    if seen_frontier.insert(k) {
                                        // Prioritize the redirect target.
                                        frontier.insert(0, u.to_string());
                                    }
                                }
                            }
                        }
                    }

                    let (_local_text, local_text_chars, _local_text_clipped) =
                        Self::truncate_to_chars(&local_extracted_obj.text, max_chars);
                    let local_empty_extraction = local_text_chars == 0 && local_bytes_len > 0;
                    let local_low_signal = !local_empty_extraction
                        && local_text_chars > 0
                        && Self::looks_like_bundle_gunk(&local_extracted_obj.text);

                    if let Some(render_tuple) = {
                        let wants_render = (local_empty_extraction
                            && render_fallback_on_empty_extraction)
                            || (local_low_signal && render_fallback_on_low_signal);
                        if !wants_render {
                            None
                        } else if matches!(privacy_mode_from_env(), PrivacyMode::Offline) {
                            local_extracted_obj
                                .warnings
                                .push("render_fallback_not_supported");
                            None
                        } else if matches!(
                            std::env::var("WEBPIPE_RENDER_DISABLE")
                                .unwrap_or_default()
                                .trim()
                                .to_ascii_lowercase()
                                .as_str(),
                            "1" | "true" | "yes" | "on"
                        ) {
                            local_extracted_obj
                                .warnings
                                .push("render_fallback_disabled");
                            None
                        } else {
                            let pm = privacy_mode_from_env();
                            let anon_proxy = anon_proxy_from_env();
                            let requires_proxy = matches!(pm, PrivacyMode::Anonymous)
                                && !is_localhost_url(fetch_url0.as_str());
                            let proxy_for_render: Option<&str> = if requires_proxy {
                                match anon_proxy.as_deref() {
                                    Some(p) => {
                                        let pl = p.trim().to_ascii_lowercase();
                                        if pl.starts_with("socks5h://") {
                                            local_extracted_obj
                                                .warnings
                                                .push("render_fallback_not_supported");
                                            None
                                        } else {
                                            Some(p)
                                        }
                                    }
                                    None => {
                                        local_extracted_obj
                                            .warnings
                                            .push("render_fallback_not_configured");
                                        None
                                    }
                                }
                            } else {
                                None
                            };

                            if requires_proxy && proxy_for_render.is_none() {
                                None
                            } else {
                                match webpipe_local::render_playwright::render_html_playwright(
                                    fetch_url0.as_str(),
                                    timeout_ms_eff,
                                    proxy_for_render,
                                )
                                .await
                                {
                                    Ok(pr) => {
                                        let html = pr.html;
                                        let bytes = html.as_bytes().to_vec();
                                        let bytes_len = bytes.len();
                                        let mut a = serde_json::Map::new();
                                        a.insert(
                                            "local".to_string(),
                                            serde_json::json!({
                                                "ok": true,
                                                "final_url": local_final_url,
                                                "status": local_status,
                                                "content_type": local_content_type,
                                                "bytes": local_bytes_len,
                                                "truncated": local_truncated,
                                                "source": local_source,
                                                "empty_extraction": local_empty_extraction,
                                                "low_signal": local_low_signal
                                            }),
                                        );
                                        if let Some(obj) = local_retry_obj.take() {
                                            a.insert("local_retry".to_string(), obj);
                                        }
                                        a.insert(
                                            "render_fallback".to_string(),
                                            serde_json::json!({
                                                "ok": true,
                                                "backend": "playwright",
                                                "mode": pr.mode,
                                                "elapsed_ms": pr.elapsed_ms,
                                                "console_error_count": pr.console_error_count,
                                                "bytes": bytes_len
                                            }),
                                        );
                                        attempts = serde_json::Value::Object(a);

                                        let mut extracted0 =
                                            webpipe_local::extract::best_effort_text_from_bytes(
                                                bytes.as_ref(),
                                                Some("text/html"),
                                                pr.final_url.as_str(),
                                                width,
                                                500,
                                            );
                                        extracted0.warnings.push(if local_empty_extraction {
                                            "render_fallback_on_empty_extraction"
                                        } else {
                                            "render_fallback_on_low_signal"
                                        });
                                        Some((
                                            html,
                                            bytes,
                                            pr.final_url,
                                            pr.status.unwrap_or(200),
                                            Some("text/html".to_string()),
                                            bytes_len,
                                            false,
                                            "network",
                                            true,
                                            false,
                                            extracted0,
                                            cache_io_timed_out,
                                        ))
                                    }
                                    Err(_) => {
                                        local_extracted_obj.warnings.push("render_fallback_failed");
                                        None
                                    }
                                }
                            }
                        }
                    } {
                        render_tuple
                    } else if (local_empty_extraction && firecrawl_fallback_on_empty_extraction)
                        || (local_low_signal && firecrawl_fallback_on_low_signal)
                    {
                        if let Some(fc) = firecrawl_fallback.as_ref() {
                            let max_age_ms = cache_ttl_s.map(|s| s.saturating_mul(1000));
                            match fc.fetch_markdown(url, timeout_ms, max_age_ms).await {
                                Ok(r) => {
                                    let md = r.markdown;
                                    let md_bytes = md.as_bytes().to_vec();
                                    let fc_bytes = Self::approx_bytes_len(&md);
                                    let mut a = serde_json::Map::new();
                                    if let Some(obj) = local_attempt_obj.take() {
                                        a.insert("local".to_string(), obj);
                                    } else {
                                        a.insert(
                                            "local".to_string(),
                                            serde_json::json!({
                                                "ok": true,
                                                "final_url": local_final_url,
                                                "status": local_status,
                                                "content_type": local_content_type,
                                                "bytes": local_bytes_len,
                                                "truncated": local_truncated,
                                                "source": local_source,
                                                "empty_extraction": local_empty_extraction,
                                                "low_signal": local_low_signal
                                            }),
                                        );
                                    }
                                    if let Some(obj) = local_retry_obj.take() {
                                        a.insert("local_retry".to_string(), obj);
                                    }
                                    a.insert(
                                        "firecrawl".to_string(),
                                        serde_json::json!({
                                            "ok": true,
                                            "elapsed_ms": r.elapsed_ms,
                                            "bytes": fc_bytes
                                        }),
                                    );
                                    attempts = serde_json::Value::Object(a);
                                    (
                                        md.clone(),
                                        md_bytes,
                                        url.clone(),
                                        200u16,
                                        Some("text/markdown".to_string()),
                                        fc_bytes,
                                        false,
                                        "network",
                                        false,
                                        true,
                                        webpipe_local::extract::ExtractedText {
                                            engine: "firecrawl",
                                            text: md,
                                            warnings: vec![if local_empty_extraction {
                                                "firecrawl_fallback_on_empty_extraction"
                                            } else {
                                                "firecrawl_fallback_on_low_signal"
                                            }],
                                        },
                                        cache_io_timed_out,
                                    )
                                }
                                Err(e) => {
                                    let mut a = serde_json::Map::new();
                                    if let Some(obj) = local_attempt_obj.take() {
                                        a.insert("local".to_string(), obj);
                                    } else {
                                        a.insert(
                                            "local".to_string(),
                                            serde_json::json!({
                                                "ok": true,
                                                "final_url": local_final_url,
                                                "status": local_status,
                                                "content_type": local_content_type,
                                                "bytes": local_bytes_len,
                                                "truncated": local_truncated,
                                                "source": local_source,
                                                "empty_extraction": local_empty_extraction,
                                                "low_signal": local_low_signal
                                            }),
                                        );
                                    }
                                    if let Some(obj) = local_retry_obj.take() {
                                        a.insert("local_retry".to_string(), obj);
                                    }
                                    a.insert(
                                        "firecrawl".to_string(),
                                        serde_json::json!({
                                            "ok": false,
                                            "error": e.to_string()
                                        }),
                                    );
                                    attempts = serde_json::Value::Object(a);
                                    let bytes = fetched.bytes;
                                    (
                                        local_raw_text,
                                        bytes,
                                        local_final_url,
                                        local_status,
                                        local_content_type,
                                        local_bytes_len,
                                        local_truncated,
                                        local_source,
                                        false,
                                        false,
                                        local_extracted_obj,
                                        cache_io_timed_out,
                                    )
                                }
                            }
                        } else {
                            let mut a = serde_json::Map::new();
                            if let Some(obj) = local_attempt_obj.take() {
                                a.insert("local".to_string(), obj);
                            } else {
                                a.insert(
                                    "local".to_string(),
                                    serde_json::json!({
                                        "ok": true,
                                        "final_url": local_final_url,
                                        "status": local_status,
                                        "content_type": local_content_type,
                                        "bytes": local_bytes_len,
                                        "truncated": local_truncated,
                                        "source": local_source,
                                        "empty_extraction": local_empty_extraction,
                                        "low_signal": local_low_signal
                                    }),
                                );
                            }
                            if let Some(obj) = local_retry_obj.take() {
                                a.insert("local_retry".to_string(), obj);
                            }
                            a.insert(
                                "firecrawl".to_string(),
                                serde_json::json!({
                                    "ok": false,
                                    "error": "not_configured"
                                }),
                            );
                            attempts = serde_json::Value::Object(a);
                            let bytes = fetched.bytes;
                            (
                                local_raw_text,
                                bytes,
                                local_final_url,
                                local_status,
                                local_content_type,
                                local_bytes_len,
                                local_truncated,
                                local_source,
                                false,
                                false,
                                local_extracted_obj,
                                cache_io_timed_out,
                            )
                        }
                    } else {
                        if local_empty_extraction {
                            let mut local_obj = serde_json::json!({
                                "ok": true,
                                "final_url": local_final_url,
                                "status": local_status,
                                "content_type": local_content_type,
                                "bytes": local_bytes_len,
                                "truncated": local_truncated,
                                "source": local_source,
                            });
                            if local_empty_extraction {
                                local_obj["empty_extraction"] = serde_json::json!(true);
                            }
                            if local_pdf_like {
                                local_obj["pdf_like"] = serde_json::json!(true);
                            }
                            attempts = serde_json::json!({ "local": local_obj });
                        }
                        let bytes = fetched.bytes;
                        (
                            local_raw_text,
                            bytes,
                            local_final_url,
                            local_status,
                            local_content_type,
                            local_bytes_len,
                            local_truncated,
                            local_source,
                            false,
                            false,
                            local_extracted_obj,
                            cache_io_timed_out,
                        )
                    }
                };
                // Offload chunking/structure parsing to the blocking pool (avoid stalling async runtime).
                let raw_bytes = std::sync::Arc::new(raw_bytes);
                let pipeline0 = {
                    let bytes = raw_bytes.clone();
                    let ct = content_type.clone();
                    let final_url2 = final_url.clone();
                    let query2 = query.clone();
                    let extract_timeout_ms = std::env::var("WEBPIPE_EXTRACT_PIPELINE_TIMEOUT_MS")
                        .ok()
                        .and_then(|s| s.trim().parse::<u64>().ok())
                        // Default: stay within the per-URL timeout, but don't preempt it too aggressively.
                        // The pipeline timeout exists to prevent hangs in blocking extraction, not to clip
                        // legitimate slow parses (notably large PDFs) when the caller already allowed more time.
                        .unwrap_or(timeout_ms_eff.saturating_sub(1_000).clamp(4_000, 45_000));
                    if extract_timeout_ms == 0 {
                        // Deterministic: 0ms means “skip extraction immediately”.
                        let cfg = webpipe_local::extract::ExtractPipelineCfg {
                            query: Some(query.as_str()),
                            width,
                            max_chars,
                            top_chunks,
                            max_chunk_chars,
                            include_structure: false,
                            max_outline_items: 0,
                            max_blocks: 0,
                            max_block_chars: 0,
                        };
                        let mut p =
                            webpipe_local::extract::extract_pipeline_from_bytes(&[], None, "", cfg);
                        p.extracted.warnings.push("extract_pipeline_timeout");
                        p
                    } else {
                        let handle = tokio::task::spawn_blocking(move || {
                            webpipe_local::extract::extract_pipeline_from_extracted(
                                &bytes,
                                ct.as_deref(),
                                final_url2.as_str(),
                                extracted_obj,
                                webpipe_local::extract::ExtractPipelineCfg {
                                    query: Some(query2.as_str()),
                                    width,
                                    max_chars,
                                    top_chunks,
                                    max_chunk_chars,
                                    include_structure,
                                    max_outline_items,
                                    max_blocks,
                                    max_block_chars,
                                },
                            )
                        });
                        match tokio::time::timeout(
                            std::time::Duration::from_millis(extract_timeout_ms),
                            handle,
                        )
                        .await
                        {
                            Ok(join) => join.map_err(|e| {
                                McpError::internal_error(
                                    format!("extract pipeline join failed: {e}"),
                                    None,
                                )
                            })?,
                            Err(_) => {
                                // Fail open: return a minimal empty pipeline rather than hanging.
                                let cfg = webpipe_local::extract::ExtractPipelineCfg {
                                    query: Some(query.as_str()),
                                    width,
                                    max_chars,
                                    top_chunks,
                                    max_chunk_chars,
                                    include_structure: false,
                                    max_outline_items: 0,
                                    max_blocks: 0,
                                    max_block_chars: 0,
                                };
                                let mut p = webpipe_local::extract::extract_pipeline_from_bytes(
                                    &[],
                                    None,
                                    "",
                                    cfg,
                                );
                                p.extracted.warnings.push("extract_pipeline_timeout");
                                p
                            }
                        }
                    }
                };
                #[cfg(feature = "vision-gemini")]
                let mut pipeline = pipeline0;
                #[cfg(not(feature = "vision-gemini"))]
                let pipeline = pipeline0;

                // Opportunistic multimodal for web_search_extract results: only when
                // - feature enabled
                // - no_network is false
                // - local extraction is empty
                // - bytes look like an image
                // - GEMINI key present
                #[cfg(feature = "vision-gemini")]
                {
                    let vision_mode = webpipe_local::vision_gemini::gemini_enabled_mode_from_env();
                    let is_image_like = content_type
                        .as_deref()
                        .unwrap_or("")
                        .to_ascii_lowercase()
                        .starts_with("image/")
                        || webpipe_local::extract::bytes_look_like_image(&raw_bytes);
                    if !no_network
                        && vision_mode != "off"
                        && is_image_like
                        && pipeline.text_chars == 0
                        && !raw_bytes.is_empty()
                        && webpipe_local::vision_gemini::gemini_api_key_from_env().is_some()
                    {
                        let mime0 = content_type
                            .as_deref()
                            .unwrap_or("application/octet-stream");
                        let mime = mime0.split(';').next().unwrap_or(mime0).trim();
                        match webpipe_local::vision_gemini::gemini_image_to_text(
                            self.http.clone(),
                            &raw_bytes,
                            mime,
                        )
                        .await
                        {
                            Ok((_model_id, text)) => {
                                let ex = webpipe_local::extract::ExtractedText {
                                    engine: "gemini_vision",
                                    text,
                                    warnings: vec!["gemini_used"],
                                };
                                let bytes = raw_bytes.clone();
                                let ct = content_type.clone();
                                let final_url2 = final_url.clone();
                                let query2 = query.clone();
                                match tokio::task::spawn_blocking(move || {
                                    webpipe_local::extract::extract_pipeline_from_extracted(
                                        &bytes,
                                        ct.as_deref(),
                                        final_url2.as_str(),
                                        ex,
                                        webpipe_local::extract::ExtractPipelineCfg {
                                            query: Some(query2.as_str()),
                                            width,
                                            max_chars,
                                            top_chunks,
                                            max_chunk_chars,
                                            include_structure,
                                            max_outline_items,
                                            max_blocks,
                                            max_block_chars,
                                        },
                                    )
                                })
                                .await
                                {
                                    Ok(p2) => pipeline = p2,
                                    Err(_) => {
                                        pipeline.extracted.warnings.push("gemini_pipeline_failed")
                                    }
                                }
                            }
                            Err(code) => {
                                if vision_mode == "strict" {
                                    pipeline.extracted.warnings.push(code);
                                } else {
                                    pipeline.extracted.warnings.push("gemini_failed");
                                }
                            }
                        }
                    }
                }
                let extracted_obj = pipeline.extracted;
                let text = extracted_obj.text.clone();
                let text_chars = pipeline.text_chars;
                let text_clipped = pipeline.text_truncated;
                let structure_opt = pipeline.structure;
                let chunks0 = pipeline.chunks;
                let empty_extraction = text_chars == 0 && bytes_len > 0;
                let is_pdf_like = Self::content_type_is_pdf(content_type.as_deref())
                    || Self::url_looks_like_pdf(final_url.as_str())
                    || webpipe_local::extract::bytes_look_like_pdf(&raw_bytes)
                    || extracted_obj.engine.starts_with("pdf-");

                let mut links_timed_out = false;
                let links = if include_links
                    && fetch_backend == "local"
                    && !is_pdf_like
                    && !used_firecrawl_fallback
                    && !used_firecrawl_agentic
                {
                    // Link extraction should be based on the raw document (not the chosen text-extraction
                    // engine). Otherwise, pages that select `html_main`/`html_hint` would incorrectly show
                    // `links: []` even though the HTML contains lots of links.
                    //
                    // Keep this conservative: only attempt link extraction for "text-like" content.
                    let ct0 = content_type
                        .as_deref()
                        .unwrap_or("")
                        .split(';')
                        .next()
                        .unwrap_or("")
                        .trim()
                        .to_ascii_lowercase();
                    let engine0: &str = extracted_obj.engine;
                    let text_like = ct0.starts_with("text/")
                        || engine0.starts_with("html")
                        || engine0 == "markdown";
                    if text_like {
                        let raw_text2 = raw_text.clone();
                        let base_url = final_url.clone();
                        let links_timeout_ms = std::env::var("WEBPIPE_LINKS_TIMEOUT_MS")
                            .ok()
                            .and_then(|s| s.trim().parse::<u64>().ok())
                            .unwrap_or(1_500);
                        if links_timeout_ms == 0 {
                            links_timed_out = true;
                            Vec::new()
                        } else {
                            let handle = tokio::task::spawn_blocking(move || {
                                webpipe_local::links::extract_links(
                                    &raw_text2,
                                    Some(base_url.as_str()),
                                    max_links,
                                )
                            });
                            match tokio::time::timeout(
                                std::time::Duration::from_millis(links_timeout_ms),
                                handle,
                            )
                            .await
                            {
                                Ok(join) => join.unwrap_or_else(|_| Vec::new()),
                                Err(_) => {
                                    links_timed_out = true;
                                    Vec::new()
                                }
                            }
                        }
                    } else {
                        Vec::new()
                    }
                } else {
                    Vec::new()
                };

                let mut warnings: Vec<&'static str> = Vec::new();
                if truncated {
                    warnings.push("body_truncated_by_max_bytes");
                }
                let cache_io_disabled = std::env::var("WEBPIPE_CACHE_IO_TIMEOUT_MS")
                    .ok()
                    .and_then(|s| s.trim().parse::<u64>().ok())
                    .unwrap_or(2_000)
                    == 0;
                if cache_io_timed_out || cache_io_disabled {
                    warnings.push("cache_io_timeout");
                }
                let status_bad = status >= 400;
                if status_bad {
                    warnings.push("http_status_error");
                }
                if status == 429 {
                    warnings.push("http_rate_limited");
                }
                if let Some(o) = attempts.as_object() {
                    if let Some(lr) = o.get("local_retry") {
                        if lr.get("ok").and_then(|v| v.as_bool()) == Some(true) {
                            warnings.push("retried_due_to_truncation");
                        } else {
                            warnings.push("truncation_retry_failed");
                        }
                    }
                }
                if text_clipped {
                    warnings.push("text_truncated_by_max_chars");
                }
                if empty_extraction {
                    warnings.push("empty_extraction");
                }
                if used_firecrawl_agentic {
                    warnings.push("firecrawl_agentic");
                }
                for w in &extracted_obj.warnings {
                    warnings.push(*w);
                }
                if include_links
                    && (fetch_backend == "firecrawl"
                        || used_firecrawl_fallback
                        || used_firecrawl_agentic)
                {
                    warnings.push("links_unavailable_for_firecrawl");
                }
                if include_links && is_pdf_like {
                    warnings.push("links_unavailable_for_pdf");
                }
                if include_links && links_timed_out {
                    warnings.push("links_timeout");
                }
                if no_network && !url_is_localhost(url) {
                    warnings.push("cache_only");
                }
                // Surface “JS wall / auth wall” in a stable way so the agent can choose a different URL.
                // Use extracted text (best effort) + title (if available) + status.
                let title_opt = structure_opt
                    .as_ref()
                    .and_then(|s| s.title.as_ref())
                    .map(|s| s.as_str());
                if looks_like_js_challenge(status, &extracted_obj.text, title_opt) {
                    warnings.push("blocked_by_js_challenge");
                } else if looks_like_silent_throttle(status, &extracted_obj.text, title_opt) {
                    warnings.push("silently_throttled");
                }
                if extracted_obj.engine == "html_main"
                    && bytes_len >= 50_000
                    && structure_opt
                        .as_ref()
                        .is_some_and(structure_looks_like_ui_shell)
                    && extracted_text_looks_like_nav_shell(&extracted_obj.text)
                {
                    warnings.push("main_content_low_signal");
                }

                // Reduce “JS bundle gunk” in returned chunks (common on Next.js/SPA docs).
                // This is display-only: raw_text is still used for internal discovery.
                let (chunks, filtered_low_signal) = Self::filter_low_signal_chunks(chunks0);
                if filtered_low_signal {
                    warnings.push("chunks_filtered_low_signal");
                }
                // Explicitly mark “all chunks look low-signal” even if we had to fail-open.
                // This makes it easier for clients/agents to decide to switch backends/URLs.
                let all_low_signal =
                    !chunks.is_empty() && chunks.iter().all(Self::chunk_is_low_signal);
                if all_low_signal {
                    warnings.push("all_chunks_low_signal");
                }

                let page_signal = chunks.iter().map(|c| c.score).max().unwrap_or(0);
                // Stuck detection: repeated low-signal pages should trigger an extra search round (if allowed),
                // rather than burning the remaining URL budget on adjacent low-quality pages.
                if page_signal == 0 || warnings.contains(&"empty_extraction") {
                    stuck_streak = stuck_streak.saturating_add(1);
                } else {
                    stuck_streak = 0;
                }
                if warnings.contains(&"blocked_by_js_challenge") {
                    stuck_streak = stuck_streak.saturating_add(1);
                }
                if warnings.contains(&"silently_throttled") {
                    stuck_streak = stuck_streak.saturating_add(1);
                }

                // Per-provider “junk” feedback (best-effort, deterministic):
                // - hard junk: blocked_by_js_challenge
                // - soft junk: empty_extraction / main_content_low_signal / chunks_filtered_low_signal
                total_urls_ok = total_urls_ok.saturating_add(1);
                let url_blocked = warnings
                    .iter()
                    .any(|w| normalize_warning_code(w) == "blocked_by_js_challenge");
                let url_soft = warnings.iter().any(|w| {
                    matches!(
                        normalize_warning_code(w),
                        "empty_extraction"
                            | "main_content_low_signal"
                            | "chunks_filtered_low_signal"
                    )
                });
                if url_blocked {
                    hard_junk_urls = hard_junk_urls.saturating_add(1);
                } else if url_soft {
                    soft_junk_urls = soft_junk_urls.saturating_add(1);
                }

                // Collect chunk candidates with per-URL provenance signals for selection.
                let warning_penalty = Self::warning_penalty(&warnings);
                let cache_hit = fetch_source == "cache";
                if !status_bad {
                    for c in &chunks {
                        all_chunks.push(ChunkCandidate {
                            url: url.clone(),
                            score: c.score,
                            start_char: c.start_char,
                            end_char: c.end_char,
                            text: c.text.clone(),
                            warning_penalty,
                            cache_hit,
                        });
                    }
                }

                // Always include a small preview so query-mode returns “actual content”
                // without requiring include_text=true (which can be large).
                //
                // Keep this single-line and bounded to stay Cursor-friendly.
                let cap = 700usize;
                let mut text_preview_source = "prefix";
                let mut preview_base: &str = &text;
                // In query-mode, prefer previewing the best matching chunk rather than the
                // beginning of the page (which is often nav/chrome).
                if !query.trim().is_empty() {
                    if let Some(best) = chunks.iter().find(|c| !c.text.trim().is_empty()) {
                        // `best_chunks_default` uses score=1. Treat that as a fallback indicator so
                        // consumers can distinguish “query matched” vs “contentful preview”.
                        text_preview_source = if best.score >= 2 {
                            "top_chunk"
                        } else {
                            "top_chunk_fallback"
                        };
                        preview_base = best.text.as_str();
                    }
                }

                let mut text_preview: String = preview_base.chars().take(cap).collect();
                text_preview = text_preview
                    .split_whitespace()
                    .collect::<Vec<_>>()
                    .join(" ");
                let text_preview_truncated = preview_base.chars().count() > cap;

                let mut one = serde_json::json!({
                    "url": url,
                    "ok": true,
                    "fetch_backend": if used_firecrawl_fallback || used_firecrawl_agentic {
                        "firecrawl"
                    } else if used_render_fallback {
                        "render"
                    } else {
                        fetch_backend.as_str()
                    },
                    "final_url": final_url,
                    "status": status,
                    "content_type": content_type,
                    "bytes": bytes_len,
                    "truncated": truncated,
                    "fetch_source": fetch_source,
                    // Always include attempts (null or object) so compact mode stays diagnosable.
                    "attempts": attempts,
                    "extract": {
                        "engine": extracted_obj.engine,
                        "width": width,
                        "max_chars": max_chars,
                        "text_chars": text_chars,
                        "text_truncated": text_clipped,
                        "text_preview": text_preview,
                        "text_preview_source": text_preview_source,
                        "text_preview_truncated": text_preview_truncated,
                        "top_chunks": top_chunks,
                        "max_chunk_chars": max_chunk_chars,
                        "chunks": chunks
                    },
                    "elapsed_ms": per_t0.elapsed().as_millis()
                });
                // Deterministic quality scorecard (tail-risk detector).
                // Additive: safe for existing consumers.
                let quality = Self::quality_scorecard(
                    Some(query.as_str()),
                    &text,
                    status,
                    extracted_obj.engine,
                    &warnings,
                );
                one["extract"]["quality"] = quality;
                if !warnings.is_empty() {
                    one["warnings"] = serde_json::json!(warnings);
                    let codes = warning_codes_from(&warnings);
                    one["warning_codes"] = serde_json::json!(codes.clone());
                    one["warning_hints"] = warning_hints_from(&codes);
                    self.stats_record_warnings(&warnings);
                }
                if include_text {
                    one["extract"]["text"] = serde_json::json!(text);
                }
                if include_links {
                    if used_firecrawl_fallback {
                        one["extract"]["links"] = serde_json::json!([]);
                    } else {
                        one["extract"]["links"] = serde_json::json!(links);
                    }
                }
                if include_structure {
                    if let Some(s) = structure_opt.as_ref() {
                        one["extract"]["structure"] = serde_json::json!(s);
                    }
                }
                if semantic_rerank && !query.trim().is_empty() {
                    let cands: Vec<(usize, usize, String)> = chunks
                        .iter()
                        .map(|c| (c.start_char, c.end_char, c.text.clone()))
                        .collect();
                    let sem = self
                        .semantic_rerank_chunks_best(&query, &cands, semantic_top_k)
                        .await;
                    one["extract"]["semantic"] = serde_json::json!(sem);
                }
                if compact {
                    // Reduce duplication: `top_chunks` already carries the evidence text,
                    // so per-URL compact mode drops `extract.chunks` (and some knob echoes).
                    if let Some(ex) = one.get_mut("extract").and_then(|v| v.as_object_mut()) {
                        ex.remove("chunks");
                        ex.remove("width");
                        ex.remove("max_chars");
                        ex.remove("top_chunks");
                        ex.remove("max_chunk_chars");
                    }

                    // Keep only stable identifiers + extract payload + warnings.
                    let mut out = serde_json::Map::new();
                    for k in [
                        "url",
                        "final_url",
                        "ok",
                        "fetch_backend",
                        "attempts",
                        "status",
                        "content_type",
                        "bytes",
                        "elapsed_ms",
                        "warnings",
                        "warning_codes",
                        "warning_hints",
                        "extract",
                    ] {
                        if let Some(v) = one.get(k) {
                            out.insert(k.to_string(), v.clone());
                        }
                    }
                    one = serde_json::Value::Object(out);
                }
                per_url.push(one);
                // Internal discovery: even if include_links=false, expand frontier when agentic.
                // This should work for:
                // - HTML extraction engines (html2text/html_main/html_hint)
                // - Firecrawl fallback (markdown) and markdown-ish content
                // Cursor-first heuristic:
                // - When the user provided multiple `urls=[...]`, do NOT expand beyond those URLs;
                //   the user is already curating sources, and link-walking can drift to login/CTA pages.
                // - When `urls` were not provided (search-mode), or exactly one URL was provided
                //   (portal->article), expansion is allowed (bounded by frontier_max).
                let allow_agentic_expansion = agentic
                    && fetch_backend == "local"
                    && !is_pdf_like
                    && (!user_urls_provided || user_urls_len <= 1);
                if allow_agentic_expansion {
                    let ct0 = content_type
                        .as_deref()
                        .unwrap_or("")
                        .split(';')
                        .next()
                        .unwrap_or("")
                        .trim()
                        .to_ascii_lowercase();
                    let markdown_like = used_firecrawl_fallback
                        || used_firecrawl_agentic
                        || extracted_obj.engine == "markdown"
                        || ct0 == "text/markdown"
                        || ct0 == "text/x-markdown";
                    let links_timeout_ms = std::env::var("WEBPIPE_LINKS_TIMEOUT_MS")
                        .ok()
                        .and_then(|s| s.trim().parse::<u64>().ok())
                        .unwrap_or(1_500);
                    let mut links_timed_out = false;
                    let discovered = if links_timeout_ms == 0 {
                        links_timed_out = true;
                        Vec::new()
                    } else {
                        // Offload parsing to the blocking pool; link extraction can get expensive on some pages.
                        let raw = raw_text.clone();
                        let base_url = final_url.clone();
                        let markdown_like2 = markdown_like;
                        let max_links2 = max_links.min(50);
                        let handle = tokio::task::spawn_blocking(move || {
                            let base = Some(base_url.as_str());
                            if markdown_like2 {
                                webpipe_local::links::extract_markdown_link_candidates(
                                    &raw, base, max_links2,
                                )
                            } else {
                                webpipe_local::links::extract_link_candidates(
                                    &raw, base, max_links2,
                                )
                            }
                        });
                        match tokio::time::timeout(
                            std::time::Duration::from_millis(links_timeout_ms),
                            handle,
                        )
                        .await
                        {
                            Ok(join) => join.unwrap_or_else(|_| Vec::new()),
                            Err(_) => {
                                links_timed_out = true;
                                Vec::new()
                            }
                        }
                    };
                    if links_timed_out {
                        warnings.push("links_timeout");
                    }
                    // Propagate content relevance to discovered links. This is the “loop”:
                    // good pages contribute candidate URLs more strongly than weak pages.
                    let parent_relevance = chunks.iter().map(|c| c.score).max().unwrap_or(0);
                    let mut added = 0usize;
                    for cand in discovered {
                        let u = cand.url;
                        if frontier.len() >= frontier_max {
                            break;
                        }
                        // Drop obvious auth/challenge/tracking/homepage URLs from discovery.
                        if url_looks_like_auth_or_challenge(&u)
                            || url_looks_like_promo_or_tracking(&u)
                            || url_looks_like_low_value_homepage(&u)
                        {
                            continue;
                        }
                        if !url_allowed_by_domain_filters(&u, &domains_allow, &domains_deny) {
                            continue;
                        }
                        if let Some(ref hs) = url_scope_hosts {
                            // Only allow discovery within the user's provided host set.
                            let Ok(p) = reqwest::Url::parse(u.trim()) else {
                                continue;
                            };
                            let Some(h) = p.host_str() else {
                                continue;
                            };
                            if !hs.contains(&h.to_ascii_lowercase()) {
                                continue;
                            }
                        }
                        if let Some(k) = canonicalize_url_no_frag(&u) {
                            if seen_frontier.insert(k.clone()) {
                                // Record/update prior relevance for this discovered URL.
                                let entry = priors.entry(k.clone()).or_insert(0);
                                if !cand.text.trim().is_empty() {
                                    let (lbl, _n, _clip) = Self::truncate_to_chars(&cand.text, 120);
                                    let replace = match link_labels.get(&k) {
                                        None => true,
                                        Some(existing) => lbl.len() > existing.len(),
                                    };
                                    if replace {
                                        link_labels.insert(k.clone(), lbl);
                                    }
                                }
                                // Use anchor text when available; it's often more semantic than the URL.
                                let anchor_scrub = textprep::scrub(&cand.text);
                                let url_scrub = textprep::scrub(&u);
                                let mut hits = 0u64;
                                for t in &q_toks {
                                    if anchor_scrub.contains(t.as_str()) {
                                        hits += 2; // anchor text counts more than URL string
                                    } else if url_scrub.contains(t.as_str()) {
                                        hits += 1;
                                    }
                                }
                                // Generic bias toward primary artifacts: PDF links often contain the
                                // full document even when the anchor text is just "PDF" and doesn't
                                // match the query tokens (e.g., arXiv).
                                let pdf_like_link = {
                                    let a_lc = cand.text.to_ascii_lowercase();
                                    a_lc.split_whitespace().any(|w| w == "pdf")
                                        || Self::url_looks_like_pdf(&u)
                                };
                                if pdf_like_link {
                                    hits = hits.max(1);
                                }
                                // IMPORTANT: allow “hub page → relevant link” hops even when the hub page
                                // itself has no query-matching chunks (parent_relevance==0). In that case,
                                // we still want anchor/URL token matches to influence the next pick.
                                let base_parent = if parent_relevance == 0 {
                                    1
                                } else {
                                    parent_relevance
                                };
                                let prior_add = base_parent.saturating_mul(hits.min(10));
                                *entry = (*entry).max(prior_add);
                                frontier.push(u);
                                added += 1;
                            }
                        }
                    }
                    if let Some(last) = agentic_trace.last_mut() {
                        if let Some(obj) = last.as_object_mut() {
                            obj.insert("frontier_added".to_string(), serde_json::json!(added));
                            obj.insert(
                                "frontier_len_after".to_string(),
                                serde_json::json!(frontier.len()),
                            );
                        }
                    }
                }

                // If we're stuck, proactively broaden the frontier via another search round.
                if agentic
                    && !no_network
                    && !query.trim().is_empty()
                    && stuck_streak >= 2
                    && search_rounds < max_search_rounds
                {
                    frontier.clear();
                    agentic_trace.push(serde_json::json!({
                        "stuck": true,
                        "stuck_streak": stuck_streak,
                        "action": "search_more",
                    }));
                    stuck_streak = 0;
                }
            }

            // Filter out redirect sources if their target was also fetched.
            // This reduces noise in the output (e.g. "Click here to be redirected" pages).
            let mut successful_targets = std::collections::HashSet::new();
            for r in &per_url {
                if r.get("ok").and_then(|v| v.as_bool()) == Some(true) {
                    if let Some(u) = r.get("final_url").and_then(|v| v.as_str()) {
                        successful_targets.insert(u.to_string());
                    }
                }
            }

            let mut redirect_urls_to_drop = std::collections::HashSet::new();
            for r in &per_url {
                let is_redirect_engine = r
                    .get("extract")
                    .and_then(|e| e.get("engine"))
                    .and_then(|s| s.as_str())
                    == Some("redirect");

                if is_redirect_engine {
                    if let Some(target) = r
                        .get("extract")
                        .and_then(|e| e.get("text"))
                        .and_then(|t| t.as_str())
                    {
                        // Check exact match or relative resolution
                        let hit = successful_targets.contains(target);
                        let hit_abs = if !hit {
                            r.get("final_url")
                                .and_then(|b| b.as_str())
                                .and_then(|b| reqwest::Url::parse(b).ok())
                                .and_then(|base| base.join(target).ok())
                                .map(|u| successful_targets.contains(u.as_str()))
                                .unwrap_or(false)
                        } else {
                            false
                        };

                        if hit || hit_abs {
                            if let Some(u) = r.get("url").and_then(|v| v.as_str()) {
                                redirect_urls_to_drop.insert(u.to_string());
                            }
                        }
                    }
                }
            }

            if !redirect_urls_to_drop.is_empty() {
                per_url.retain(|r| {
                    let u = r.get("url").and_then(|v| v.as_str()).unwrap_or("");
                    !redirect_urls_to_drop.contains(u)
                });
                all_chunks.retain(|c| !redirect_urls_to_drop.contains(&c.url));
            }

            let selected = Self::select_top_chunks(all_chunks, top_chunks, selection_mode.as_str());
            let max_selected_score = selected.iter().map(|c| c.score).max().unwrap_or(0);
            let top_chunks_out: Vec<serde_json::Value> = selected
                .into_iter()
                .map(|c| {
                    serde_json::json!({
                        "url": c.url,
                        "score": c.score,
                        "start_char": c.start_char,
                        "end_char": c.end_char,
                        "text": c.text
                    })
                })
                .collect();

            let mode = if search_steps.is_empty() {
                "urls"
            } else {
                "search"
            };
            let provider_out = if mode == "urls" {
                "urls".to_string()
            } else {
                requested_provider.clone()
            };

            let mut payload = serde_json::json!({
                "ok": true,
                "mode": mode,
                "provider": provider_out,
                "query": query,
                "request": {
                    "provider": requested_provider,
                        "auto_mode": requested_auto_mode,
                    "selection_mode": selection_mode,
                    "exploration": exploration,
                    "fetch_backend": fetch_backend,
                    "max_results": max_results,
                    "max_urls": max_urls,
                    "max_parallel_urls": max_parallel_urls,
                    "url_selection_mode": url_selection_mode,
                    "timeout_ms": timeout_ms,
                    "deadline_ms": deadline_ms,
                    "max_bytes": max_bytes,
                    "retry_on_truncation": retry_on_truncation,
                    "truncation_retry_max_bytes": truncation_retry_max_bytes,
                    "width": width,
                    "max_chars": max_chars,
                    "top_chunks": top_chunks,
                    "max_chunk_chars": max_chunk_chars,
                    "include_links": include_links,
                    "max_links": max_links,
                    "include_text": include_text,
                    "domains_allow": domains_allow,
                    "domains_deny": domains_deny,
                    "agentic": agentic,
                    "agentic_max_search_rounds": max_search_rounds,
                    "agentic_frontier_max": frontier_max,
                    "planner_max_calls": planner_max_calls,
                        "no_network": no_network,
                    "firecrawl_fallback_on_empty_extraction": firecrawl_fallback_on_empty_extraction,
                    "firecrawl_fallback_on_low_signal": firecrawl_fallback_on_low_signal,
                    "render_fallback_on_empty_extraction": render_fallback_on_empty_extraction,
                    "render_fallback_on_low_signal": render_fallback_on_low_signal,
                    "cache": { "read": cache_read_effective, "write": cache_write_effective, "ttl_s": cache_ttl_s },
                    "compact": compact
                },
                "url_count_in": urls.len(),
                "url_count_used": per_url.len(),
                "results": per_url,
                "top_chunks": top_chunks_out
            });
            if !search_steps.is_empty() {
                payload["search"] = serde_json::json!({ "steps": search_steps });
            }
            if let Some(ref k) = query_key {
                payload["query_key"] = serde_json::json!(k);
            }
            // If fetch_backend="render" is requested but nothing succeeded, fail closed with a
            // clear top-level error. This avoids confusing "ok=true but empty results" outcomes.
            if fetch_backend == "render"
                && per_url
                    .iter()
                    .all(|v| v.get("ok").and_then(|b| b.as_bool()) != Some(true))
            {
                payload["ok"] = serde_json::json!(false);
                payload["error"] = error_obj(
                    ErrorCode::NotConfigured,
                    "render backend is not configured (no URLs succeeded)",
                    "Install Playwright (Node) + browsers, or set fetch_backend=\"local\" for non-JS pages."
                );
            }
            // E2E/debugging: always surface which provider actually supplied results, even when
            // the requested provider was "auto" or "merge".
            //
            // For urls-mode (no search step), we leave this unset (or set to null) rather than
            // inventing a synthetic provider name.
            if let Some(p) = search_backend_provider.as_deref() {
                payload["backend_provider"] = serde_json::json!(p);
            } else {
                payload["backend_provider"] = serde_json::Value::Null;
            }
            if agentic {
                if compact {
                    let mut stuck_events = 0usize;
                    let mut frontier_added_total = 0usize;
                    for t in &agentic_trace {
                        if t.get("stuck").and_then(|v| v.as_bool()) == Some(true) {
                            stuck_events = stuck_events.saturating_add(1);
                        }
                        frontier_added_total = frontier_added_total.saturating_add(
                            t.get("frontier_added")
                                .and_then(|v| v.as_u64())
                                .unwrap_or(0) as usize,
                        );
                    }
                    payload["agentic"] = serde_json::json!({
                        "enabled": true,
                        "trace_len": agentic_trace.len(),
                        "search_rounds": search_rounds,
                        "stuck_events": stuck_events,
                        "frontier_added_total": frontier_added_total,
                        "frontier_len_final": frontier.len(),
                        "urls_fetched": per_url.len(),
                    });
                } else {
                    payload["agentic"] = serde_json::json!({
                        "enabled": true,
                        "trace": agentic_trace
                    });
                }
            }

            // Tool-level warning: none of the selected chunks appears to match the query (beyond
            // the query-less fallback chunks, which carry score=1).
            //
            // This most often means:
            // - URLs don't actually contain the query, or
            // - the relevant section lies beyond max_chars, or
            // - the page is still dominated by nav/app-shell (try render/firecrawl).
            if !query.trim().is_empty() && max_selected_score <= 1 {
                payload["warning_codes"] = serde_json::json!(["no_query_overlap_any_url"]);
                payload["warning_hints"] = warning_hints_from(&["no_query_overlap_any_url"]);
            }

            if deadline_exceeded_partial {
                // Tool-level warning (not tied to a specific URL). Keep it compact; merge logic later
                // will combine this with per-URL warnings.
                // Merge with any existing tool-level warning codes.
                let mut merged = std::collections::BTreeSet::<String>::new();
                if let Some(arr) = payload.get("warning_codes").and_then(|v| v.as_array()) {
                    for x in arr.iter().filter_map(|v| v.as_str()) {
                        let s = normalize_warning_code(x).to_string();
                        if !s.is_empty() {
                            merged.insert(s);
                        }
                    }
                }
                merged.insert("deadline_exceeded_partial".to_string());
                let merged_vec = merged.into_iter().collect::<Vec<_>>();
                payload["warning_codes"] = serde_json::json!(merged_vec.clone());
                let codes_ref = merged_vec
                    .iter()
                    .take(12)
                    .map(|s| s.as_str())
                    .collect::<Vec<_>>();
                payload["warning_hints"] = warning_hints_from(&codes_ref);
            }

            if let Some(p) = search_backend_provider.as_deref() {
                // Only apply this feedback to single-provider choices (not "merge").
                if (p == "brave" || p == "tavily") && total_urls_ok > 0 {
                    let hard_junk = hard_junk_urls > 0;
                    let soft_junk = Self::compute_search_junk_label(
                        hard_junk_urls,
                        soft_junk_urls,
                        total_urls_ok,
                    );
                    let junk = hard_junk || soft_junk;
                    self.stats_set_last_search_outcome_junk_level_qk(
                        p,
                        junk,
                        hard_junk,
                        query_key.as_deref(),
                    );
                }
            }

            // Aggregate per-source warning codes to a small top-level summary so agents/UI
            // don’t need to scan the full results array to notice that something went wrong.
            //
            // Keep this bounded and stable: add `warning_codes` + `warning_hints` + `warning_summary`.
            let mut warn_counts = std::collections::BTreeMap::<String, u64>::new();
            for r in &per_url {
                if let Some(codes) = r.get("warning_codes").and_then(|v| v.as_array()) {
                    for c in codes.iter().filter_map(|v| v.as_str()) {
                        let cc = normalize_warning_code(c).to_string();
                        if cc.is_empty() {
                            continue;
                        }
                        *warn_counts.entry(cc).or_insert(0) += 1;
                    }
                }
                if let Some(ws) = r.get("warnings").and_then(|v| v.as_array()) {
                    for w in ws.iter().filter_map(|v| v.as_str()) {
                        let cc = normalize_warning_code(w).to_string();
                        if cc.is_empty() {
                            continue;
                        }
                        *warn_counts.entry(cc).or_insert(0) += 1;
                    }
                }
            }
            if !warn_counts.is_empty() {
                fn is_actionable_warning_code(code: &str) -> bool {
                    match code {
                        // Very common, usually not worth surfacing as a top-level "warning".
                        "boilerplate_reduced" => false,
                        // Informational / workflow notes (still available per-source + in warning_summary).
                        "cache_doc_reused" => false,
                        "cache_only" => false,
                        "tavily_used" => false,
                        _ => true,
                    }
                }
                fn severity_rank(code: &str) -> u8 {
                    match code {
                        "blocked_by_js_challenge" => 0,
                        "render_fallback_failed" => 1,
                        "render_fallback_not_supported" => 1,
                        "render_fallback_not_configured" => 1,
                        "empty_extraction" => 2,
                        "all_chunks_low_signal" => 3,
                        "main_content_low_signal" => 4,
                        "chunks_filtered_low_signal" => 5,
                        "body_truncated_by_max_bytes" => 6,
                        _ => 9,
                    }
                }

                let total_events: u64 = warn_counts.values().copied().sum();
                let mut codes = warn_counts.keys().cloned().collect::<Vec<_>>();
                codes.sort_by(|a, b| {
                    let ra = severity_rank(a);
                    let rb = severity_rank(b);
                    if ra != rb {
                        return ra.cmp(&rb);
                    }
                    let ca = warn_counts.get(a).copied().unwrap_or(0);
                    let cb = warn_counts.get(b).copied().unwrap_or(0);
                    // Higher count first for same severity.
                    cb.cmp(&ca).then_with(|| a.cmp(b))
                });

                let actionable_codes = codes
                    .iter()
                    .filter(|c| is_actionable_warning_code(c))
                    .cloned()
                    .collect::<Vec<_>>();

                // Merge into any existing top-level warning_codes (e.g. tool-level warnings).
                let mut merged = std::collections::BTreeSet::<String>::new();
                if let Some(arr) = payload.get("warning_codes").and_then(|v| v.as_array()) {
                    for x in arr.iter().filter_map(|v| v.as_str()) {
                        let s = normalize_warning_code(x).to_string();
                        if !s.is_empty() {
                            merged.insert(s);
                        }
                    }
                }
                for c in &actionable_codes {
                    merged.insert(c.clone());
                }

                let merged_vec = merged.into_iter().collect::<Vec<_>>();
                payload["warning_codes"] = serde_json::json!(merged_vec.clone());

                // Hints should cover merged codes (including tool-level warnings), but stay bounded.
                let codes_ref = merged_vec
                    .iter()
                    .filter(|c| is_actionable_warning_code(c.as_str()))
                    .take(12)
                    .map(|s| s.as_str())
                    .collect::<Vec<_>>();
                payload["warning_hints"] = warning_hints_from(&codes_ref);

                let actionable_counts = warn_counts
                    .iter()
                    .filter(|(k, _)| is_actionable_warning_code(k))
                    .map(|(k, v)| (k.clone(), *v))
                    .collect::<std::collections::BTreeMap<_, _>>();

                payload["warning_summary"] = serde_json::json!({
                    "total": total_events,
                    "unique": warn_counts.len(),
                    "counts": warn_counts,
                    "actionable_unique": actionable_counts.len(),
                    "actionable_counts": actionable_counts,
                });
            }
            add_envelope_fields(&mut payload, "web_search_extract", t0.elapsed().as_millis());
            if minimal_output {
                strip_minimal_output(&mut payload);
            }
            let md = web_search_extract_markdown(&payload);
            Ok(tool_result_markdown_with_json(payload, md))
        }

        #[tool(
            description = "Best for: offline / deterministic retrieval from previously fetched pages — no network calls. Use this when you want reproducible results from your local cache corpus. Not this when you need fresh network data — use search_evidence instead. Output: top_chunks[] from cached documents matching the query.",
            input_schema = Arc::new(tool_input_schema_draft07::<WebCacheSearchExtractArgs>()),
            annotations(
                title = "Cache search+extract",
                read_only_hint = true,
                open_world_hint = false
            )
        )]
        async fn web_cache_search_extract(
            &self,
            params: Parameters<Option<WebCacheSearchExtractArgs>>,
        ) -> Result<CallToolResult, McpError> {
            let args = params.0.unwrap_or_default();
            self.stats_inc_tool("web_cache_search_extract");
            let t0 = std::time::Instant::now();

            let query = args.query.clone().unwrap_or_default().trim().to_string();
            if query.is_empty() {
                let mut payload = serde_json::json!({
                    "ok": false,
                    "query": args.query.unwrap_or_default(),
                    "error": error_obj(ErrorCode::InvalidParams, "query must be a non-empty string", "Pass a non-empty `query`."),
                });
                add_envelope_fields(
                    &mut payload,
                    "web_cache_search_extract",
                    t0.elapsed().as_millis(),
                );
                let md = web_cache_search_extract_markdown(&payload);
                return Ok(tool_result_markdown_with_json(payload, md));
            }

            let Some(cache_dir) = cache_dir_from_env() else {
                let mut payload = serde_json::json!({
                    "ok": false,
                    "query": query,
                    "error": error_obj(
                        ErrorCode::NotConfigured,
                        "WEBPIPE_CACHE_DIR is not set",
                        "Set WEBPIPE_CACHE_DIR to enable cache-only search."
                    ),
                });
                add_envelope_fields(
                    &mut payload,
                    "web_cache_search_extract",
                    t0.elapsed().as_millis(),
                );
                let md = web_cache_search_extract_markdown(&payload);
                return Ok(tool_result_markdown_with_json(payload, md));
            };

            let max_docs = args.max_docs.unwrap_or(50).min(500);
            let max_scan_entries = args.max_scan_entries.unwrap_or(2000).min(20000);
            let max_chars = args.max_chars.unwrap_or(20_000).min(200_000);
            let max_bytes = args.max_bytes.unwrap_or(5_000_000);
            let width = args.width.unwrap_or(100).clamp(20, 240);
            let top_chunks = args.top_chunks.unwrap_or(5).min(50);
            let max_chunk_chars = args.max_chunk_chars.unwrap_or(500).min(5_000);
            // Default to structure output for higher-quality chunk selection.
            let include_structure = args.include_structure.unwrap_or(true);
            let max_outline_items = args.max_outline_items.unwrap_or(25).min(200);
            let max_blocks = args.max_blocks.unwrap_or(40).min(200);
            let max_block_chars = args.max_block_chars.unwrap_or(400).min(2000);
            let include_text = args.include_text.unwrap_or(false);
            let semantic_rerank = args.semantic_rerank.unwrap_or(false);
            let semantic_top_k = args.semantic_top_k.unwrap_or(5).min(50);
            let compact = args.compact.unwrap_or(true);

            let cache_dir_s = cache_dir.to_string_lossy().to_string();
            let query_s = query.clone();
            let cache_dir2 = cache_dir.clone();
            let cache_timeout_ms = std::env::var("WEBPIPE_CACHE_SEARCH_TIMEOUT_MS")
                .ok()
                .and_then(|s| s.trim().parse::<u64>().ok())
                .unwrap_or(20_000)
                .clamp(0, 300_000);
            if cache_timeout_ms == 0 {
                let mut payload = serde_json::json!({
                    "ok": false,
                    "query": query,
                    "cache_dir": cache_dir_s,
                    "error": error_obj(
                        ErrorCode::CacheError,
                        "cache search timed out after 0ms",
                        "Increase WEBPIPE_CACHE_SEARCH_TIMEOUT_MS (or reduce max_scan_entries/max_docs)."
                    ),
                    "warnings": ["cache_search_timeout"],
                    "warning_codes": ["cache_search_timeout"],
                    "warning_hints": warning_hints_from(&["cache_search_timeout"]),
                    "request": {
                        "max_docs": max_docs,
                        "max_scan_entries": max_scan_entries,
                        "max_chars": max_chars,
                        "max_bytes": max_bytes,
                        "width": width,
                        "top_chunks": top_chunks,
                        "max_chunk_chars": max_chunk_chars,
                        "include_structure": include_structure,
                        "include_text": include_text,
                        "semantic_rerank": semantic_rerank,
                        "semantic_top_k": semantic_top_k,
                        "compact": compact,
                        "cache_search_timeout_ms": cache_timeout_ms
                    }
                });
                add_envelope_fields(
                    &mut payload,
                    "web_cache_search_extract",
                    t0.elapsed().as_millis(),
                );
                let md = web_cache_search_extract_markdown(&payload);
                return Ok(tool_result_markdown_with_json(payload, md));
            }
            let handle = tokio::task::spawn_blocking(move || {
                webpipe_local::cache_search::cache_search_extract(
                    &cache_dir2,
                    &query_s,
                    max_docs,
                    max_chars,
                    max_bytes,
                    width,
                    top_chunks,
                    max_chunk_chars,
                    include_structure,
                    max_outline_items,
                    max_blocks,
                    max_block_chars,
                    include_text,
                    max_scan_entries,
                )
            });
            let r = match tokio::time::timeout(
                std::time::Duration::from_millis(cache_timeout_ms),
                handle,
            )
            .await
            {
                Ok(join) => join.map_err(|e| {
                    McpError::internal_error(format!("cache search task failed: {e}"), None)
                })?,
                Err(_) => {
                    let mut payload = serde_json::json!({
                        "ok": false,
                        "query": query,
                        "cache_dir": cache_dir_s,
                        "error": error_obj(
                            ErrorCode::CacheError,
                            format!("cache search timed out after {cache_timeout_ms}ms"),
                            "Increase WEBPIPE_CACHE_SEARCH_TIMEOUT_MS (or reduce max_scan_entries/max_docs)."
                        ),
                        "warnings": ["cache_search_timeout"],
                        "warning_codes": ["cache_search_timeout"],
                        "warning_hints": warning_hints_from(&["cache_search_timeout"]),
                        "request": {
                            "max_docs": max_docs,
                            "max_scan_entries": max_scan_entries,
                            "max_chars": max_chars,
                            "max_bytes": max_bytes,
                            "width": width,
                            "top_chunks": top_chunks,
                            "max_chunk_chars": max_chunk_chars,
                            "include_structure": include_structure,
                            "include_text": include_text,
                            "semantic_rerank": semantic_rerank,
                            "semantic_top_k": semantic_top_k,
                            "compact": compact,
                            "cache_search_timeout_ms": cache_timeout_ms
                        }
                    });
                    add_envelope_fields(
                        &mut payload,
                        "web_cache_search_extract",
                        t0.elapsed().as_millis(),
                    );
                    let md = web_cache_search_extract_markdown(&payload);
                    return Ok(tool_result_markdown_with_json(payload, md));
                }
            };

            let mut results_json =
                serde_json::to_value(&r.results).unwrap_or_else(|_| serde_json::json!([]));

            // Deduplicate repeated URLs (cache search can emit multiple entries per URL).
            // This keeps offline workflows from feeling "broken" (same URL repeated many times).
            let mut dedup_stats: Option<(usize, usize)> = None;
            if let Some(arr) = results_json.as_array_mut() {
                let before = arr.len();
                fn key_for(one: &serde_json::Value) -> Option<String> {
                    // Prefer the user-facing `url` field for dedup: cache search is a UX tool,
                    // and repeated visible URLs feel broken even if `final_url` differs.
                    let u = one
                        .get("url")
                        .and_then(|v| v.as_str())
                        .or_else(|| one.get("final_url").and_then(|v| v.as_str()))
                        .unwrap_or("")
                        .trim();
                    if u.is_empty() {
                        return None;
                    }
                    // Best-effort canonicalization: drop fragment (and keep the rest).
                    // We deliberately keep the query string, because it can meaningfully identify
                    // distinct pages in some doc sites.
                    let canon = reqwest::Url::parse(u)
                        .ok()
                        .map(|mut x| {
                            x.set_fragment(None);
                            x.to_string()
                        })
                        .unwrap_or_else(|| u.to_string());
                    Some(canon)
                }

                fn merge_str_array(dst: &mut serde_json::Value, src: &serde_json::Value) {
                    let Some(dst_arr) = dst.as_array_mut() else {
                        return;
                    };
                    let Some(src_arr) = src.as_array() else {
                        return;
                    };
                    let mut seen = std::collections::BTreeSet::<String>::new();
                    for v in dst_arr.iter() {
                        if let Some(s) = v.as_str() {
                            seen.insert(s.to_string());
                        }
                    }
                    for v in src_arr {
                        if let Some(s) = v.as_str() {
                            if seen.insert(s.to_string()) {
                                dst_arr.push(serde_json::json!(s));
                            }
                        }
                    }
                }

                fn chunk_key(c: &serde_json::Value) -> String {
                    let sc = c.get("start_char").and_then(|v| v.as_u64()).unwrap_or(0);
                    let ec = c.get("end_char").and_then(|v| v.as_u64()).unwrap_or(0);
                    let t = c.get("text").and_then(|v| v.as_str()).unwrap_or("");
                    format!("{sc}:{ec}:{t}")
                }

                // Merge docs by canonical URL.
                let mut merged: std::collections::BTreeMap<String, serde_json::Value> =
                    std::collections::BTreeMap::new();
                for one in arr.drain(..) {
                    let Some(k) = key_for(&one) else { continue };
                    if let Some(prev) = merged.get_mut(&k) {
                        // Prefer higher score and fresher fetched timestamp.
                        let prev_score = prev.get("score").and_then(|v| v.as_f64()).unwrap_or(0.0);
                        let one_score = one.get("score").and_then(|v| v.as_f64()).unwrap_or(0.0);
                        if one_score > prev_score {
                            prev.as_object_mut().map(|m| {
                                m.insert("score".to_string(), serde_json::json!(one_score))
                            });
                        }
                        let prev_ts = prev
                            .get("fetched_at_epoch_s")
                            .and_then(|v| v.as_u64())
                            .unwrap_or(0);
                        let one_ts = one
                            .get("fetched_at_epoch_s")
                            .and_then(|v| v.as_u64())
                            .unwrap_or(0);
                        if one_ts > prev_ts {
                            if let Some(m) = prev.as_object_mut() {
                                if let Some(v) = one.get("fetched_at_epoch_s") {
                                    m.insert("fetched_at_epoch_s".to_string(), v.clone());
                                }
                            }
                        }

                        // Merge warnings.
                        if prev.get("warnings").is_some() || one.get("warnings").is_some() {
                            let mut dst = prev
                                .get("warnings")
                                .cloned()
                                .unwrap_or_else(|| serde_json::json!([]));
                            let src = one
                                .get("warnings")
                                .cloned()
                                .unwrap_or_else(|| serde_json::json!([]));
                            merge_str_array(&mut dst, &src);
                            prev.as_object_mut()
                                .map(|m| m.insert("warnings".to_string(), dst));
                        }

                        // Merge chunks (dedupe by (start,end,text)).
                        if prev.get("chunks").is_some() || one.get("chunks").is_some() {
                            let mut dst = prev
                                .get("chunks")
                                .cloned()
                                .unwrap_or_else(|| serde_json::json!([]));
                            let src = one
                                .get("chunks")
                                .cloned()
                                .unwrap_or_else(|| serde_json::json!([]));
                            if let (Some(dst_arr), Some(src_arr)) =
                                (dst.as_array_mut(), src.as_array())
                            {
                                let mut seen = std::collections::BTreeSet::<String>::new();
                                for c in dst_arr.iter() {
                                    seen.insert(chunk_key(c));
                                }
                                for c in src_arr {
                                    let ck = chunk_key(c);
                                    if seen.insert(ck) {
                                        dst_arr.push(c.clone());
                                    }
                                }
                                // Keep chunks in a stable, score-desc order when available.
                                dst_arr.sort_by(|a, b| {
                                    let sa = a.get("score").and_then(|v| v.as_f64()).unwrap_or(0.0);
                                    let sb = b.get("score").and_then(|v| v.as_f64()).unwrap_or(0.0);
                                    sb.partial_cmp(&sa).unwrap_or(std::cmp::Ordering::Equal)
                                });
                            }
                            prev.as_object_mut()
                                .map(|m| m.insert("chunks".to_string(), dst));
                        }
                    } else {
                        merged.insert(k, one);
                    }
                }

                let mut out: Vec<serde_json::Value> = merged.into_values().collect();
                out.sort_by(|a, b| {
                    let sa = a.get("score").and_then(|v| v.as_f64()).unwrap_or(0.0);
                    let sb = b.get("score").and_then(|v| v.as_f64()).unwrap_or(0.0);
                    sb.partial_cmp(&sa)
                        .unwrap_or(std::cmp::Ordering::Equal)
                        .then_with(|| {
                            let ua = a
                                .get("final_url")
                                .and_then(|v| v.as_str())
                                .or_else(|| a.get("url").and_then(|v| v.as_str()))
                                .unwrap_or("");
                            let ub = b
                                .get("final_url")
                                .and_then(|v| v.as_str())
                                .or_else(|| b.get("url").and_then(|v| v.as_str()))
                                .unwrap_or("");
                            ua.cmp(ub)
                        })
                });
                out.truncate(max_docs);
                *arr = out;
                let after = arr.len();
                if after != before {
                    dedup_stats = Some((before, after));
                }
            }

            if semantic_rerank {
                if let Some(arr) = results_json.as_array_mut() {
                    let use_embeddings = Self::openrouter_api_key_from_env().is_some();
                    let max_embed_docs = Self::semantic_embeddings_max_docs_from_env();
                    for (doc_i, one) in arr.iter_mut().enumerate() {
                        let chunks = one
                            .get("chunks")
                            .and_then(|v| v.as_array())
                            .cloned()
                            .unwrap_or_default();
                        let mut cands: Vec<(usize, usize, String)> = Vec::new();
                        for c in chunks {
                            let sc =
                                c.get("start_char").and_then(|x| x.as_u64()).unwrap_or(0) as usize;
                            let ec =
                                c.get("end_char").and_then(|x| x.as_u64()).unwrap_or(0) as usize;
                            let txt = c
                                .get("text")
                                .and_then(|x| x.as_str())
                                .unwrap_or("")
                                .to_string();
                            cands.push((sc, ec, txt));
                        }
                        let sem = if use_embeddings && doc_i < max_embed_docs && !cands.is_empty() {
                            serde_json::json!(
                                self.semantic_rerank_chunks_best(&query, &cands, semantic_top_k)
                                    .await
                            )
                        } else {
                            serde_json::json!(webpipe_local::semantic::semantic_rerank_chunks(
                                &query,
                                &cands,
                                semantic_top_k
                            ))
                        };
                        one.as_object_mut()
                            .map(|m| m.insert("semantic".to_string(), sem));
                    }
                }
            }
            // Unify shape with web_search_extract: add an `extract` object (without removing existing fields).
            if let Some(arr) = results_json.as_array_mut() {
                for one in arr {
                    let engine = one
                        .get("extraction_engine")
                        .and_then(|v| v.as_str())
                        .unwrap_or("unknown");
                    let text_chars = one
                        .get("text_chars")
                        .cloned()
                        .unwrap_or(serde_json::Value::Null);
                    let text_truncated = one
                        .get("text_truncated")
                        .cloned()
                        .unwrap_or(serde_json::Value::Null);
                    let chunks = one
                        .get("chunks")
                        .cloned()
                        .unwrap_or_else(|| serde_json::json!([]));
                    let structure = one.get("structure").cloned();
                    let mut ex = serde_json::json!({
                        "engine": engine,
                        "width": width,
                        "max_chars": max_chars,
                        "text_chars": text_chars,
                        "text_truncated": text_truncated,
                        "top_chunks": top_chunks,
                        "max_chunk_chars": max_chunk_chars,
                        "chunks": chunks,
                    });
                    if include_structure {
                        if let Some(s) = structure {
                            ex["structure"] = s;
                        }
                    }
                    one.as_object_mut()
                        .map(|m| m.insert("extract".to_string(), ex));
                }
            }

            // Add per-doc warning_codes + warning_hints (additive).
            if let Some(arr) = results_json.as_array_mut() {
                for one in arr {
                    if let Some(ws) = one.get("warnings").and_then(|v| v.as_array()) {
                        let mut wv: Vec<&'static str> = Vec::new();
                        for x in ws {
                            if let Some(s) = x.as_str() {
                                // warning strings in cache search are expected to be static codes.
                                // If a non-static string appears, skip it (do not allocate new statics).
                                // This keeps warning_codes bounded + stable.
                                if let Some(s2) = match s {
                                    "cache_doc_reused" => Some("cache_doc_reused"),
                                    "text_truncated_by_max_chars" => {
                                        Some("text_truncated_by_max_chars")
                                    }
                                    "text_truncated_by_max_text_chars" => {
                                        Some("text_truncated_by_max_text_chars")
                                    }
                                    _ => None,
                                } {
                                    wv.push(s2);
                                }
                            }
                        }
                        if !wv.is_empty() {
                            let codes = warning_codes_from(&wv);
                            if let Some(m) = one.as_object_mut() {
                                m.insert(
                                    "warning_codes".to_string(),
                                    serde_json::json!(codes.clone()),
                                );
                                m.insert("warning_hints".to_string(), warning_hints_from(&codes));
                            }
                        }
                    }
                }
            }

            // Optional compact mode: reduce duplication by retaining only key fields + `extract`.
            if compact {
                if let Some(arr) = results_json.as_array_mut() {
                    for one in arr {
                        let mut out = serde_json::Map::new();
                        for k in [
                            "url",
                            "final_url",
                            "status",
                            "content_type",
                            "bytes",
                            "score",
                            "fetched_at_epoch_s",
                            "extraction_engine",
                            "warnings",
                            "warning_codes",
                            "warning_hints",
                            "semantic",
                            "extract",
                        ] {
                            if let Some(v) = one.get(k) {
                                out.insert(k.to_string(), v.clone());
                            }
                        }
                        *one = serde_json::Value::Object(out);
                    }
                }
            }

            let mut payload = serde_json::json!({
                "ok": r.ok,
                "query": query,
                "query_key": Self::query_key(&query),
                "cache_dir": cache_dir_s,
                "scanned_entries": r.scanned_entries,
                "selected_docs": r.selected_docs,
                "results": results_json,
                "request": {
                    "max_docs": max_docs,
                    "max_scan_entries": max_scan_entries,
                    "max_chars": max_chars,
                    "max_bytes": max_bytes,
                    "width": width,
                    "top_chunks": top_chunks,
                    "max_chunk_chars": max_chunk_chars,
                    "include_text": include_text,
                    "include_structure": include_structure,
                    "max_outline_items": max_outline_items,
                    "max_blocks": max_blocks,
                    "max_block_chars": max_block_chars,
                    "semantic_rerank": semantic_rerank,
                    "semantic_top_k": semantic_top_k,
                    "compact": compact
                }
            });
            if let Some((before, after)) = dedup_stats {
                payload["dedup"] = serde_json::json!({
                    "before": before,
                    "after": after,
                    "dropped": before.saturating_sub(after)
                });
            }
            if semantic_rerank {
                let sem_probe = webpipe_local::semantic::semantic_rerank_chunks(&query, &[], 1);
                let codes: Vec<&'static str> = sem_probe
                    .warnings
                    .iter()
                    .copied()
                    .map(normalize_warning_code)
                    .collect();
                if !codes.is_empty() && !sem_probe.ok {
                    payload["semantic_status"] = serde_json::json!({
                        "ok": false,
                        "backend": sem_probe.backend,
                        "model_id": sem_probe.model_id,
                        "warning_codes": codes.clone(),
                        "warning_hints": warning_hints_from(&codes),
                    });
                }
            }
            if !r.warnings.is_empty() {
                payload["warnings"] = serde_json::json!(r.warnings);
                let codes = warning_codes_from(&r.warnings);
                payload["warning_codes"] = serde_json::json!(codes.clone());
                payload["warning_hints"] = warning_hints_from(&codes);
            }
            add_envelope_fields(
                &mut payload,
                "web_cache_search_extract",
                t0.elapsed().as_millis(),
            );
            let md = web_cache_search_extract_markdown(&payload);
            Ok(tool_result_markdown_with_json(payload, md))
        }

        #[tool(
            description = "Best for: multi-source research questions that require gathering and synthesizing evidence across several pages. Not this for single-URL extraction — use web_extract. Not this when you want inspectable evidence without LLM synthesis — use search_evidence with synthesize=false. Output (synthesize=false): top_chunks[] + evidence[]. Output (synthesize=true): answer text + citations (non-deterministic; not reproducible from cache).",
            input_schema = Arc::new(tool_input_schema_draft07::<WebDeepResearchArgs>()),
            annotations(title = "Deep research", read_only_hint = true, open_world_hint = true)
        )]
        async fn web_deep_research(
            &self,
            params: Parameters<Option<WebDeepResearchArgs>>,
        ) -> Result<CallToolResult, McpError> {
            let args = params.0.unwrap_or_default();
            self.stats_inc_tool("web_deep_research");
            let t0 = std::time::Instant::now();

            let query = args.query.trim().to_string();
            if query.is_empty() {
                let mut payload = serde_json::json!({
                    "ok": false,
                    "query": args.query,
                    "error": error_obj(ErrorCode::InvalidParams, "query must be a non-empty string", "Pass a question/prompt as `query`."),
                });
                add_envelope_fields(&mut payload, "web_deep_research", t0.elapsed().as_millis());
                let md = web_deep_research_markdown(&payload);
                return Ok(tool_result_markdown_with_json(payload, md));
            }

            let max_results_user = args.max_results.is_some();
            let max_urls_user = args.max_urls.is_some();
            let max_chars_user = args.max_chars.is_some();
            let top_chunks_user = args.top_chunks.is_some();
            let max_chunk_chars_user = args.max_chunk_chars.is_some();

            let mut max_results = args.max_results.unwrap_or(5).clamp(1, 20);
            let mut max_urls = args.max_urls.unwrap_or(3).clamp(1, 10);
            let timeout_ms = args.timeout_ms.unwrap_or(20_000);
            let max_bytes = args.max_bytes.unwrap_or(5_000_000);
            let width = args.width.unwrap_or(100).clamp(20, 240);
            let mut max_chars = args.max_chars.unwrap_or(20_000).min(200_000);
            let mut top_chunks = args.top_chunks.unwrap_or(5).min(50);
            let mut max_chunk_chars = args.max_chunk_chars.unwrap_or(500).min(5_000);
            let include_links = args.include_links.unwrap_or(false);
            let max_links = args.max_links.unwrap_or(25).min(500);
            let include_evidence = args.include_evidence.unwrap_or(true);
            let synthesize = args.synthesize.unwrap_or(true);
            let audit = args.audit.unwrap_or(false);

            if audit && !include_evidence {
                let mut payload = serde_json::json!({
                    "ok": false,
                    "query": query,
                    "model": args.model.unwrap_or_else(|| "sonar-deep-research".to_string()),
                    "error": error_obj(
                        ErrorCode::InvalidParams,
                        "audit=true requires include_evidence=true",
                        "Set include_evidence=true (audit mode requires returning the evidence pack)."
                    ),
                });
                add_envelope_fields(&mut payload, "web_deep_research", t0.elapsed().as_millis());
                let md = web_deep_research_markdown(&payload);
                return Ok(tool_result_markdown_with_json(payload, md));
            }
            if audit && args.search_mode.is_some() {
                let mut payload = serde_json::json!({
                    "ok": false,
                    "query": query,
                    "model": args.model.unwrap_or_else(|| "sonar-deep-research".to_string()),
                    "error": error_obj(
                        ErrorCode::NotSupported,
                        "audit=true cannot be combined with search_mode",
                        "Unset search_mode (provider-side browsing is not audit-friendly), or set audit=false."
                    ),
                });
                add_envelope_fields(&mut payload, "web_deep_research", t0.elapsed().as_millis());
                let md = web_deep_research_markdown(&payload);
                return Ok(tool_result_markdown_with_json(payload, md));
            }

            let exploration = args
                .exploration
                .clone()
                .unwrap_or_else(|| "balanced".to_string());
            match exploration.as_str() {
                "balanced" => {}
                "wide" => {
                    if !max_results_user {
                        max_results = 10;
                    }
                    if !max_urls_user {
                        max_urls = 5;
                    }
                    if !max_chars_user {
                        max_chars = 10_000;
                    }
                    if !top_chunks_user {
                        top_chunks = 3;
                    }
                    if !max_chunk_chars_user {
                        max_chunk_chars = 400;
                    }
                }
                "deep" => {
                    if !max_results_user {
                        max_results = 5;
                    }
                    if !max_urls_user {
                        max_urls = 2;
                    }
                    if !max_chars_user {
                        max_chars = 60_000;
                    }
                    if !top_chunks_user {
                        top_chunks = 10;
                    }
                    if !max_chunk_chars_user {
                        max_chunk_chars = 800;
                    }
                }
                _ => {
                    let mut payload = serde_json::json!({
                        "ok": false,
                        "query": args.query,
                        "exploration": exploration,
                        "error": error_obj(
                            ErrorCode::InvalidParams,
                            "unknown exploration preset",
                            "Allowed exploration values: balanced, wide, deep"
                        ),
                    });
                    add_envelope_fields(
                        &mut payload,
                        "web_deep_research",
                        t0.elapsed().as_millis(),
                    );
                    let md = web_deep_research_markdown(&payload);
                    return Ok(tool_result_markdown_with_json(payload, md));
                }
            }

            let provider = args.provider.unwrap_or_else(|| "auto".to_string());
            let auto_mode = args
                .auto_mode
                .clone()
                .unwrap_or_else(|| "fallback".to_string());
            let selection_mode = args
                .selection_mode
                .clone()
                .unwrap_or_else(|| "score".to_string());
            let model = args
                .model
                .unwrap_or_else(|| "sonar-deep-research".to_string());
            let fetch_backend = args.fetch_backend.unwrap_or_else(|| "local".to_string());
            let no_network = args.no_network.unwrap_or(false);
            if selection_mode.as_str() != "score" && selection_mode.as_str() != "pareto" {
                let mut payload = serde_json::json!({
                    "ok": false,
                    "query": query,
                    "model": model,
                    "error": error_obj(
                        ErrorCode::InvalidParams,
                        "unknown selection_mode",
                        "Allowed selection_mode values: score, pareto"
                    ),
                });
                add_envelope_fields(&mut payload, "web_deep_research", t0.elapsed().as_millis());
                let md = web_deep_research_markdown(&payload);
                return Ok(tool_result_markdown_with_json(payload, md));
            }

            let max_answer_chars = args.max_answer_chars.unwrap_or(20_000).min(200_000);
            let reasoning_effort = args.reasoning_effort.clone();
            let max_tokens = args.max_tokens;
            let temperature = args.temperature;
            let top_p = args.top_p;
            let llm_backend = args
                .llm_backend
                .clone()
                .unwrap_or_else(|| "auto".to_string());
            let llm_model = args.llm_model.clone();

            // 1) Gather evidence with our own bounded pipeline (so even if Perplexity is flaky, we can inspect what we fed it).
            let firecrawl_fallback_on_empty_extraction = fetch_backend == "local"
                && (has_env("WEBPIPE_FIRECRAWL_API_KEY") || has_env("FIRECRAWL_API_KEY"));

            if no_network && args.urls.is_none() {
                let mut payload = serde_json::json!({
                    "ok": false,
                    "query": query,
                    "model": model,
                    "error": error_obj(
                        ErrorCode::NotSupported,
                        "no_network=true requires urls=[...] (offline evidence only)",
                        "Pass urls=[...] to run offline, or set no_network=false."
                    ),
                });
                add_envelope_fields(&mut payload, "web_deep_research", t0.elapsed().as_millis());
                let md = web_deep_research_markdown(&payload);
                return Ok(tool_result_markdown_with_json(payload, md));
            }
            if no_network && args.search_mode.is_some() {
                let mut payload = serde_json::json!({
                    "ok": false,
                    "query": query,
                    "model": model,
                    "error": error_obj(
                        ErrorCode::NotSupported,
                        "no_network=true cannot be combined with search_mode",
                        "Unset search_mode (Perplexity browsing is network-only), or set no_network=false."
                    ),
                });
                add_envelope_fields(&mut payload, "web_deep_research", t0.elapsed().as_millis());
                let md = web_deep_research_markdown(&payload);
                return Ok(tool_result_markdown_with_json(payload, md));
            }

            let evidence_r = self
                .web_search_extract(p(WebSearchExtractArgs {
                    query: Some(query.clone()),
                    urls: args.urls.clone(),
                    provider: Some(provider.clone()),
                    auto_mode: Some(auto_mode.clone()),
                    selection_mode: Some(selection_mode.clone()),
                    url_selection_mode: Some("query_rank".to_string()),
                    fetch_backend: Some(fetch_backend.clone()),
                    no_network: Some(no_network),
                    firecrawl_fallback_on_empty_extraction: Some(
                        firecrawl_fallback_on_empty_extraction,
                    ),
                    firecrawl_fallback_on_low_signal: Some(false),
                    render_fallback_on_empty_extraction: Some(false),
                    render_fallback_on_low_signal: Some(false),
                    max_results: Some(max_results),
                    max_urls: Some(max_urls),
                    timeout_ms: Some(timeout_ms),
                    max_bytes: Some(max_bytes),
                    width: Some(width),
                    max_chars: Some(max_chars),
                    top_chunks: Some(top_chunks),
                    max_chunk_chars: Some(max_chunk_chars),
                    include_links: Some(include_links),
                    include_structure: Some(false),
                    max_outline_items: None,
                    max_blocks: None,
                    max_block_chars: None,
                    semantic_rerank: None,
                    semantic_top_k: None,
                    max_links: Some(max_links),
                    include_text: Some(false),
                    cache_read: Some(true),
                    cache_write: Some(true),
                    cache_ttl_s: None,
                    exploration: None,
                    agentic: Some(false),
                    agentic_selector: Some("lexical".to_string()),
                    agentic_max_search_rounds: None,
                    agentic_frontier_max: None,
                    planner_max_calls: None,
                    retry_on_truncation: None,
                    truncation_retry_max_bytes: None,
                    // The web_deep_research tool builds its own compact evidence pack below; keep
                    // the upstream evidence call compact by default to avoid huge payloads.
                    compact: Some(true),
                    ..Default::default()
                }))
                .await?;

            let evidence = payload_from_result(&evidence_r);
            if evidence.get("ok").and_then(|v| v.as_bool()) != Some(true) {
                let mut payload = serde_json::json!({
                    "ok": false,
                    "query": query,
                    "model": model,
                    "error": error_obj(
                        ErrorCode::SearchFailed,
                        "failed to gather evidence via search_evidence",
                        "Inspect the `evidence` field for details; try a different provider or reduce max_urls/max_results."
                    ),
                    "request": {
                        "provider": provider,
                        "auto_mode": auto_mode,
                        "selection_mode": selection_mode,
                        "fetch_backend": fetch_backend,
                        "max_results": max_results,
                        "max_urls": max_urls
                    }
                });
                if include_evidence {
                    payload["evidence"] = evidence;
                }
                add_envelope_fields(&mut payload, "web_deep_research", t0.elapsed().as_millis());
                return Ok(tool_result(payload));
            }

            // Note: we do not run a second hidden evidence pass here. If Firecrawl is available and
            // fetch_backend="local", the per-URL fallback is handled inside web_search_extract.

            // Build a compact evidence pack for the LLM: keep only URLs + top chunks, never full HTML.
            let mut compact_results = Vec::new();
            if let Some(rs) = evidence.get("results").and_then(|v| v.as_array()) {
                for r in rs {
                    compact_results.push(serde_json::json!({
                        "url": r.get("url").cloned().unwrap_or(serde_json::Value::Null),
                        "final_url": r.get("final_url").cloned().unwrap_or(serde_json::Value::Null),
                        "ok": r.get("ok").cloned().unwrap_or(serde_json::Value::Null),
                        "fetch_backend": r.get("fetch_backend").cloned().unwrap_or(serde_json::Value::Null),
                        "status": r.get("status").cloned().unwrap_or(serde_json::Value::Null),
                        "content_type": r.get("content_type").cloned().unwrap_or(serde_json::Value::Null),
                        "bytes": r.get("bytes").cloned().unwrap_or(serde_json::Value::Null),
                        "extract": {
                            "text_chars": r.get("extract").and_then(|e| e.get("text_chars")).cloned().unwrap_or(serde_json::Value::Null),
                            "chunks": r.get("extract").and_then(|e| e.get("chunks")).cloned().unwrap_or(serde_json::Value::Null),
                        },
                        "warnings": r.get("warnings").cloned().unwrap_or(serde_json::Value::Null),
                    }));
                }
            }
            let evidence_for_llm = serde_json::json!({
                "schema_version": 1,
                "kind": "webpipe_evidence_pack",
                "question": query,
                "question_key": Self::query_key(&query),
                "selection": { "provider": provider.clone(), "auto_mode": auto_mode.clone(), "selection_mode": selection_mode.clone(), "fetch_backend": fetch_backend.clone(), "firecrawl_fallback_on_empty_extraction": firecrawl_fallback_on_empty_extraction },
                "top_chunks": evidence.get("top_chunks").cloned().unwrap_or_else(|| serde_json::json!([])),
                "results": compact_results,
                "firecrawl_retry": serde_json::Value::Null,
                "papers": serde_json::Value::Null,
                "arxiv": serde_json::Value::Null
            });

            // Optionally add bounded ArXiv evidence.
            fn looks_paper_like(q: &str) -> bool {
                let s = q.to_lowercase();
                if s.contains("arxiv") || s.contains("doi:") || s.contains("doi ") {
                    return true;
                }
                let needles = [
                    "paper",
                    "preprint",
                    "abstract",
                    "proceedings",
                    "journal",
                    "conference",
                    "icml",
                    "neurips",
                    "iclr",
                    "acl",
                    "emnlp",
                    "naacl",
                    "cvpr",
                ];
                if needles.iter().any(|n| s.contains(n)) {
                    return true;
                }
                // ArXiv id-ish patterns.
                let chars: Vec<char> = s.chars().collect();
                for i in 0..chars.len().saturating_sub(9) {
                    // yyyy.xxxxx (very rough)
                    let a = chars.get(i..i + 4).unwrap_or(&[]);
                    let b = chars.get(i + 4).copied().unwrap_or('_');
                    let c = chars.get(i + 5..i + 9).unwrap_or(&[]);
                    if a.iter().all(|ch| ch.is_ascii_digit())
                        && b == '.'
                        && c.iter().all(|ch| ch.is_ascii_digit())
                    {
                        return true;
                    }
                }
                false
            }

            // Optionally add bounded "papers" evidence (Semantic Scholar / OpenAlex / optional Scholar API).
            let papers_mode = args
                .papers_mode
                .clone()
                .unwrap_or_else(|| "auto".to_string());
            let papers_enabled = match papers_mode.as_str() {
                "off" => false,
                "on" => true,
                "auto" => looks_paper_like(&query),
                _ => {
                    let mut payload = serde_json::json!({
                        "ok": false,
                        "query": query,
                        "model": model,
                        "papers_mode": papers_mode,
                        "error": error_obj(
                            ErrorCode::InvalidParams,
                            "unknown papers_mode",
                            "Allowed papers_mode values: off, auto, on"
                        ),
                    });
                    add_envelope_fields(
                        &mut payload,
                        "web_deep_research",
                        t0.elapsed().as_millis(),
                    );
                    return Ok(tool_result(payload));
                }
            };

            let papers_block = if papers_enabled && !no_network {
                let max_papers = args.papers_max_papers.unwrap_or(5).clamp(1, 20);
                let backends = args.papers_backends.clone().unwrap_or_default();
                let years = args.papers_years.clone().unwrap_or_default();
                let papers_timeout_ms = args.papers_timeout_ms.unwrap_or(12_000).min(60_000);
                let include_abs = args.papers_include_abstract.unwrap_or(false);

                match webpipe_local::papers::paper_search(
                    self.http.clone(),
                    query.clone(),
                    backends.clone(),
                    years.clone(),
                    // limit is per-backend; keep it bounded and aligned with max_papers.
                    max_papers,
                    papers_timeout_ms,
                    include_abs,
                )
                .await
                {
                    Ok(r) => {
                        let mut papers = Vec::new();
                        for p in r.papers.into_iter().take(max_papers) {
                            let abs = p.abstract_text.clone().unwrap_or_default();
                            let (abs, _n, _clipped) =
                                Self::truncate_to_chars(&abs, max_chunk_chars.min(2_000));
                            papers.push(serde_json::json!({
                                "title": p.title,
                                "year": p.year,
                                "authors": p.authors,
                                "venue": p.venue,
                                "url": p.url,
                                "pdf_url": p.pdf_url,
                                "doi": p.doi,
                                "arxiv_id": p.arxiv_id,
                                "citation_count": p.citation_count,
                                "abstract": if abs.trim().is_empty() { serde_json::Value::Null } else { serde_json::Value::String(abs) },
                                "source": p.source,
                            }));
                        }
                        serde_json::json!({
                            "ok": true,
                            "query": r.query,
                            "backends": r.backends,
                            "warnings": r.warnings,
                            "papers": papers,
                            "timings_ms": r.timings_ms,
                        })
                    }
                    Err(e) => {
                        let msg = e.to_string();
                        serde_json::json!({
                            "ok": false,
                            "error": msg,
                        })
                    }
                }
            } else if papers_enabled && no_network {
                serde_json::json!({
                    "ok": false,
                    "error": "skipped (no_network=true)",
                })
            } else {
                serde_json::Value::Null
            };

            let arxiv_mode = args
                .arxiv_mode
                .clone()
                .unwrap_or_else(|| "auto".to_string());
            let arxiv_enabled = match arxiv_mode.as_str() {
                "off" => false,
                "on" => true,
                "auto" => looks_paper_like(&query),
                _ => {
                    let mut payload = serde_json::json!({
                        "ok": false,
                        "query": query,
                        "model": model,
                        "arxiv_mode": arxiv_mode,
                        "error": error_obj(
                            ErrorCode::InvalidParams,
                            "unknown arxiv_mode",
                            "Allowed arxiv_mode values: off, auto, on"
                        ),
                    });
                    add_envelope_fields(
                        &mut payload,
                        "web_deep_research",
                        t0.elapsed().as_millis(),
                    );
                    return Ok(tool_result(payload));
                }
            };

            let arxiv_block = if arxiv_enabled && !no_network {
                let max_papers = args.arxiv_max_papers.unwrap_or(3).clamp(1, 10);
                let categories = args.arxiv_categories.clone().unwrap_or_default();
                let years = args.arxiv_years.clone().unwrap_or_default();
                let arxiv_timeout_ms = args.arxiv_timeout_ms.unwrap_or(8_000).min(30_000);

                match webpipe_local::arxiv::arxiv_search(
                    self.http.clone(),
                    query.clone(),
                    categories,
                    years,
                    1,
                    max_papers,
                    arxiv_timeout_ms,
                )
                .await
                {
                    Ok(r) => {
                        let mut papers = Vec::new();
                        for p in r.papers.into_iter().take(max_papers) {
                            let (summary, _n, _clipped) =
                                Self::truncate_to_chars(&p.summary, max_chunk_chars.min(2_000));
                            papers.push(serde_json::json!({
                                "arxiv_id": p.arxiv_id,
                                "url": p.url,
                                "pdf_url": p.pdf_url,
                                "title": p.title,
                                "summary": summary,
                                "published": p.published,
                                "updated": p.updated,
                                "authors": p.authors,
                                "primary_category": p.primary_category,
                                "categories": p.categories,
                            }));
                        }
                        serde_json::json!({
                            "ok": true,
                            "query": r.query,
                            "page": r.page,
                            "per_page": r.per_page,
                            "total_results": r.total_results,
                            "warnings": r.warnings,
                            "papers": papers,
                        })
                    }
                    Err(e) => {
                        let msg = e.to_string();
                        serde_json::json!({
                            "ok": false,
                            "error": msg,
                        })
                    }
                }
            } else if arxiv_enabled && no_network {
                serde_json::json!({
                    "ok": false,
                    "error": "skipped (no_network=true)",
                })
            } else {
                serde_json::Value::Null
            };

            let mut evidence_for_llm = evidence_for_llm;
            evidence_for_llm["papers"] = papers_block;
            evidence_for_llm["arxiv"] = arxiv_block;
            let evidence_pack = evidence_for_llm.clone();

            // Keep prompt bounded: pass only URLs + top chunks, never full HTML.
            let sys = "You are a careful research assistant. Use the provided evidence pack. Cite sources by URL. If evidence is insufficient, say so.";
            let user = serde_json::json!({
                "question": query,
                "evidence": evidence_for_llm,
            })
            .to_string();
            let (user, _n_user, _user_clipped) = Self::truncate_to_chars(&user, 20_000);

            if !synthesize {
                let mut payload = serde_json::json!({
                    "ok": true,
                    "query": query,
                    "query_key": Self::query_key(&query),
                    "model": model,
                    "request": {
                        "provider": provider,
                        "auto_mode": auto_mode,
                        "selection_mode": selection_mode,
                        "fetch_backend": fetch_backend,
                        "no_network": no_network,
                        "max_results": max_results,
                        "max_urls": max_urls,
                        "timeout_ms": timeout_ms,
                        "max_bytes": max_bytes,
                        "width": width,
                        "max_chars": max_chars,
                        "top_chunks": top_chunks,
                        "max_chunk_chars": max_chunk_chars,
                        "include_links": include_links,
                        "max_links": max_links,
                        "exploration": exploration,
                        "papers_mode": papers_mode,
                        "papers_max_papers": args.papers_max_papers.unwrap_or(5).clamp(1, 20),
                        "arxiv_mode": arxiv_mode,
                        "arxiv_max_papers": args.arxiv_max_papers.unwrap_or(3).clamp(1, 10),
                        "synthesize": false,
                    },
                    "answer": {
                        "text": "",
                        "skipped": true,
                        "reason": "synthesize=false",
                    },
                });
                if include_evidence {
                    payload["evidence"] = evidence_for_llm;
                }
                add_envelope_fields(&mut payload, "web_deep_research", t0.elapsed().as_millis());
                return Ok(tool_result(payload));
            }

            // LLM backend selection.
            // - In no_network mode, network-backed providers must not be called.
            // - “Local network” (localhost only) is allowed for explicit local backends.
            if no_network && llm_backend.as_str() == "perplexity" {
                let mut payload = serde_json::json!({
                    "ok": false,
                    "query": query,
                    "model": model,
                    "error": error_obj(
                        ErrorCode::NotSupported,
                        "llm_backend=perplexity cannot be used with no_network=true",
                        "Use llm_backend=\"ollama\" or \"openai_compat\" for local synthesis, or set no_network=false."
                    ),
                });
                if include_evidence {
                    payload["evidence"] = evidence;
                }
                add_envelope_fields(&mut payload, "web_deep_research", t0.elapsed().as_millis());
                return Ok(tool_result(payload));
            }

            let selected_backend: &str = match llm_backend.as_str() {
                "perplexity" => "perplexity",
                "ollama" => "ollama",
                "openai_compat" => "openai_compat",
                "auto" => {
                    // Prefer Perplexity when it's available and we're not in strict offline mode.
                    if !no_network
                        && (has_env("WEBPIPE_PERPLEXITY_API_KEY") || has_env("PERPLEXITY_API_KEY"))
                    {
                        "perplexity"
                    } else if has_env("WEBPIPE_OPENAI_COMPAT_BASE_URL") {
                        "openai_compat"
                    } else if has_env("WEBPIPE_OLLAMA_ENABLE")
                        && std::env::var("WEBPIPE_OLLAMA_ENABLE")
                            .ok()
                            .is_some_and(|v| v.trim().eq_ignore_ascii_case("true"))
                    {
                        "ollama"
                    } else {
                        "none"
                    }
                }
                other => {
                    let mut payload = serde_json::json!({
                        "ok": false,
                        "query": query,
                        "model": model,
                        "error": error_obj(
                            ErrorCode::InvalidParams,
                            format!("unknown llm_backend: {other}"),
                            "Allowed llm_backend values: auto, perplexity, ollama, openai_compat"
                        ),
                    });
                    if include_evidence {
                        payload["evidence"] = evidence;
                        payload["evidence_pack"] = evidence_pack.clone();
                    }
                    add_envelope_fields(
                        &mut payload,
                        "web_deep_research",
                        t0.elapsed().as_millis(),
                    );
                    return Ok(tool_result(payload));
                }
            };

            if selected_backend == "none" {
                let mut payload = serde_json::json!({
                    "ok": false,
                    "query": query,
                    "model": model,
                    "error": error_obj(
                        ErrorCode::NotConfigured,
                        "no LLM backend configured for synthesis",
                        "Configure WEBPIPE_PERPLEXITY_API_KEY, or set WEBPIPE_OPENAI_COMPAT_BASE_URL (and llm_model / WEBPIPE_OPENAI_COMPAT_MODEL), or enable Ollama via WEBPIPE_OLLAMA_ENABLE=true."
                    ),
                    "request": { "llm_backend": llm_backend },
                });
                if include_evidence {
                    payload["evidence"] = evidence;
                }
                add_envelope_fields(&mut payload, "web_deep_research", t0.elapsed().as_millis());
                return Ok(tool_result(payload));
            }

            // 2) Synthesize via selected backend.
            let mut deep_warnings: Vec<&'static str> = Vec::new();
            let mut citations: Vec<String> = Vec::new();
            if let Some(arr) = evidence.get("top_chunks").and_then(|v| v.as_array()) {
                for c in arr {
                    if let Some(u) = c.get("url").and_then(|x| x.as_str()) {
                        if !citations.iter().any(|x| x == u) {
                            citations.push(u.to_string());
                        }
                    }
                }
            }

            if selected_backend == "ollama" {
                let llm_t0 = std::time::Instant::now();
                let ollama = match webpipe_local::ollama::OllamaClient::from_env(self.http.clone())
                {
                    Ok(c) => c,
                    Err(e) => {
                        self.stats_record_llm_backend(
                            "ollama",
                            false,
                            llm_t0.elapsed().as_millis() as u64,
                            Some(&e.to_string()),
                        );
                        let mut payload = serde_json::json!({
                            "ok": false,
                            "query": query,
                            "model": model,
                            "error": error_obj(
                                ErrorCode::NotConfigured,
                                e.to_string(),
                                "Enable Ollama synthesis by setting WEBPIPE_OLLAMA_ENABLE=true (and ensure Ollama is running)."
                            ),
                            "request": { "llm_backend": llm_backend },
                        });
                        if include_evidence {
                            payload["evidence"] = evidence;
                            payload["evidence_pack"] = evidence_pack.clone();
                        }
                        add_envelope_fields(
                            &mut payload,
                            "web_deep_research",
                            t0.elapsed().as_millis(),
                        );
                        return Ok(tool_result(payload));
                    }
                };

                if no_network {
                    // Best-effort safety: require localhost base URL when no_network=true.
                    let b = ollama.base_url().to_string();
                    let ok_local =
                        b.contains("127.0.0.1") || b.contains("localhost") || b.contains("[::1]");
                    if !ok_local {
                        let mut payload = serde_json::json!({
                            "ok": false,
                            "query": query,
                            "model": model,
                            "error": error_obj(
                                ErrorCode::NotSupported,
                                "no_network=true requires Ollama to be localhost",
                                "Set WEBPIPE_OLLAMA_BASE_URL to http://127.0.0.1:11434 (or set no_network=false)."
                            ),
                            "request": { "llm_backend": llm_backend, "ollama_base_url": b },
                        });
                        if include_evidence {
                            payload["evidence"] = evidence;
                            payload["evidence_pack"] = evidence_pack.clone();
                        }
                        add_envelope_fields(
                            &mut payload,
                            "web_deep_research",
                            t0.elapsed().as_millis(),
                        );
                        return Ok(tool_result(payload));
                    }
                }

                deep_warnings.push("llm_ollama_used");
                let answer = match ollama.chat(sys, &user, timeout_ms).await {
                    Ok(s) => s,
                    Err(e) => {
                        self.stats_record_llm_backend(
                            "ollama",
                            false,
                            llm_t0.elapsed().as_millis() as u64,
                            Some(&e.to_string()),
                        );
                        let mut payload = serde_json::json!({
                            "ok": false,
                            "query": query,
                            "model": model,
                            "error": error_obj(ErrorCode::ProviderUnavailable, e.to_string(), "Ollama synthesis failed. Check that Ollama is running and the model name exists."),
                            "request": { "llm_backend": "ollama" },
                        });
                        if include_evidence {
                            payload["evidence"] = evidence;
                            payload["evidence_pack"] = evidence_pack.clone();
                        }
                        add_envelope_fields(
                            &mut payload,
                            "web_deep_research",
                            t0.elapsed().as_millis(),
                        );
                        return Ok(tool_result(payload));
                    }
                };
                self.stats_record_llm_backend(
                    "ollama",
                    true,
                    llm_t0.elapsed().as_millis() as u64,
                    None,
                );
                let (answer, _n, clipped) = Self::truncate_to_chars(&answer, max_answer_chars);
                let mut payload = serde_json::json!({
                    "ok": true,
                    "provider": "ollama",
                    "query": query,
                    "request": { "llm_backend": llm_backend, "timeout_ms": timeout_ms, "no_network": no_network },
                    "answer": { "text": answer, "truncated": clipped, "citations": citations },
                });
                if !deep_warnings.is_empty() {
                    payload["warnings"] = serde_json::json!(deep_warnings);
                    let codes = warning_codes_from(&deep_warnings);
                    payload["warning_codes"] = serde_json::json!(codes.clone());
                    payload["warning_hints"] = warning_hints_from(&codes);
                    self.stats_record_warnings(&deep_warnings);
                }
                if include_evidence {
                    payload["evidence"] = evidence;
                    payload["evidence_pack"] = evidence_pack.clone();
                }
                if let Some(now) = args.now_epoch_s {
                    payload["generated_at_epoch_s"] = serde_json::json!(now);
                }
                add_envelope_fields(&mut payload, "web_deep_research", t0.elapsed().as_millis());
                let md = web_deep_research_markdown(&payload);
                return Ok(tool_result_markdown_with_json(payload, md));
            }

            if selected_backend == "openai_compat" {
                let llm_t0 = std::time::Instant::now();
                let client = match webpipe_local::openai_compat::OpenAiCompatClient::from_env(
                    self.http.clone(),
                    llm_model.clone(),
                ) {
                    Ok(c) => c,
                    Err(e) => {
                        self.stats_record_llm_backend(
                            "openai_compat",
                            false,
                            llm_t0.elapsed().as_millis() as u64,
                            Some(&e.to_string()),
                        );
                        let mut payload = serde_json::json!({
                            "ok": false,
                            "query": query,
                            "model": model,
                            "error": error_obj(
                                ErrorCode::NotConfigured,
                                e.to_string(),
                                "Set WEBPIPE_OPENAI_COMPAT_BASE_URL and (llm_model or WEBPIPE_OPENAI_COMPAT_MODEL)."
                            ),
                            "request": { "llm_backend": llm_backend },
                        });
                        if include_evidence {
                            payload["evidence"] = evidence;
                            payload["evidence_pack"] = evidence_pack.clone();
                        }
                        add_envelope_fields(
                            &mut payload,
                            "web_deep_research",
                            t0.elapsed().as_millis(),
                        );
                        return Ok(tool_result(payload));
                    }
                };

                if no_network {
                    let b = client.base_url().to_string();
                    let ok_local =
                        b.contains("127.0.0.1") || b.contains("localhost") || b.contains("[::1]");
                    if !ok_local {
                        let mut payload = serde_json::json!({
                            "ok": false,
                            "query": query,
                            "model": model,
                            "error": error_obj(
                                ErrorCode::NotSupported,
                                "no_network=true requires openai_compat base URL to be localhost",
                                "Set WEBPIPE_OPENAI_COMPAT_BASE_URL to http://127.0.0.1:<port> (or set no_network=false)."
                            ),
                            "request": { "llm_backend": llm_backend, "openai_compat_base_url": b },
                        });
                        if include_evidence {
                            payload["evidence"] = evidence;
                            payload["evidence_pack"] = evidence_pack.clone();
                        }
                        add_envelope_fields(
                            &mut payload,
                            "web_deep_research",
                            t0.elapsed().as_millis(),
                        );
                        return Ok(tool_result(payload));
                    }
                }

                deep_warnings.push("llm_openai_compat_used");
                let answer = match client
                    .chat(sys, &user, timeout_ms, max_tokens, temperature, top_p)
                    .await
                {
                    Ok(s) => s,
                    Err(e) => {
                        self.stats_record_llm_backend(
                            "openai_compat",
                            false,
                            llm_t0.elapsed().as_millis() as u64,
                            Some(&e.to_string()),
                        );
                        let mut payload = serde_json::json!({
                            "ok": false,
                            "query": query,
                            "model": model,
                            "error": error_obj(ErrorCode::ProviderUnavailable, e.to_string(), "OpenAI-compatible synthesis failed. Check the base URL, model name, and auth."),
                            "request": { "llm_backend": "openai_compat" },
                        });
                        if include_evidence {
                            payload["evidence"] = evidence;
                        }
                        add_envelope_fields(
                            &mut payload,
                            "web_deep_research",
                            t0.elapsed().as_millis(),
                        );
                        return Ok(tool_result(payload));
                    }
                };
                self.stats_record_llm_backend(
                    "openai_compat",
                    true,
                    llm_t0.elapsed().as_millis() as u64,
                    None,
                );
                let (answer, _n, clipped) = Self::truncate_to_chars(&answer, max_answer_chars);
                let mut payload = serde_json::json!({
                    "ok": true,
                    "provider": "openai_compat",
                    "query": query,
                    "request": { "llm_backend": llm_backend, "timeout_ms": timeout_ms, "no_network": no_network },
                    "answer": { "text": answer, "truncated": clipped, "citations": citations },
                });
                if !deep_warnings.is_empty() {
                    payload["warnings"] = serde_json::json!(deep_warnings);
                    let codes = warning_codes_from(&deep_warnings);
                    payload["warning_codes"] = serde_json::json!(codes.clone());
                    payload["warning_hints"] = warning_hints_from(&codes);
                }
                if include_evidence {
                    payload["evidence"] = evidence;
                    payload["evidence_pack"] = evidence_pack.clone();
                }
                if let Some(now) = args.now_epoch_s {
                    payload["generated_at_epoch_s"] = serde_json::json!(now);
                }
                add_envelope_fields(&mut payload, "web_deep_research", t0.elapsed().as_millis());
                let md = web_deep_research_markdown(&payload);
                return Ok(tool_result_markdown_with_json(payload, md));
            }

            // Otherwise: Perplexity API path (existing behavior).
            let pplx = match webpipe_local::perplexity::PerplexityClient::from_env(
                self.http.clone(),
            ) {
                Ok(c) => c,
                Err(e) => {
                    let mut payload = serde_json::json!({
                        "ok": false,
                        "query": query,
                        "model": model,
                        "error": error_obj(ErrorCode::NotConfigured, e.to_string(), "Set WEBPIPE_PERPLEXITY_API_KEY (or PERPLEXITY_API_KEY) in the webpipe MCP server environment."),
                        "request": { "llm_backend": llm_backend },
                    });
                    if include_evidence {
                        payload["evidence"] = evidence;
                        payload["evidence_pack"] = evidence_pack.clone();
                    }
                    add_envelope_fields(
                        &mut payload,
                        "web_deep_research",
                        t0.elapsed().as_millis(),
                    );
                    return Ok(tool_result(payload));
                }
            };

            // By default, disable provider-side browsing. We already gather an evidence pack ourselves;
            // letting the provider browse makes results less inspectable and can be much more expensive.
            //
            // If the provider rejects "off", we retry once without `search_mode` (and surface a warning).
            let mut effective_search_mode = args.search_mode.clone();
            if !no_network && effective_search_mode.is_none() {
                effective_search_mode = Some("off".to_string());
            }

            let req = webpipe_local::perplexity::ChatCompletionsRequest {
                model: model.clone(),
                messages: vec![
                    webpipe_local::perplexity::Message {
                        role: "system".to_string(),
                        content: sys.to_string(),
                    },
                    webpipe_local::perplexity::Message {
                        role: "user".to_string(),
                        content: user,
                    },
                ],
                max_tokens,
                temperature,
                top_p,
                search_mode: effective_search_mode.clone(),
                reasoning_effort: reasoning_effort.clone(),
            };

            let resp = match pplx.chat_completions(req.clone()).await {
                Ok(r) => r,
                Err(e) => {
                    // Retry once if we set the default `search_mode="off"` and the provider rejected it.
                    // (Do not retry if user explicitly set search_mode.)
                    let defaulted_off = args.search_mode.is_none()
                        && effective_search_mode.as_deref() == Some("off")
                        && !no_network;
                    if defaulted_off {
                        deep_warnings.push("perplexity_search_mode_off_rejected");
                        if audit {
                            let mut payload = serde_json::json!({
                                "ok": false,
                                "query": query,
                                "model": model,
                                "error": error_obj(
                                    ErrorCode::NotSupported,
                                    "perplexity rejected search_mode=\"off\" (audit mode cannot proceed)",
                                    "Set llm_backend to a local backend (ollama/openai_compat), or set audit=false."
                                ),
                                "request": {
                                    "provider": provider,
                                    "auto_mode": auto_mode,
                                    "selection_mode": selection_mode,
                                    "fetch_backend": fetch_backend,
                                    "max_results": max_results,
                                    "max_urls": max_urls
                                },
                                "warnings": ["perplexity_search_mode_off_rejected"]
                            });
                            let codes =
                                warning_codes_from(&["perplexity_search_mode_off_rejected"]);
                            payload["warning_codes"] = serde_json::json!(codes.clone());
                            payload["warning_hints"] = warning_hints_from(&codes);
                            if include_evidence {
                                payload["evidence"] = evidence;
                                payload["evidence_pack"] = evidence_pack.clone();
                            }
                            add_envelope_fields(
                                &mut payload,
                                "web_deep_research",
                                t0.elapsed().as_millis(),
                            );
                            return Ok(tool_result(payload));
                        }
                        let mut req2 = req;
                        req2.search_mode = None;
                        if let Ok(r2) = pplx.chat_completions(req2).await {
                            r2
                        } else {
                            let mut payload = serde_json::json!({
                                "ok": false,
                                "query": query,
                                "model": model,
                                "error": error_obj(ErrorCode::ProviderUnavailable, e.to_string(), "Retry later or reduce scope. You can still use search_evidence output as evidence."),
                                "request": {
                                    "provider": provider,
                                    "auto_mode": auto_mode,
                                    "selection_mode": selection_mode,
                                    "fetch_backend": fetch_backend,
                                    "max_results": max_results,
                                    "max_urls": max_urls,
                                },
                            });
                            if include_evidence {
                                payload["evidence"] = evidence;
                            }
                            add_envelope_fields(
                                &mut payload,
                                "web_deep_research",
                                t0.elapsed().as_millis(),
                            );
                            return Ok(tool_result(payload));
                        }
                    } else {
                        let mut payload = serde_json::json!({
                            "ok": false,
                            "query": query,
                            "model": model,
                            "error": error_obj(ErrorCode::ProviderUnavailable, e.to_string(), "Retry later or reduce scope. You can still use search_evidence output as evidence."),
                            "request": {
                                "provider": provider,
                                "auto_mode": auto_mode,
                                "selection_mode": selection_mode,
                                "fetch_backend": fetch_backend,
                                "max_results": max_results,
                                "max_urls": max_urls,
                            },
                        });
                        if include_evidence {
                            payload["evidence"] = evidence;
                        }
                        add_envelope_fields(
                            &mut payload,
                            "web_deep_research",
                            t0.elapsed().as_millis(),
                        );
                        return Ok(tool_result(payload));
                    }
                }
            };

            let answer_text = resp
                .choices
                .first()
                .map(|c| c.message.content.clone())
                .unwrap_or_default();
            let (answer, _n, clipped) = Self::truncate_to_chars(&answer_text, max_answer_chars);

            let mut payload = serde_json::json!({
                "ok": true,
                "query": query,
                "query_key": Self::query_key(&query),
                "model": model,
                "request": {
                    "provider": provider,
                    "auto_mode": auto_mode,
                    "selection_mode": selection_mode,
                    "fetch_backend": fetch_backend,
                    "no_network": no_network,
                    "max_results": max_results,
                    "max_urls": max_urls,
                    "timeout_ms": timeout_ms,
                    "max_bytes": max_bytes,
                    "width": width,
                    "max_chars": max_chars,
                    "top_chunks": top_chunks,
                    "max_chunk_chars": max_chunk_chars,
                    "include_links": include_links,
                    "max_links": max_links,
                    "search_mode": effective_search_mode,
                    "reasoning_effort": reasoning_effort,
                    "max_answer_chars": max_answer_chars,
                    "max_tokens": max_tokens,
                    "temperature": temperature,
                    "top_p": top_p
                },
                "answer": {
                    "text": answer,
                    "truncated": clipped,
                    "citations": resp.citations.unwrap_or_default()
                },
                "usage": resp.usage,
                "timings_ms": resp.timings_ms
            });
            if !deep_warnings.is_empty() {
                payload["warnings"] = serde_json::json!(deep_warnings);
                let codes = warning_codes_from(&deep_warnings);
                payload["warning_codes"] = serde_json::json!(codes.clone());
                payload["warning_hints"] = warning_hints_from(&codes);
            }
            if include_evidence {
                payload["evidence"] = evidence;
                payload["evidence_pack"] = evidence_pack.clone();
            }
            if let Some(now) = args.now_epoch_s {
                payload["generated_at_epoch_s"] = serde_json::json!(now);
            }
            add_envelope_fields(&mut payload, "web_deep_research", t0.elapsed().as_millis());
            let md = web_deep_research_markdown(&payload);
            Ok(tool_result_markdown_with_json(payload, md))
        }

        #[tool(
            description = "Best for: checking HTTP status, inspecting response headers, or fetching raw bytes from a URL you already know. Not this when you need readable text or ranked evidence — use web_extract (single URL) or search_evidence (search + extract) instead. Output: status, content_type, bytes_len, truncated, optional bounded text (set include_text=true). Cache-first; drops auth/cookie headers by default.",
            input_schema = Arc::new(tool_input_schema_draft07::<WebFetchArgs>()),
            annotations(title = "Fetch URL", read_only_hint = true, open_world_hint = true)
        )]
        async fn web_fetch(
            &self,
            params: Parameters<Option<WebFetchArgs>>,
        ) -> Result<CallToolResult, McpError> {
            let args = params.0.unwrap_or_default();
            self.stats_inc_tool("web_fetch");
            let include_headers = args.include_headers.unwrap_or(false);
            let include_text = args.include_text.unwrap_or(false);
            let max_text_chars = args.max_text_chars.unwrap_or(20_000).min(200_000);
            let fetch_backend = args.fetch_backend.unwrap_or_else(|| "local".to_string());
            let url = args.url.clone().unwrap_or_default();
            let privacy = privacy_mode_from_env();

            // Offline-only mode: never allow non-localhost fetches. (But keep invalid_params
            // behavior higher priority so empty/invalid URLs remain stable.)
            let offline_only = privacy == PrivacyMode::Offline;
            if offline_only && !url.trim().is_empty() && !is_localhost_url(&url) {
                let t0 = std::time::Instant::now();
                let mut payload = serde_json::json!({
                    "ok": false,
                    "url": url,
                    "error": error_obj(
                        ErrorCode::NotSupported,
                        "offline-only mode forbids non-localhost fetches",
                        "Set WEBPIPE_OFFLINE_ONLY=0 (or fetch via localhost fixtures / warmed cache with no_network=true)."
                    ),
                    "request": {
                        "fetch_backend": fetch_backend,
                        "offline_only": true
                    }
                });
                add_envelope_fields(&mut payload, "web_fetch", t0.elapsed().as_millis());
                let md = web_fetch_markdown(&payload);
                return Ok(tool_result_markdown_with_json(payload, md));
            }

            if privacy == PrivacyMode::Anonymous {
                // Require proxy for non-localhost outbound fetches.
                if !url.trim().is_empty()
                    && !is_localhost_url(&url)
                    && anon_proxy_from_env().is_none()
                {
                    let t0 = std::time::Instant::now();
                    let mut payload = serde_json::json!({
                        "ok": false,
                        "url": url,
                        "error": error_obj(
                            ErrorCode::NotConfigured,
                            "anonymous mode requires a proxy",
                            "Set WEBPIPE_ANON_PROXY (recommended for Tor: socks5h://127.0.0.1:9050) or set WEBPIPE_PRIVACY_MODE=normal."
                        ),
                        "request": {
                            "privacy_mode": "anonymous",
                            "anon_proxy_configured": false
                        }
                    });
                    add_envelope_fields(&mut payload, "web_fetch", t0.elapsed().as_millis());
                    let md = web_fetch_markdown(&payload);
                    return Ok(tool_result_markdown_with_json(payload, md));
                }
                // Firecrawl is a remote fetch backend (keyed, remote service); do not allow in anonymous mode.
                if fetch_backend == "firecrawl" {
                    let t0 = std::time::Instant::now();
                    let mut payload = serde_json::json!({
                        "ok": false,
                        "url": url,
                        "error": error_obj(
                            ErrorCode::NotSupported,
                            "fetch_backend=\"firecrawl\" is disabled in anonymous mode",
                            "Use fetch_backend=\"local\" (routed via WEBPIPE_ANON_PROXY) or set WEBPIPE_PRIVACY_MODE=normal."
                        ),
                        "request": {
                            "privacy_mode": "anonymous",
                            "fetch_backend": fetch_backend
                        }
                    });
                    add_envelope_fields(&mut payload, "web_fetch", t0.elapsed().as_millis());
                    let md = web_fetch_markdown(&payload);
                    return Ok(tool_result_markdown_with_json(payload, md));
                }
            }

            // Note: offline-only mode forbids *non-localhost* networking, but it does not force
            // cache-only behavior for localhost fixtures (tests rely on this).
            let no_network = args.no_network.unwrap_or(false);
            let cache_read_effective = args.cache_read.unwrap_or(true) || no_network;
            let cache_write_effective = if no_network {
                false
            } else {
                args.cache_write.unwrap_or(true)
            };

            let t0 = std::time::Instant::now();
            if url.trim().is_empty() {
                let mut payload = serde_json::json!({
                    "ok": false,
                    "url": url,
                    "error": error_obj(
                        ErrorCode::InvalidParams,
                        "url must be non-empty",
                        "Pass an absolute URL like https://example.com."
                    ),
                    "request": {
                        "fetch_backend": fetch_backend,
                        "no_network": no_network,
                        "offline_only": offline_only_enabled(),
                        "timeout_ms": args.timeout_ms.or(Some(15_000)),
                        "max_bytes": args.max_bytes.or(Some(5_000_000)),
                        "cache": { "read": cache_read_effective, "write": cache_write_effective, "ttl_s": args.cache_ttl_s },
                        "include_text": include_text,
                        "max_text_chars": max_text_chars,
                        "include_headers": include_headers
                    }
                });
                add_envelope_fields(&mut payload, "web_fetch", t0.elapsed().as_millis());
                let md = web_fetch_markdown(&payload);
                return Ok(tool_result_markdown_with_json(payload, md));
            }
            if fetch_backend.as_str() != "local" && fetch_backend.as_str() != "firecrawl" {
                let mut payload = serde_json::json!({
                    "ok": false,
                    "url": url,
                    "error": error_obj(
                        ErrorCode::InvalidParams,
                        "unknown fetch_backend",
                        "Allowed fetch_backends: local, firecrawl"
                    ),
                    "request": { "fetch_backend": fetch_backend }
                });
                add_envelope_fields(&mut payload, "web_fetch", t0.elapsed().as_millis());
                let md = web_fetch_markdown(&payload);
                return Ok(tool_result_markdown_with_json(payload, md));
            }

            if no_network && fetch_backend == "firecrawl" {
                let mut payload = serde_json::json!({
                    "ok": false,
                    "url": url,
                    "error": error_obj(
                        ErrorCode::NotSupported,
                        "no_network=true cannot be used with fetch_backend=\"firecrawl\"",
                        "Use fetch_backend=\"local\" with a warmed WEBPIPE_CACHE_DIR, or set no_network=false."
                    ),
                    "request": {
                        "fetch_backend": fetch_backend,
                        "no_network": no_network,
                        "offline_only": offline_only_enabled(),
                        "cache": { "read": cache_read_effective, "write": cache_write_effective, "ttl_s": args.cache_ttl_s }
                    }
                });
                add_envelope_fields(&mut payload, "web_fetch", t0.elapsed().as_millis());
                let md = web_fetch_markdown(&payload);
                return Ok(tool_result_markdown_with_json(payload, md));
            }

            if fetch_backend == "firecrawl" {
                let timeout_ms = args.timeout_ms.unwrap_or(15_000);
                let max_age_ms = args.cache_ttl_s.map(|s| s.saturating_mul(1000));
                let fc = match webpipe_local::firecrawl::FirecrawlClient::from_env(
                    self.http.clone(),
                ) {
                    Ok(c) => c,
                    Err(e) => {
                        let msg = e.to_string();
                        self.stats_record_fetch_backend(
                            "firecrawl",
                            false,
                            t0.elapsed().as_millis() as u64,
                            Some(&msg),
                        );
                        let mut payload = serde_json::json!({
                            "ok": false,
                            "url": url,
                            "error": error_obj(
                                ErrorCode::NotConfigured,
                                msg,
                                "Set WEBPIPE_FIRECRAWL_API_KEY (or FIRECRAWL_API_KEY) to use fetch_backend=\"firecrawl\"."
                            ),
                            "request": { "fetch_backend": fetch_backend, "timeout_ms": timeout_ms }
                        });
                        add_envelope_fields(&mut payload, "web_fetch", t0.elapsed().as_millis());
                        let md = web_fetch_markdown(&payload);
                        return Ok(tool_result_markdown_with_json(payload, md));
                    }
                };
                let r = match fc.fetch_markdown(&url, timeout_ms, max_age_ms).await {
                    Ok(r) => r,
                    Err(e) => {
                        let msg = e.to_string();
                        self.stats_record_fetch_backend(
                            "firecrawl",
                            false,
                            t0.elapsed().as_millis() as u64,
                            Some(&msg),
                        );
                        let mut payload = serde_json::json!({
                            "ok": false,
                            "url": url,
                            "error": error_obj(
                                ErrorCode::FetchFailed,
                                msg,
                                "Firecrawl fetch failed. If this looks transient, retry later or reduce scope."
                            ),
                            "request": { "fetch_backend": fetch_backend, "timeout_ms": timeout_ms }
                        });
                        add_envelope_fields(&mut payload, "web_fetch", t0.elapsed().as_millis());
                        let md = web_fetch_markdown(&payload);
                        return Ok(tool_result_markdown_with_json(payload, md));
                    }
                };
                let bytes_est = Self::approx_bytes_len(&r.markdown);
                let cleaned = clean_text_for_output(r.markdown);
                let (text, n, text_clipped) = Self::truncate_to_chars(&cleaned, max_text_chars);
                let mut warnings: Vec<&'static str> = Vec::new();
                if text_clipped {
                    warnings.push("text_truncated_by_max_text_chars");
                }

                let mut payload = serde_json::json!({
                    "ok": true,
                    "fetch_backend": "firecrawl",
                    "url": url,
                    "final_url": url,
                    "status": 200,
                    "content_type": "text/markdown",
                    "bytes": bytes_est,
                    "truncated": false,
                    "source": "network",
                    "text_chars": n,
                    "text_truncated": text_clipped,
                    "timings_ms": {
                        "total": t0.elapsed().as_millis(),
                        "firecrawl_fetch": r.elapsed_ms
                    }
                });
                add_envelope_fields(&mut payload, "web_fetch", t0.elapsed().as_millis());
                payload["request"] = serde_json::json!({
                    "fetch_backend": "firecrawl",
                    "no_network": no_network,
                    "timeout_ms": timeout_ms,
                    "cache_ttl_s": args.cache_ttl_s,
                    "include_text": include_text,
                    "max_text_chars": max_text_chars,
                    "include_headers": include_headers
                });
                if include_text {
                    payload["body_text"] = serde_json::json!(text);
                }
                if include_headers {
                    payload["headers"] = serde_json::json!({});
                    warnings.push("headers_unavailable_for_firecrawl");
                }
                if !warnings.is_empty() {
                    payload["warnings"] = serde_json::json!(warnings);
                    let codes = warning_codes_from(&warnings);
                    payload["warning_codes"] = serde_json::json!(codes.clone());
                    payload["warning_hints"] = warning_hints_from(&codes);
                    self.stats_record_warnings(&warnings);
                }
                self.stats_record_fetch_backend(
                    "firecrawl",
                    true,
                    t0.elapsed().as_millis() as u64,
                    None,
                );
                let md = web_fetch_markdown(&payload);
                return Ok(tool_result_markdown_with_json(payload, md));
            }
            let req = FetchRequest {
                url: url.clone(),
                timeout_ms: args.timeout_ms.or(Some(15_000)),
                max_bytes: args.max_bytes.or(Some(5_000_000)),
                headers: BTreeMap::new(), // filled below (after filtering)
                cache: FetchCachePolicy {
                    read: args.cache_read.unwrap_or(true) || no_network,
                    write: if no_network {
                        false
                    } else {
                        args.cache_write.unwrap_or(true)
                    },
                    ttl_s: args.cache_ttl_s,
                },
            };
            // Filter user-provided request headers at the boundary so they don't affect:
            // - request execution (LocalFetcher drops them anyway by default)
            // - cache keys / on-disk cache layout
            // (values are never echoed, but hashed cache keys still count as "touching secrets")
            let allow_unsafe_headers = matches!(
                std::env::var("WEBPIPE_ALLOW_UNSAFE_HEADERS")
                    .unwrap_or_default()
                    .trim()
                    .to_ascii_lowercase()
                    .as_str(),
                "1" | "true" | "yes" | "on"
            );
            let raw_headers: BTreeMap<String, String> = args.headers.unwrap_or_default();
            let mut dropped_request_headers: Vec<&'static str> = Vec::new();
            let mut filtered_headers: BTreeMap<String, String> = BTreeMap::new();
            for (k, v) in raw_headers {
                let kl = k.trim().to_ascii_lowercase();
                let sensitive = matches!(
                    kl.as_str(),
                    "authorization" | "cookie" | "proxy-authorization"
                );
                if !allow_unsafe_headers && sensitive {
                    dropped_request_headers.push(match kl.as_str() {
                        "authorization" => "authorization",
                        "cookie" => "cookie",
                        _ => "proxy-authorization",
                    });
                    continue;
                }
                filtered_headers.insert(k, v);
            }
            dropped_request_headers.sort();
            dropped_request_headers.dedup();
            let req = FetchRequest {
                headers: filtered_headers,
                ..req
            };

            if no_network && !url_is_localhost(&req.url) {
                match self.fetcher.cache_get(&req) {
                    Ok(Some(resp)) => {
                        let is_pdf_like = Self::content_type_is_pdf(resp.content_type.as_deref())
                            || Self::url_looks_like_pdf(resp.final_url.as_str())
                            || webpipe_local::extract::bytes_look_like_pdf(&resp.bytes);
                        let (text, n, text_clipped) = if include_text && !is_pdf_like {
                            let raw = resp.text_lossy().to_string();
                            let cleaned = clean_text_for_output(raw);
                            Self::truncate_to_chars(cleaned.as_str(), max_text_chars)
                        } else {
                            (String::new(), 0, false)
                        };
                        let mut warnings: Vec<&'static str> = Vec::new();
                        if resp.truncated {
                            warnings.push("body_truncated_by_max_bytes");
                        }
                        let status_bad = resp.status >= 400;
                        if status_bad {
                            warnings.push("http_status_error");
                        }
                        if resp.status == 429 {
                            warnings.push("http_rate_limited");
                        }
                        if text_clipped {
                            warnings.push("text_truncated_by_max_text_chars");
                        }
                        if is_pdf_like && include_text {
                            warnings.push("text_unavailable_for_pdf_use_web_extract");
                        }
                        if !url_is_localhost(&req.url) {
                            warnings.push("cache_only");
                        }
                        if !dropped_request_headers.is_empty() {
                            warnings.push("unsafe_request_headers_dropped");
                        }

                        let mut payload = serde_json::json!({
                            "ok": true,
                            "fetch_backend": "local",
                            "url": resp.url,
                            "final_url": resp.final_url,
                            "status": resp.status,
                            "content_type": resp.content_type,
                            "bytes": resp.bytes.len(),
                            "truncated": resp.truncated,
                            "source": "cache",
                            "text_chars": n,
                            "text_truncated": text_clipped,
                            "timings_ms": { "total": t0.elapsed().as_millis() },
                            "request": {
                                "fetch_backend": "local",
                                "no_network": true,
                                "timeout_ms": req.timeout_ms,
                                "max_bytes": req.max_bytes,
                                "cache": { "read": true, "write": false, "ttl_s": req.cache.ttl_s },
                                "include_text": include_text,
                                "max_text_chars": max_text_chars,
                                "include_headers": include_headers
                            },
                            "warnings": warnings
                        });
                        add_envelope_fields(&mut payload, "web_fetch", t0.elapsed().as_millis());
                        if !dropped_request_headers.is_empty() {
                            payload["request"]["dropped_request_headers"] =
                                serde_json::json!(dropped_request_headers);
                        }
                        if !warnings.is_empty() {
                            let codes = warning_codes_from(&warnings);
                            payload["warning_codes"] = serde_json::json!(codes.clone());
                            payload["warning_hints"] = warning_hints_from(&codes);
                            self.stats_record_warnings(&warnings);
                        }
                        if include_text {
                            payload["body_text"] = serde_json::json!(text);
                        }
                        if include_headers {
                            payload["headers"] = serde_json::json!(resp.headers);
                        }
                        let md = web_fetch_markdown(&payload);
                        return Ok(tool_result_markdown_with_json(payload, md));
                    }
                    Ok(None) => {
                        let warns: Vec<&'static str> = vec!["no_network_may_require_warm_cache"];
                        let codes = warning_codes_from(&warns);
                        let mut payload = serde_json::json!({
                            "ok": false,
                            "url": url,
                            "error": error_obj(
                                ErrorCode::FetchFailed,
                                "cache miss in no_network mode",
                                "Warm the cache first (run without no_network), or set no_network=false."
                            ),
                            "request": { "fetch_backend": "local", "no_network": true, "cache": { "read": true, "write": false, "ttl_s": req.cache.ttl_s } },
                            "warnings": warns,
                            "warning_codes": codes.clone(),
                            "warning_hints": warning_hints_from(&codes)
                        });
                        add_envelope_fields(&mut payload, "web_fetch", t0.elapsed().as_millis());
                        let md = web_fetch_markdown(&payload);
                        return Ok(tool_result_markdown_with_json(payload, md));
                    }
                    Err(e) => {
                        let mut payload = serde_json::json!({
                            "ok": false,
                            "url": url,
                            "error": error_obj(ErrorCode::CacheError, e.to_string(), "Cache read failed in no_network mode."),
                            "request": { "fetch_backend": "local", "no_network": true }
                        });
                        add_envelope_fields(&mut payload, "web_fetch", t0.elapsed().as_millis());
                        let md = web_fetch_markdown(&payload);
                        return Ok(tool_result_markdown_with_json(payload, md));
                    }
                }
            }

            let resp = match self.fetcher.fetch(&req).await {
                Ok(r) => r,
                Err(e) => {
                    let (code, hint) = match &e {
                        WebpipeError::InvalidUrl(_) => (
                            ErrorCode::InvalidUrl,
                            "Check that the URL is absolute (includes http/https) and is well-formed.",
                        ),
                        WebpipeError::Fetch(_) => (
                            ErrorCode::FetchFailed,
                            "Fetch failed. If this looks transient, retry with a smaller max_bytes and a larger timeout_ms; otherwise try a different URL.",
                        ),
                        WebpipeError::Cache(_) => (
                            ErrorCode::CacheError,
                            "Cache failed. Retry with cache_read=false/cache_write=false to isolate network vs cache issues.",
                        ),
                        WebpipeError::Search(_) => (
                            ErrorCode::UnexpectedError,
                            "Unexpected search error while fetching. This is likely a bug; include this payload when reporting.",
                        ),
                        WebpipeError::Llm(_) => (
                            ErrorCode::UnexpectedError,
                            "Unexpected LLM error while fetching. This is likely a bug; include this payload when reporting.",
                        ),
                        WebpipeError::NotConfigured(_) => (
                            ErrorCode::NotConfigured,
                            "This fetch backend should not require API keys; check your server environment and configuration.",
                        ),
                        WebpipeError::NotSupported(_) => (
                            ErrorCode::NotSupported,
                            "This operation is not supported by the current backend/config.",
                        ),
                    };
                    let mut payload = serde_json::json!({
                        "ok": false,
                        "url": url,
                        "error": error_obj(code, e.to_string(), hint),
                        "request": {
                            "fetch_backend": fetch_backend,
                            "no_network": no_network,
                            "timeout_ms": req.timeout_ms,
                            "max_bytes": req.max_bytes,
                            "cache": { "read": req.cache.read, "write": req.cache.write, "ttl_s": req.cache.ttl_s },
                            "include_text": include_text,
                            "max_text_chars": max_text_chars,
                            "include_headers": include_headers
                        }
                    });
                    if !dropped_request_headers.is_empty() {
                        payload["request"]["dropped_request_headers"] =
                            serde_json::json!(dropped_request_headers);
                    }
                    add_envelope_fields(&mut payload, "web_fetch", t0.elapsed().as_millis());
                    let md = web_fetch_markdown(&payload);
                    return Ok(tool_result_markdown_with_json(payload, md));
                }
            };

            let is_pdf_like = Self::content_type_is_pdf(resp.content_type.as_deref())
                || Self::url_looks_like_pdf(resp.final_url.as_str())
                || webpipe_local::extract::bytes_look_like_pdf(&resp.bytes);
            let (text, n, text_clipped) = if include_text && !is_pdf_like {
                let raw = resp.text_lossy().to_string();
                let cleaned = clean_text_for_output(raw);
                Self::truncate_to_chars(cleaned.as_str(), max_text_chars)
            } else {
                (String::new(), 0, false)
            };
            let mut warnings: Vec<&'static str> = Vec::new();
            if resp.truncated {
                warnings.push("body_truncated_by_max_bytes");
            }
            if resp.timings_ms.contains_key("cache_get_timeout")
                || resp.timings_ms.contains_key("cache_put_timeout")
            {
                warnings.push("cache_io_timeout");
            }
            let status_bad = resp.status >= 400;
            if status_bad {
                warnings.push("http_status_error");
            }
            if resp.status == 429 {
                warnings.push("http_rate_limited");
            }
            if text_clipped {
                warnings.push("text_truncated_by_max_text_chars");
            }
            if is_pdf_like && include_text {
                warnings.push("text_unavailable_for_pdf_use_web_extract");
            }
            if !dropped_request_headers.is_empty() {
                warnings.push("unsafe_request_headers_dropped");
            }

            let mut payload = serde_json::json!({
                "ok": true,
                "fetch_backend": "local",
                "url": resp.url,
                "final_url": resp.final_url,
                "status": resp.status,
                "content_type": resp.content_type,
                "bytes": resp.bytes.len(),
                "truncated": resp.truncated,
                "source": match resp.source {
                    FetchSource::Cache => "cache",
                    FetchSource::Network => "network",
                },
                "text_chars": n,
                "text_truncated": text_clipped,
                "timings_ms": {
                    "total": t0.elapsed().as_millis(),
                    "cache_get": resp.timings_ms.get("cache_get").copied().unwrap_or(0),
                    "cache_get_timeout": resp.timings_ms.get("cache_get_timeout").copied().unwrap_or(0),
                    "network_fetch": resp.timings_ms.get("network_fetch").copied().unwrap_or(0),
                    "cache_put": resp.timings_ms.get("cache_put").copied().unwrap_or(0),
                    "cache_put_timeout": resp.timings_ms.get("cache_put_timeout").copied().unwrap_or(0),
                }
            });
            add_envelope_fields(&mut payload, "web_fetch", t0.elapsed().as_millis());

            payload["request"] = serde_json::json!({
                "fetch_backend": "local",
                "no_network": no_network,
                "timeout_ms": req.timeout_ms,
                "max_bytes": req.max_bytes,
                "cache": { "read": req.cache.read, "write": req.cache.write, "ttl_s": req.cache.ttl_s },
                "include_text": include_text,
                "max_text_chars": max_text_chars,
                "include_headers": include_headers
            });
            if !dropped_request_headers.is_empty() {
                payload["request"]["dropped_request_headers"] =
                    serde_json::json!(dropped_request_headers);
            }

            if include_text {
                payload["body_text"] = serde_json::json!(text);
            }

            if include_headers {
                payload["headers"] = serde_json::json!(resp.headers);
            }
            if !warnings.is_empty() {
                payload["warnings"] = serde_json::json!(warnings);
                let codes = warning_codes_from(&warnings);
                payload["warning_codes"] = serde_json::json!(codes.clone());
                payload["warning_hints"] = warning_hints_from(&codes);
                self.stats_record_warnings(&warnings);
            }
            self.stats_record_fetch_backend("local", true, t0.elapsed().as_millis() as u64, None);

            let md = web_fetch_markdown(&payload);
            Ok(tool_result_markdown_with_json(payload, md))
        }

        #[tool(
            description = "DEPRECATED: use web_fetch instead (identical args and behavior). This alias exists for backward compatibility only and is excluded from the default toolset.",
            input_schema = Arc::new(tool_input_schema_draft07::<WebFetchArgs>()),
            annotations(title = "HTTP fetch (alias)", read_only_hint = true, open_world_hint = true)
        )]
        async fn http_fetch(
            &self,
            params: Parameters<Option<WebFetchArgs>>,
        ) -> Result<CallToolResult, McpError> {
            self.web_fetch(params).await
        }

        #[tool(
            description = "Best for: getting a list of relevant URLs and snippets for a query. Not this when you also need to extract page content — use search_evidence instead. Output: results[] with url/title/snippet, provider, and selection metadata. Providers: brave, tavily, searxng, auto (picks best configured).",
            input_schema = Arc::new(tool_input_schema_draft07::<WebSearchArgs>()),
            annotations(title = "Web search", read_only_hint = true, open_world_hint = true)
        )]
        async fn web_search(
            &self,
            params: Parameters<Option<WebSearchArgs>>,
        ) -> Result<CallToolResult, McpError> {
            let args = params.0.unwrap_or_default();
            let max_results = args.max_results.unwrap_or(10).clamp(1, 20);
            let timeout_ms = args.timeout_ms.unwrap_or(20_000).min(60_000);

            let provider_name = args.provider.clone().unwrap_or_else(|| "brave".to_string());
            let auto_mode = args.auto_mode.unwrap_or_else(|| "fallback".to_string());
            let client = self.http.clone();

            // Keep these as plain values so we can reuse them across fallbacks without moving.
            let query = args.query.clone().unwrap_or_default();
            let language = args.language;
            let country = args.country;

            let t0 = std::time::Instant::now();
            self.stats_inc_tool("web_search");
            match privacy_mode_from_env() {
                PrivacyMode::Offline => {
                    let query = query.clone();
                    let mut payload = serde_json::json!({
                        "ok": false,
                        "provider": provider_name,
                        "query": query,
                        "query_key": Self::query_key(&query),
                        "max_results": max_results,
                        "request": {
                            "offline_only": true
                        },
                        "error": error_obj(
                            ErrorCode::NotSupported,
                            "offline-only mode disables web_search",
                            "Use urls=[...] + no_network=true workflows (search_evidence with urls, or web_cache_search_extract) or set WEBPIPE_OFFLINE_ONLY=0."
                        )
                    });
                    add_envelope_fields(&mut payload, "web_search", t0.elapsed().as_millis());
                    let md = web_search_markdown(&payload);
                    return Ok(tool_result_markdown_with_json(payload, md));
                }
                PrivacyMode::Anonymous => {
                    let query = query.clone();
                    let mut payload = serde_json::json!({
                        "ok": false,
                        "provider": provider_name,
                        "query": query,
                        "query_key": Self::query_key(&query),
                        "max_results": max_results,
                        "request": {
                            "privacy_mode": "anonymous"
                        },
                        "error": error_obj(
                            ErrorCode::NotSupported,
                            "anonymous mode disables web_search (API providers are deanonymizing)",
                            "Use search_evidence with urls=[...] (fetch routed via WEBPIPE_ANON_PROXY), or use web_cache_search_extract, or set WEBPIPE_PRIVACY_MODE=normal."
                        )
                    });
                    add_envelope_fields(&mut payload, "web_search", t0.elapsed().as_millis());
                    let md = web_search_markdown(&payload);
                    return Ok(tool_result_markdown_with_json(payload, md));
                }
                PrivacyMode::Normal => {}
            }
            if query.trim().is_empty() {
                let mut payload = serde_json::json!({
                    "ok": false,
                    "provider": provider_name,
                    "query": query,
                    "query_key": Self::query_key(&query),
                    "max_results": max_results,
                    "request": {
                        "provider": provider_name,
                        "auto_mode": auto_mode,
                        "query": query,
                        "query_key": Self::query_key(&query),
                        "max_results": max_results,
                        "language": language,
                        "country": country
                    },
                    "error": error_obj(
                        ErrorCode::InvalidParams,
                        "query must be non-empty",
                        "Pass a non-empty search query."
                    )
                });
                add_envelope_fields(&mut payload, "web_search", t0.elapsed().as_millis());
                let md = web_search_markdown(&payload);
                return Ok(tool_result_markdown_with_json(payload, md));
            }

            let q = SearchQuery {
                query: query.clone(),
                max_results: Some(max_results),
                language: language.clone(),
                country: country.clone(),
                timeout_ms: Some(timeout_ms),
            };
            let qk = Self::query_key(&query);

            if provider_name.as_str() != "auto"
                && provider_name.as_str() != "brave"
                && provider_name.as_str() != "tavily"
                && provider_name.as_str() != "searxng"
            {
                let mut payload = serde_json::json!({
                    "ok": false,
                    "provider": provider_name,
                    "query": query,
                    "max_results": max_results,
                    "request": {
                        "provider": provider_name,
                        "auto_mode": auto_mode,
                        "query": query,
                        "max_results": max_results,
                        "language": language,
                        "country": country
                    },
                    "error": error_obj(
                        ErrorCode::InvalidParams,
                        format!("unknown provider: {}", provider_name),
                        "provider must be one of: auto, brave, tavily, searxng"
                    )
                });
                add_envelope_fields(&mut payload, "web_search", t0.elapsed().as_millis());
                let md = web_search_markdown(&payload);
                return Ok(tool_result_markdown_with_json(payload, md));
            }

            if provider_name.as_str() == "auto"
                && auto_mode.as_str() != "fallback"
                && auto_mode.as_str() != "merge"
                && auto_mode.as_str() != "mab"
            {
                let mut payload = serde_json::json!({
                    "ok": false,
                    "provider": "auto",
                    "query": query,
                    "max_results": max_results,
                    "request": {
                        "provider": "auto",
                        "auto_mode": auto_mode,
                        "query": query,
                        "max_results": max_results,
                        "language": language,
                        "country": country
                    },
                    "error": error_obj(
                        ErrorCode::InvalidParams,
                        "unknown auto_mode",
                        "When provider=\"auto\", auto_mode must be one of: fallback, merge, mab"
                    )
                });
                add_envelope_fields(&mut payload, "web_search", t0.elapsed().as_millis());
                let md = web_search_markdown(&payload);
                return Ok(tool_result_markdown_with_json(payload, md));
            }

            let resp = match provider_name.as_str() {
                "auto" => {
                    if auto_mode.as_str() == "merge" {
                        // Merge mode: query all configured providers and dedup by URL.
                        // ok=true if at least one provider succeeds.
                        fn canonicalize_url(url: &str) -> String {
                            if let Ok(mut u) = reqwest::Url::parse(url) {
                                u.set_fragment(None);
                                return u.to_string();
                            }
                            url.trim().to_string()
                        }

                        let brave_env =
                            has_env("WEBPIPE_BRAVE_API_KEY") || has_env("BRAVE_SEARCH_API_KEY");
                        let tavily_env =
                            has_env("WEBPIPE_TAVILY_API_KEY") || has_env("TAVILY_API_KEY");
                        let searxng_env = has_env("WEBPIPE_SEARXNG_ENDPOINT")
                            || has_env("WEBPIPE_SEARXNG_ENDPOINTS");
                        if !brave_env && !tavily_env && !searxng_env {
                            let mut payload = serde_json::json!({
                                "ok": false,
                                "provider": "auto",
                                "backend_provider": "merge",
                                "query": query.clone(),
                                "max_results": max_results,
                                "selection": { "requested_provider": "auto", "auto_mode": "merge", "selected_provider": "merge" },
                                "request": { "provider": "auto", "auto_mode": "merge", "query": query.clone(), "max_results": max_results, "language": language, "country": country },
                                "providers": [],
                                "error": error_obj(
                                    ErrorCode::NotConfigured,
                                    "no web search providers configured",
                                    "Set WEBPIPE_BRAVE_API_KEY / WEBPIPE_TAVILY_API_KEY / WEBPIPE_SEARXNG_ENDPOINT(S), or choose provider explicitly."
                                )
                            });
                            add_envelope_fields(
                                &mut payload,
                                "web_search",
                                t0.elapsed().as_millis(),
                            );
                            let md = web_search_markdown(&payload);
                            return Ok(tool_result_markdown_with_json(payload, md));
                        }

                        #[derive(Debug)]
                        struct ProviderOutcome {
                            name: &'static str,
                            ok: bool,
                            cost_units: u64,
                            elapsed_ms: u64,
                            results: Vec<webpipe_core::SearchResult>,
                            error: Option<String>,
                        }

                        let brave_fut = async {
                            if !brave_env {
                                return None;
                            }
                            let pt0 = std::time::Instant::now();
                            match webpipe_local::search::BraveSearchProvider::from_env(
                                client.clone(),
                            ) {
                                Ok(p) => match p.search(&q).await {
                                    Ok(r) => Some(ProviderOutcome {
                                        name: "brave",
                                        ok: true,
                                        cost_units: r.cost_units,
                                        elapsed_ms: pt0.elapsed().as_millis() as u64,
                                        results: r.results,
                                        error: None,
                                    }),
                                    Err(e) => Some(ProviderOutcome {
                                        name: "brave",
                                        ok: false,
                                        cost_units: 0,
                                        elapsed_ms: pt0.elapsed().as_millis() as u64,
                                        results: Vec::new(),
                                        error: Some(e.to_string()),
                                    }),
                                },
                                Err(e) => Some(ProviderOutcome {
                                    name: "brave",
                                    ok: false,
                                    cost_units: 0,
                                    elapsed_ms: pt0.elapsed().as_millis() as u64,
                                    results: Vec::new(),
                                    error: Some(e.to_string()),
                                }),
                            }
                        };

                        let searxng_fut = async {
                            if !searxng_env {
                                return None;
                            }
                            let pt0 = std::time::Instant::now();
                            match webpipe_local::search::SearxngSearchProvider::from_env(
                                client.clone(),
                            ) {
                                Ok(p) => match p.search(&q).await {
                                    Ok(r) => Some(ProviderOutcome {
                                        name: "searxng",
                                        ok: true,
                                        cost_units: r.cost_units,
                                        elapsed_ms: pt0.elapsed().as_millis() as u64,
                                        results: r.results,
                                        error: None,
                                    }),
                                    Err(e) => Some(ProviderOutcome {
                                        name: "searxng",
                                        ok: false,
                                        cost_units: 0,
                                        elapsed_ms: pt0.elapsed().as_millis() as u64,
                                        results: Vec::new(),
                                        error: Some(e.to_string()),
                                    }),
                                },
                                Err(e) => Some(ProviderOutcome {
                                    name: "searxng",
                                    ok: false,
                                    cost_units: 0,
                                    elapsed_ms: pt0.elapsed().as_millis() as u64,
                                    results: Vec::new(),
                                    error: Some(e.to_string()),
                                }),
                            }
                        };

                        let tavily_fut = async {
                            if !tavily_env {
                                return None;
                            }
                            let pt0 = std::time::Instant::now();
                            match webpipe_local::search::TavilySearchProvider::from_env(
                                client.clone(),
                            ) {
                                Ok(p) => match p.search(&q).await {
                                    Ok(r) => Some(ProviderOutcome {
                                        name: "tavily",
                                        ok: true,
                                        cost_units: r.cost_units,
                                        elapsed_ms: pt0.elapsed().as_millis() as u64,
                                        results: r.results,
                                        error: None,
                                    }),
                                    Err(e) => Some(ProviderOutcome {
                                        name: "tavily",
                                        ok: false,
                                        cost_units: 0,
                                        elapsed_ms: pt0.elapsed().as_millis() as u64,
                                        results: Vec::new(),
                                        error: Some(e.to_string()),
                                    }),
                                },
                                Err(e) => Some(ProviderOutcome {
                                    name: "tavily",
                                    ok: false,
                                    cost_units: 0,
                                    elapsed_ms: pt0.elapsed().as_millis() as u64,
                                    results: Vec::new(),
                                    error: Some(e.to_string()),
                                }),
                            }
                        };

                        let (brave_out, searxng_out, tavily_out) =
                            tokio::join!(brave_fut, searxng_fut, tavily_fut);

                        let mut providers = Vec::new();
                        let mut merged: Vec<webpipe_core::SearchResult> = Vec::new();
                        let mut seen = std::collections::BTreeSet::<String>::new();
                        let mut cost_units_total: u64 = 0;

                        // Stable provider ordering for deterministic JSON.
                        let outs: Vec<ProviderOutcome> = vec![brave_out, searxng_out, tavily_out]
                            .into_iter()
                            .flatten()
                            .collect();
                        for o in &outs {
                            if o.ok {
                                cost_units_total = cost_units_total.saturating_add(o.cost_units);
                            }
                            let mut row = serde_json::json!({
                                "name": o.name,
                                "ok": o.ok,
                                "elapsed_ms": o.elapsed_ms
                            });
                            if o.ok {
                                row["cost_units"] = serde_json::json!(o.cost_units);
                            } else if let Some(e) = o.error.as_ref() {
                                row["error"] = serde_json::json!(e);
                            }
                            providers.push(row);

                            self.stats_record_search_provider_qk(
                                o.name,
                                o.ok,
                                if o.ok { o.cost_units } else { 0 },
                                o.elapsed_ms,
                                o.error.as_deref(),
                                qk.as_deref(),
                            );
                        }

                        // Merge results from all providers, bounded by max_results.
                        for o in outs {
                            if !o.ok {
                                continue;
                            }
                            for rr in o.results {
                                let key = canonicalize_url(&rr.url);
                                if seen.insert(key) {
                                    merged.push(rr);
                                }
                                if merged.len() >= max_results {
                                    break;
                                }
                            }
                            if merged.len() >= max_results {
                                break;
                            }
                        }

                        let ok_any = providers
                            .iter()
                            .any(|p| p.get("ok").and_then(|v| v.as_bool()) == Some(true));
                        if !ok_any {
                            let mut payload = serde_json::json!({
                                "ok": false,
                                "provider": "auto",
                                "backend_provider": "merge",
                                "query": query.clone(),
                                "max_results": max_results,
                                "selection": { "requested_provider": "auto", "auto_mode": "merge", "selected_provider": "merge" },
                                "request": { "provider": "auto", "auto_mode": "merge", "query": query.clone(), "max_results": max_results, "language": language, "country": country },
                                "providers": providers,
                                "error": error_obj(
                                    ErrorCode::SearchFailed,
                                    "all configured providers failed",
                                    "Inspect `providers` for per-provider errors; retry later or switch provider."
                                )
                            });
                            add_envelope_fields(
                                &mut payload,
                                "web_search",
                                t0.elapsed().as_millis(),
                            );
                            let md = web_search_markdown(&payload);
                            return Ok(tool_result_markdown_with_json(payload, md));
                        }

                        let mut payload = serde_json::json!({
                            "ok": true,
                            "provider": "auto",
                            "backend_provider": "merge",
                            "query": query.clone(),
                            "max_results": max_results,
                            "request": { "provider": "auto", "auto_mode": "merge", "query": q.query, "max_results": max_results, "language": q.language, "country": q.country },
                            "selection": { "requested_provider": "auto", "auto_mode": "merge", "selected_provider": "merge" },
                            "providers": providers,
                            "cost_units": cost_units_total,
                            "timings_ms": { "total": t0.elapsed().as_millis() },
                            "results": merged
                        });
                        let ok_all = payload["providers"]
                            .as_array()
                            .unwrap_or(&vec![])
                            .iter()
                            .all(|p| p.get("ok").and_then(|v| v.as_bool()) == Some(true));
                        if !ok_all {
                            // Partial results in merge mode: at least one provider failed.
                            // Also mark when Tavily succeeded (so callers can budget for credits).
                            let mut ws: Vec<&'static str> = vec!["partial_results"];
                            let tavily_used = payload["providers"]
                                .as_array()
                                .unwrap_or(&vec![])
                                .iter()
                                .any(|p| {
                                    p.get("name").and_then(|v| v.as_str()) == Some("tavily")
                                        && p.get("ok").and_then(|v| v.as_bool()) == Some(true)
                                });
                            if tavily_used {
                                ws.push("tavily_used");
                            }
                            payload["warnings"] = serde_json::json!(ws);
                            let codes = warning_codes_from(&ws);
                            payload["warning_codes"] = serde_json::json!(codes.clone());
                            payload["warning_hints"] = warning_hints_from(&codes);
                        }
                        add_envelope_fields(&mut payload, "web_search", t0.elapsed().as_millis());
                        let md = web_search_markdown(&payload);
                        return Ok(tool_result_markdown_with_json(payload, md));
                    }

                    if auto_mode.as_str() == "mab" {
                        // Deterministic “bandit-ish” auto selection.
                        // Goal: adapt to observed success/latency/cost without introducing randomness.
                        //
                        // Selection:
                        // - Explore each configured provider at least once (in stable order).
                        // - Then select on a Pareto frontier (multi-objective), and deterministic
                        //   scalarization.
                        fn env_f64(key: &str) -> Option<f64> {
                            std::env::var(key)
                                .ok()
                                .and_then(|v| v.trim().parse::<f64>().ok())
                        }

                        let brave_env =
                            has_env("WEBPIPE_BRAVE_API_KEY") || has_env("BRAVE_SEARCH_API_KEY");
                        let tavily_env =
                            has_env("WEBPIPE_TAVILY_API_KEY") || has_env("TAVILY_API_KEY");
                        let searxng_env = has_env("WEBPIPE_SEARXNG_ENDPOINT")
                            || has_env("WEBPIPE_SEARXNG_ENDPOINTS");
                        if !brave_env && !tavily_env && !searxng_env {
                            let mut payload = serde_json::json!({
                                "ok": false,
                                "provider": "auto",
                                "query": query.clone(),
                                "max_results": max_results,
                                "selection": { "requested_provider": "auto", "auto_mode": "mab", "selected_provider": "none" },
                                "request": { "provider": "auto", "auto_mode": "mab", "query": query.clone(), "max_results": max_results, "language": language, "country": country },
                                "error": error_obj(
                                    ErrorCode::NotConfigured,
                                    "no web search providers configured",
                                    "Set WEBPIPE_BRAVE_API_KEY / WEBPIPE_TAVILY_API_KEY / WEBPIPE_SEARXNG_ENDPOINT(S), or choose provider explicitly."
                                )
                            });
                            add_envelope_fields(
                                &mut payload,
                                "web_search",
                                t0.elapsed().as_millis(),
                            );
                            let md = web_search_markdown(&payload);
                            return Ok(tool_result_markdown_with_json(payload, md));
                        }

                        let exploration_c = env_f64("WEBPIPE_MAB_EXPLORATION_C").unwrap_or(0.7);
                        let cost_w = env_f64("WEBPIPE_MAB_COST_WEIGHT").unwrap_or(0.0);
                        let lat_w = env_f64("WEBPIPE_MAB_LATENCY_WEIGHT").unwrap_or(0.0);
                        let junk_w = env_f64("WEBPIPE_MAB_JUNK_WEIGHT").unwrap_or(0.0);
                        let hard_junk_w = env_f64("WEBPIPE_MAB_HARD_JUNK_WEIGHT").unwrap_or(0.0);
                        let max_junk_rate = env_f64("WEBPIPE_ROUTING_MAX_JUNK_RATE");
                        let max_hard_junk_rate = env_f64("WEBPIPE_ROUTING_MAX_HARD_JUNK_RATE");
                        let max_http_429_rate = env_f64("WEBPIPE_ROUTING_MAX_HTTP_429_RATE");
                        let max_mean_cost_units = env_f64("WEBPIPE_ROUTING_MAX_MEAN_COST_UNITS");

                        // Snapshot windowed summaries so we don't hold a mutex across await.
                        // If routing_context includes query_key and we have stats for this query_key, prefer them.
                        let (summaries, routing_context_used) =
                            self.snapshot_search_summaries_for_query_key(qk.as_deref());
                        let tavily_budget_units = std::env::var("WEBPIPE_TAVILY_BUDGET_UNITS")
                            .ok()
                            .and_then(|v| v.trim().parse::<u64>().ok());
                        let brave_budget_units = std::env::var("WEBPIPE_BRAVE_BUDGET_UNITS")
                            .ok()
                            .and_then(|v| v.trim().parse::<u64>().ok());
                        let provider_totals = { self.stats_lock().search_providers.clone() };

                        let searxng_eps = if searxng_env {
                            webpipe_local::search::searxng_endpoints_from_env()
                        } else {
                            Vec::new()
                        };

                        let mut order: Vec<String> = Vec::new();
                        if brave_env {
                            order.push("brave".to_string());
                        }
                        // Prefer self-hosted “free-ish” SearXNG before paid providers when available.
                        if searxng_env {
                            if searxng_eps.len() > 1 {
                                for i in 0..searxng_eps.len() {
                                    order.push(format!("searxng#{i}"));
                                }
                            } else {
                                order.push("searxng".to_string());
                            }
                        }
                        if tavily_env {
                            order.push("tavily".to_string());
                        }
                        // Budget filter (best-effort): if a budget exists and we've exceeded it, don't select that arm.
                        order.retain(|name| {
                            let spent =
                                provider_totals.get(name).map(|p| p.cost_units).unwrap_or(0);
                            match name.as_str() {
                                "tavily" => tavily_budget_units.map(|b| spent < b).unwrap_or(true),
                                "brave" => brave_budget_units.map(|b| spent < b).unwrap_or(true),
                                _ => true,
                            }
                        });
                        if order.is_empty() {
                            let mut payload = serde_json::json!({
                                "ok": false,
                                "provider": "auto",
                                "query": query.clone(),
                                "max_results": max_results,
                                "selection": { "requested_provider": "auto", "auto_mode": "mab", "selected_provider": "none" },
                                "request": { "provider": "auto", "auto_mode": "mab", "query": query.clone(), "max_results": max_results, "language": language, "country": country },
                                "error": error_obj(
                                    ErrorCode::NotSupported,
                                    "all configured providers are over budget (or filtered)",
                                    "Raise budgets, reset stats window, or choose provider explicitly."
                                )
                            });
                            add_envelope_fields(
                                &mut payload,
                                "web_search",
                                t0.elapsed().as_millis(),
                            );
                            let md = web_search_markdown(&payload);
                            return Ok(tool_result_markdown_with_json(payload, md));
                        }

                        let cfg = muxer::MabConfig {
                            exploration_c,
                            cost_weight: cost_w,
                            latency_weight: lat_w,
                            junk_weight: junk_w,
                            hard_junk_weight: hard_junk_w,
                            max_junk_rate,
                            max_hard_junk_rate,
                            max_http_429_rate,
                            max_mean_cost_units,
                        };
                        let sel = muxer::select_mab(&order, &summaries, &cfg);
                        let chosen = sel.chosen.clone();
                        let debug_rows = serde_json::to_value(&sel.candidates)
                            .unwrap_or_else(|_| serde_json::json!([]));
                        let frontier_names = serde_json::json!(sel.frontier);

                        let pt0 = std::time::Instant::now();
                        let (backend_provider, selected_arm, out) = match chosen.as_str() {
                            "tavily" => {
                                let provider =
                                    match webpipe_local::search::TavilySearchProvider::from_env(
                                        client.clone(),
                                    ) {
                                        Ok(p) => p,
                                        Err(e) => {
                                            let msg = e.to_string();
                                            self.stats_record_search_provider_qk(
                                                "tavily",
                                                false,
                                                0,
                                                pt0.elapsed().as_millis() as u64,
                                                Some(&msg),
                                                qk.as_deref(),
                                            );
                                            let mut payload = serde_json::json!({
                                                "ok": false,
                                                "provider": "auto",
                                                "backend_provider": "tavily",
                                                "query": query.clone(),
                                                "max_results": max_results,
                                                "selection": { "requested_provider": "auto", "auto_mode": "mab", "selected_provider": "tavily", "mab": { "candidates": debug_rows, "frontier": frontier_names, "routing_context_used": routing_context_used, "routing_query_key": qk } },
                                                "request": { "provider": "auto", "auto_mode": "mab", "query": query.clone(), "max_results": max_results, "language": language, "country": country },
                                                "error": error_obj(ErrorCode::NotConfigured, msg, "Tavily was selected but is not configured. Set WEBPIPE_TAVILY_API_KEY (or TAVILY_API_KEY), or use provider=brave.")
                                            });
                                            add_envelope_fields(
                                                &mut payload,
                                                "web_search",
                                                t0.elapsed().as_millis(),
                                            );
                                            let md = web_search_markdown(&payload);
                                            return Ok(tool_result_markdown_with_json(payload, md));
                                        }
                                    };
                                match provider.search(&q).await {
                                    Ok(r) => {
                                        self.stats_record_search_provider_qk(
                                            "tavily",
                                            true,
                                            r.cost_units,
                                            pt0.elapsed().as_millis() as u64,
                                            None,
                                            qk.as_deref(),
                                        );
                                        ("tavily", None, r)
                                    }
                                    Err(e) => {
                                        let msg = e.to_string();
                                        self.stats_record_search_provider_qk(
                                            "tavily",
                                            false,
                                            0,
                                            pt0.elapsed().as_millis() as u64,
                                            Some(&msg),
                                            qk.as_deref(),
                                        );
                                        let hint = search_failed_hint(
                                            "tavily",
                                            &msg,
                                            "Auto (mab) selected Tavily, but it failed. Retry later; reduce max_results; or use provider=brave.",
                                        );
                                        let mut payload = serde_json::json!({
                                            "ok": false,
                                            "provider": "auto",
                                            "backend_provider": "tavily",
                                            "query": query.clone(),
                                            "max_results": max_results,
                                            "selection": { "requested_provider": "auto", "auto_mode": "mab", "selected_provider": "tavily", "mab": { "candidates": debug_rows, "frontier": frontier_names, "routing_context_used": routing_context_used, "routing_query_key": qk } },
                                            "request": { "provider": "auto", "auto_mode": "mab", "query": query.clone(), "max_results": max_results, "language": language, "country": country },
                                            "error": error_obj(ErrorCode::SearchFailed, msg, hint)
                                        });
                                        add_envelope_fields(
                                            &mut payload,
                                            "web_search",
                                            t0.elapsed().as_millis(),
                                        );
                                        let md = web_search_markdown(&payload);
                                        return Ok(tool_result_markdown_with_json(payload, md));
                                    }
                                }
                            }
                            s if s == "searxng" || s.starts_with("searxng#") => {
                                let arm_idx = parse_searxng_arm(s);
                                let stats_key = s.to_string();

                                let run = async {
                                    if let Some(i) = arm_idx {
                                        let Some(ep) = searxng_eps.get(i) else {
                                            return Err(WebpipeError::NotSupported(format!(
                                                "searxng arm index out of range: {i}"
                                            )));
                                        };
                                        webpipe_local::search::searxng_search_at_endpoint(
                                            &client, ep, &q,
                                        )
                                        .await
                                    } else if searxng_eps.len() == 1 {
                                        webpipe_local::search::searxng_search_at_endpoint(
                                            &client,
                                            &searxng_eps[0],
                                            &q,
                                        )
                                        .await
                                    } else {
                                        let p =
                                            webpipe_local::search::SearxngSearchProvider::from_env(
                                                client.clone(),
                                            )?;
                                        p.search(&q).await
                                    }
                                };

                                match run.await {
                                    Ok(r) => {
                                        self.stats_record_search_provider_qk(
                                            &stats_key,
                                            true,
                                            r.cost_units,
                                            pt0.elapsed().as_millis() as u64,
                                            None,
                                            qk.as_deref(),
                                        );
                                        ("searxng", arm_idx.map(|i| format!("searxng#{i}")), r)
                                    }
                                    Err(e) => {
                                        let msg = e.to_string();
                                        self.stats_record_search_provider_qk(
                                            &stats_key,
                                            false,
                                            0,
                                            pt0.elapsed().as_millis() as u64,
                                            Some(&msg),
                                            qk.as_deref(),
                                        );
                                        let hint = search_failed_hint(
                                            "searxng",
                                            &msg,
                                            "Auto (mab) selected SearXNG, but it failed. Retry later; reduce max_results; or use urls=[...] to skip search.",
                                        );
                                        let mut payload = serde_json::json!({
                                            "ok": false,
                                            "provider": "auto",
                                            "backend_provider": "searxng",
                                            "query": query.clone(),
                                            "max_results": max_results,
                                            "selection": { "requested_provider": "auto", "auto_mode": "mab", "selected_provider": "searxng", "mab": { "candidates": debug_rows, "frontier": frontier_names, "routing_context_used": routing_context_used, "routing_query_key": qk } },
                                            "request": { "provider": "auto", "auto_mode": "mab", "query": query.clone(), "max_results": max_results, "language": language, "country": country },
                                            "error": error_obj(ErrorCode::SearchFailed, msg, hint)
                                        });
                                        if let Some(i) = arm_idx {
                                            payload["selection"]["selected_arm"] =
                                                serde_json::json!(format!("searxng#{i}"));
                                        }
                                        add_envelope_fields(
                                            &mut payload,
                                            "web_search",
                                            t0.elapsed().as_millis(),
                                        );
                                        let md = web_search_markdown(&payload);
                                        return Ok(tool_result_markdown_with_json(payload, md));
                                    }
                                }
                            }
                            _ => {
                                let provider =
                                    match webpipe_local::search::BraveSearchProvider::from_env(
                                        client.clone(),
                                    ) {
                                        Ok(p) => p,
                                        Err(e) => {
                                            let msg = e.to_string();
                                            self.stats_record_search_provider_qk(
                                                "brave",
                                                false,
                                                0,
                                                pt0.elapsed().as_millis() as u64,
                                                Some(&msg),
                                                qk.as_deref(),
                                            );
                                            let mut payload = serde_json::json!({
                                                "ok": false,
                                                "provider": "auto",
                                                "backend_provider": "brave",
                                                "query": query.clone(),
                                                "max_results": max_results,
                                                "selection": { "requested_provider": "auto", "auto_mode": "mab", "selected_provider": "brave", "mab": { "candidates": debug_rows, "frontier": frontier_names, "routing_context_used": routing_context_used, "routing_query_key": qk } },
                                                "request": { "provider": "auto", "auto_mode": "mab", "query": query.clone(), "max_results": max_results, "language": language, "country": country },
                                                "error": error_obj(ErrorCode::NotConfigured, msg, "Brave was selected but is not configured. Set WEBPIPE_BRAVE_API_KEY (or BRAVE_SEARCH_API_KEY), or use provider=tavily.")
                                            });
                                            add_envelope_fields(
                                                &mut payload,
                                                "web_search",
                                                t0.elapsed().as_millis(),
                                            );
                                            let md = web_search_markdown(&payload);
                                            return Ok(tool_result_markdown_with_json(payload, md));
                                        }
                                    };
                                match provider.search(&q).await {
                                    Ok(r) => {
                                        self.stats_record_search_provider_qk(
                                            "brave",
                                            true,
                                            r.cost_units,
                                            pt0.elapsed().as_millis() as u64,
                                            None,
                                            qk.as_deref(),
                                        );
                                        ("brave", None, r)
                                    }
                                    Err(e) => {
                                        let msg = e.to_string();
                                        self.stats_record_search_provider_qk(
                                            "brave",
                                            false,
                                            0,
                                            pt0.elapsed().as_millis() as u64,
                                            Some(&msg),
                                            qk.as_deref(),
                                        );
                                        let hint = search_failed_hint(
                                            "brave",
                                            &msg,
                                            "Auto (mab) selected Brave, but it failed. Retry later; reduce max_results; or use provider=tavily.",
                                        );
                                        let mut payload = serde_json::json!({
                                            "ok": false,
                                            "provider": "auto",
                                            "backend_provider": "brave",
                                            "query": query.clone(),
                                            "max_results": max_results,
                                            "selection": { "requested_provider": "auto", "auto_mode": "mab", "selected_provider": "brave", "mab": { "candidates": debug_rows, "frontier": frontier_names, "routing_context_used": routing_context_used, "routing_query_key": qk } },
                                            "request": { "provider": "auto", "auto_mode": "mab", "query": query.clone(), "max_results": max_results, "language": language, "country": country },
                                            "error": error_obj(ErrorCode::SearchFailed, msg, hint)
                                        });
                                        add_envelope_fields(
                                            &mut payload,
                                            "web_search",
                                            t0.elapsed().as_millis(),
                                        );
                                        let md = web_search_markdown(&payload);
                                        return Ok(tool_result_markdown_with_json(payload, md));
                                    }
                                }
                            }
                        };

                        let mut payload = serde_json::json!({
                            "ok": true,
                            "provider": "auto",
                            "backend_provider": backend_provider,
                            "query": query.clone(),
                            "query_key": Self::query_key(&query),
                            "max_results": max_results,
                            "request": { "provider": "auto", "auto_mode": "mab", "query": q.query, "query_key": Self::query_key(&q.query), "max_results": max_results, "language": q.language, "country": q.country },
                            "selection": { "requested_provider": "auto", "selected_provider": backend_provider, "auto_mode": "mab", "mab": { "candidates": debug_rows, "frontier": frontier_names, "routing_context_used": routing_context_used, "routing_query_key": qk, "exploration_c": exploration_c, "cost_weight": cost_w, "latency_weight": lat_w, "junk_weight": junk_w, "hard_junk_weight": hard_junk_w, "constraints": { "max_junk_rate": max_junk_rate, "max_hard_junk_rate": max_hard_junk_rate, "max_http_429_rate": max_http_429_rate, "max_mean_cost_units": max_mean_cost_units } } },
                            "cost_units": out.cost_units,
                            "timings_ms": { "total": t0.elapsed().as_millis() },
                            "results": out.results
                        });
                        if let Some(arm) = selected_arm {
                            payload["selection"]["selected_arm"] = serde_json::json!(arm);
                        }
                        if backend_provider == "tavily" {
                            payload["warnings"] = serde_json::json!(["tavily_used"]);
                            let codes = warning_codes_from(&["tavily_used"]);
                            payload["warning_codes"] = serde_json::json!(codes.clone());
                            payload["warning_hints"] = warning_hints_from(&codes);
                        }
                        add_envelope_fields(&mut payload, "web_search", t0.elapsed().as_millis());
                        let md = web_search_markdown(&payload);
                        return Ok(tool_result_markdown_with_json(payload, md));
                    }

                    // Fallback mode: “MAB with failover”.
                    //
                    // Build a deterministic provider order based on what's configured, then use the
                    // same MAB selection logic to pick a provider. If that provider fails, retry with
                    // the next best remaining provider, and so on (bounded by configured providers).
                    //
                    // This makes fallback variable-length and “as-MAB-as-possible” without changing
                    // the public `auto_mode` surface.
                    fn env_f64(key: &str) -> Option<f64> {
                        std::env::var(key)
                            .ok()
                            .and_then(|v| v.trim().parse::<f64>().ok())
                    }

                    let brave_env =
                        has_env("WEBPIPE_BRAVE_API_KEY") || has_env("BRAVE_SEARCH_API_KEY");
                    let tavily_env = has_env("WEBPIPE_TAVILY_API_KEY") || has_env("TAVILY_API_KEY");
                    let searxng_env =
                        has_env("WEBPIPE_SEARXNG_ENDPOINT") || has_env("WEBPIPE_SEARXNG_ENDPOINTS");

                    let searxng_eps = if searxng_env {
                        webpipe_local::search::searxng_endpoints_from_env()
                    } else {
                        Vec::new()
                    };

                    let mut order: Vec<String> = Vec::new();
                    if brave_env {
                        order.push("brave".to_string());
                    }
                    if searxng_env {
                        if searxng_eps.len() > 1 {
                            for i in 0..searxng_eps.len() {
                                order.push(format!("searxng#{i}"));
                            }
                        } else {
                            order.push("searxng".to_string());
                        }
                    }
                    if tavily_env {
                        order.push("tavily".to_string());
                    }

                    if order.is_empty() {
                        let mut payload = serde_json::json!({
                            "ok": false,
                            "provider": "auto",
                            "query": query.clone(),
                            "max_results": max_results,
                            "selection": { "requested_provider": "auto", "auto_mode": auto_mode, "selected_provider": "none" },
                            "request": { "provider": "auto", "auto_mode": auto_mode, "query": query.clone(), "max_results": max_results, "language": language, "country": country },
                            "error": error_obj(
                                ErrorCode::NotConfigured,
                                "no web search providers configured",
                            "Set WEBPIPE_BRAVE_API_KEY / WEBPIPE_TAVILY_API_KEY / WEBPIPE_SEARXNG_ENDPOINT(S), or choose provider explicitly."
                            )
                        });
                        add_envelope_fields(&mut payload, "web_search", t0.elapsed().as_millis());
                        return Ok(tool_result(payload));
                    }

                    // Budget filter (best-effort): keep the same semantics as MAB mode.
                    let tavily_budget_units = std::env::var("WEBPIPE_TAVILY_BUDGET_UNITS")
                        .ok()
                        .and_then(|v| v.trim().parse::<u64>().ok());
                    let brave_budget_units = std::env::var("WEBPIPE_BRAVE_BUDGET_UNITS")
                        .ok()
                        .and_then(|v| v.trim().parse::<u64>().ok());
                    let provider_totals = { self.stats_lock().search_providers.clone() };
                    order.retain(|name| {
                        let spent = provider_totals.get(name).map(|p| p.cost_units).unwrap_or(0);
                        match name.as_str() {
                            "tavily" => tavily_budget_units.map(|b| spent < b).unwrap_or(true),
                            "brave" => brave_budget_units.map(|b| spent < b).unwrap_or(true),
                            _ => true,
                        }
                    });

                    if order.is_empty() {
                        let mut payload = serde_json::json!({
                            "ok": false,
                            "provider": "auto",
                            "query": query.clone(),
                            "max_results": max_results,
                            "selection": { "requested_provider": "auto", "auto_mode": auto_mode, "selected_provider": "none" },
                            "request": { "provider": "auto", "auto_mode": auto_mode, "query": query.clone(), "max_results": max_results, "language": language, "country": country },
                            "error": error_obj(
                                ErrorCode::NotSupported,
                                "all configured providers are over budget (or filtered)",
                                "Raise budgets, reset stats window, or choose provider explicitly."
                            )
                        });
                        add_envelope_fields(&mut payload, "web_search", t0.elapsed().as_millis());
                        return Ok(tool_result(payload));
                    }

                    let (summaries, routing_context_used) =
                        self.snapshot_search_summaries_for_query_key(qk.as_deref());

                    let cfg = muxer::MabConfig {
                        exploration_c: env_f64("WEBPIPE_MAB_EXPLORATION_C").unwrap_or(0.7),
                        cost_weight: env_f64("WEBPIPE_MAB_COST_WEIGHT").unwrap_or(0.0),
                        latency_weight: env_f64("WEBPIPE_MAB_LATENCY_WEIGHT").unwrap_or(0.0),
                        junk_weight: env_f64("WEBPIPE_MAB_JUNK_WEIGHT").unwrap_or(0.0),
                        hard_junk_weight: env_f64("WEBPIPE_MAB_HARD_JUNK_WEIGHT").unwrap_or(0.0),
                        max_junk_rate: env_f64("WEBPIPE_ROUTING_MAX_JUNK_RATE"),
                        max_hard_junk_rate: env_f64("WEBPIPE_ROUTING_MAX_HARD_JUNK_RATE"),
                        max_http_429_rate: env_f64("WEBPIPE_ROUTING_MAX_HTTP_429_RATE"),
                        max_mean_cost_units: env_f64("WEBPIPE_ROUTING_MAX_MEAN_COST_UNITS"),
                    };

                    // Re-run MAB after each failure, with locally-updated summaries.
                    // This makes fallback responsive within a single call (e.g. 429 -> immediate deprioritization).
                    let mut summaries_local = summaries.clone();
                    let mut remaining = order.clone();

                    // Capture the selection-debug for the first attempt (usually what you want to inspect).
                    let sel0 = muxer::select_mab(&remaining, &summaries_local, &cfg);
                    let debug_rows0 = serde_json::to_value(&sel0.candidates)
                        .unwrap_or_else(|_| serde_json::json!([]));
                    let frontier0 = serde_json::json!(sel0.frontier.clone());

                    let mut attempts: Vec<serde_json::Value> = Vec::new();
                    let mut attempted_chain: Vec<String> = Vec::new();
                    while !remaining.is_empty() {
                        let sel = muxer::select_mab(&remaining, &summaries_local, &cfg);
                        let chosen = sel.chosen.clone();
                        if chosen.is_empty() {
                            break;
                        }
                        attempted_chain.push(chosen.clone());

                        let pt0 = std::time::Instant::now();
                        match chosen.as_str() {
                            "brave" => match webpipe_local::search::BraveSearchProvider::from_env(
                                client.clone(),
                            ) {
                                Ok(p) => match p.search(&q).await {
                                    Ok(r) => {
                                        attempts.push(serde_json::json!({"name":"brave","ok":true,"cost_units":r.cost_units,"elapsed_ms":pt0.elapsed().as_millis()}));
                                        self.stats_record_search_provider_qk(
                                            "brave",
                                            true,
                                            r.cost_units,
                                            pt0.elapsed().as_millis() as u64,
                                            None,
                                            qk.as_deref(),
                                        );
                                        let mut payload = serde_json::json!({
                                            "ok": true,
                                            "provider": "auto",
                                            "backend_provider": "brave",
                                            "query": query.clone(),
                                            "query_key": Self::query_key(&query),
                                            "max_results": max_results,
                                            "request": { "provider": "auto", "auto_mode": auto_mode, "query": q.query, "query_key": Self::query_key(&q.query), "max_results": max_results, "language": q.language, "country": q.country },
                                            "selection": { "requested_provider": "auto", "selected_provider": "brave", "auto_mode": auto_mode, "mab": { "candidates": debug_rows0, "frontier": frontier0, "routing_context_used": routing_context_used, "routing_query_key": qk, "attempted_chain": attempted_chain } },
                                            "providers": attempts,
                                            "cost_units": r.cost_units,
                                            "timings_ms": { "total": t0.elapsed().as_millis() },
                                            "results": r.results,
                                        });
                                        if payload["providers"]
                                            .as_array()
                                            .map(|a| a.len())
                                            .unwrap_or(0)
                                            >= 2
                                        {
                                            payload["warnings"] =
                                                serde_json::json!(["provider_failover"]);
                                            let codes = warning_codes_from(&["provider_failover"]);
                                            payload["warning_codes"] =
                                                serde_json::json!(codes.clone());
                                            payload["warning_hints"] = warning_hints_from(&codes);
                                        }
                                        add_envelope_fields(
                                            &mut payload,
                                            "web_search",
                                            t0.elapsed().as_millis(),
                                        );
                                        return Ok(tool_result(payload));
                                    }
                                    Err(e) => {
                                        let msg = e.to_string();
                                        let elapsed_ms = pt0.elapsed().as_millis() as u64;
                                        let http_429 = is_http_status(&msg, 429);
                                        attempts.push(serde_json::json!({"name":"brave","ok":false,"error":msg.clone(),"elapsed_ms":elapsed_ms}));
                                        self.stats_record_search_provider_qk(
                                            "brave",
                                            false,
                                            0,
                                            elapsed_ms,
                                            Some(&msg),
                                            qk.as_deref(),
                                        );
                                        // Update local summaries so the next selection sees the failure immediately.
                                        let s =
                                            summaries_local.entry("brave".to_string()).or_default();
                                        s.calls = s.calls.saturating_add(1);
                                        s.http_429 = s.http_429.saturating_add(http_429 as u64);
                                        s.elapsed_ms_sum =
                                            s.elapsed_ms_sum.saturating_add(elapsed_ms);
                                    }
                                },
                                Err(WebpipeError::NotConfigured(msg)) => {
                                    let elapsed_ms = pt0.elapsed().as_millis() as u64;
                                    let http_429 = is_http_status(&msg, 429);
                                    attempts.push(serde_json::json!({"name":"brave","ok":false,"error":msg.clone(),"elapsed_ms":elapsed_ms}));
                                    self.stats_record_search_provider_qk(
                                        "brave",
                                        false,
                                        0,
                                        elapsed_ms,
                                        Some(&msg),
                                        qk.as_deref(),
                                    );
                                    let s = summaries_local.entry("brave".to_string()).or_default();
                                    s.calls = s.calls.saturating_add(1);
                                    s.http_429 = s.http_429.saturating_add(http_429 as u64);
                                    s.elapsed_ms_sum = s.elapsed_ms_sum.saturating_add(elapsed_ms);
                                }
                                Err(e) => {
                                    return Err(McpError::internal_error(e.to_string(), None))
                                }
                            },
                            s if s == "searxng" || s.starts_with("searxng#") => {
                                let arm_idx = parse_searxng_arm(s);
                                let stats_key = s.to_string();

                                let run = async {
                                    if let Some(i) = arm_idx {
                                        let Some(ep) = searxng_eps.get(i) else {
                                            return Err(WebpipeError::NotSupported(format!(
                                                "searxng arm index out of range: {i}"
                                            )));
                                        };
                                        webpipe_local::search::searxng_search_at_endpoint(
                                            &client, ep, &q,
                                        )
                                        .await
                                    } else if searxng_eps.len() == 1 {
                                        webpipe_local::search::searxng_search_at_endpoint(
                                            &client,
                                            &searxng_eps[0],
                                            &q,
                                        )
                                        .await
                                    } else {
                                        let p =
                                            webpipe_local::search::SearxngSearchProvider::from_env(
                                                client.clone(),
                                            )?;
                                        p.search(&q).await
                                    }
                                };

                                match run.await {
                                    Ok(r) => {
                                        let mut entry = serde_json::json!({"name":"searxng","ok":true,"cost_units":r.cost_units,"elapsed_ms":pt0.elapsed().as_millis()});
                                        if let Some(i) = arm_idx {
                                            entry["arm"] =
                                                serde_json::json!(format!("searxng#{i}"));
                                        }
                                        attempts.push(entry);
                                        self.stats_record_search_provider_qk(
                                            &stats_key,
                                            true,
                                            r.cost_units,
                                            pt0.elapsed().as_millis() as u64,
                                            None,
                                            qk.as_deref(),
                                        );
                                        let mut payload = serde_json::json!({
                                            "ok": true,
                                            "provider": "auto",
                                            "backend_provider": "searxng",
                                            "query": query.clone(),
                                            "query_key": Self::query_key(&query),
                                            "max_results": max_results,
                                            "request": { "provider": "auto", "auto_mode": auto_mode, "query": q.query, "query_key": Self::query_key(&q.query), "max_results": max_results, "language": q.language, "country": q.country },
                                            "selection": { "requested_provider": "auto", "selected_provider": "searxng", "auto_mode": auto_mode, "mab": { "candidates": debug_rows0, "frontier": frontier0, "routing_context_used": routing_context_used, "routing_query_key": qk, "attempted_chain": attempted_chain } },
                                            "providers": attempts,
                                            "cost_units": r.cost_units,
                                            "timings_ms": { "total": t0.elapsed().as_millis() },
                                            "results": r.results,
                                        });
                                        if let Some(i) = arm_idx {
                                            payload["selection"]["selected_arm"] =
                                                serde_json::json!(format!("searxng#{i}"));
                                        }
                                        if payload["providers"]
                                            .as_array()
                                            .map(|a| a.len())
                                            .unwrap_or(0)
                                            >= 2
                                        {
                                            payload["warnings"] =
                                                serde_json::json!(["provider_failover"]);
                                            let codes = warning_codes_from(&["provider_failover"]);
                                            payload["warning_codes"] =
                                                serde_json::json!(codes.clone());
                                            payload["warning_hints"] = warning_hints_from(&codes);
                                        }
                                        add_envelope_fields(
                                            &mut payload,
                                            "web_search",
                                            t0.elapsed().as_millis(),
                                        );
                                        return Ok(tool_result(payload));
                                    }
                                    Err(e) => {
                                        let msg = e.to_string();
                                        let elapsed_ms = pt0.elapsed().as_millis() as u64;
                                        let http_429 = is_http_status(&msg, 429);
                                        let mut entry = serde_json::json!({"name":"searxng","ok":false,"error":msg.clone(),"elapsed_ms":elapsed_ms});
                                        if let Some(i) = arm_idx {
                                            entry["arm"] =
                                                serde_json::json!(format!("searxng#{i}"));
                                        }
                                        attempts.push(entry);
                                        self.stats_record_search_provider_qk(
                                            &stats_key,
                                            false,
                                            0,
                                            elapsed_ms,
                                            Some(&msg),
                                            qk.as_deref(),
                                        );
                                        let s = summaries_local.entry(stats_key).or_default();
                                        s.calls = s.calls.saturating_add(1);
                                        s.http_429 = s.http_429.saturating_add(http_429 as u64);
                                        s.elapsed_ms_sum =
                                            s.elapsed_ms_sum.saturating_add(elapsed_ms);
                                    }
                                }
                            }
                            "tavily" => {
                                match webpipe_local::search::TavilySearchProvider::from_env(
                                    client.clone(),
                                ) {
                                    Ok(p) => match p.search(&q).await {
                                        Ok(r) => {
                                            attempts.push(serde_json::json!({"name":"tavily","ok":true,"cost_units":r.cost_units,"elapsed_ms":pt0.elapsed().as_millis()}));
                                            self.stats_record_search_provider_qk(
                                                "tavily",
                                                true,
                                                r.cost_units,
                                                pt0.elapsed().as_millis() as u64,
                                                None,
                                                qk.as_deref(),
                                            );
                                            let mut ws: Vec<&'static str> = Vec::new();
                                            if attempts.len() >= 2 {
                                                ws.push("provider_failover");
                                            }
                                            ws.push("tavily_used");
                                            let mut payload = serde_json::json!({
                                                "ok": true,
                                                "provider": "auto",
                                                "backend_provider": "tavily",
                                                "query": query.clone(),
                                                "query_key": Self::query_key(&query),
                                                "max_results": max_results,
                                                "request": { "provider": "auto", "auto_mode": auto_mode, "query": q.query, "query_key": Self::query_key(&q.query), "max_results": max_results, "language": q.language, "country": q.country },
                                                "selection": { "requested_provider": "auto", "selected_provider": "tavily", "auto_mode": auto_mode, "mab": { "candidates": debug_rows0, "frontier": frontier0, "routing_context_used": routing_context_used, "routing_query_key": qk, "attempted_chain": attempted_chain } },
                                                "providers": attempts,
                                                "warnings": ws,
                                                "cost_units": r.cost_units,
                                                "timings_ms": { "total": t0.elapsed().as_millis() },
                                                "results": r.results,
                                            });
                                            let codes = warning_codes_from(&ws);
                                            payload["warning_codes"] =
                                                serde_json::json!(codes.clone());
                                            payload["warning_hints"] = warning_hints_from(&codes);
                                            add_envelope_fields(
                                                &mut payload,
                                                "web_search",
                                                t0.elapsed().as_millis(),
                                            );
                                            return Ok(tool_result(payload));
                                        }
                                        Err(e) => {
                                            let msg = e.to_string();
                                            let elapsed_ms = pt0.elapsed().as_millis() as u64;
                                            let http_429 = is_http_status(&msg, 429);
                                            attempts.push(serde_json::json!({"name":"tavily","ok":false,"error":msg.clone(),"elapsed_ms":elapsed_ms}));
                                            self.stats_record_search_provider_qk(
                                                "tavily",
                                                false,
                                                0,
                                                elapsed_ms,
                                                Some(&msg),
                                                qk.as_deref(),
                                            );
                                            let s = summaries_local
                                                .entry("tavily".to_string())
                                                .or_default();
                                            s.calls = s.calls.saturating_add(1);
                                            s.http_429 = s.http_429.saturating_add(http_429 as u64);
                                            s.elapsed_ms_sum =
                                                s.elapsed_ms_sum.saturating_add(elapsed_ms);
                                        }
                                    },
                                    Err(WebpipeError::NotConfigured(msg)) => {
                                        let elapsed_ms = pt0.elapsed().as_millis() as u64;
                                        let http_429 = is_http_status(&msg, 429);
                                        attempts.push(serde_json::json!({"name":"tavily","ok":false,"error":msg.clone(),"elapsed_ms":elapsed_ms}));
                                        self.stats_record_search_provider_qk(
                                            "tavily",
                                            false,
                                            0,
                                            elapsed_ms,
                                            Some(&msg),
                                            qk.as_deref(),
                                        );
                                        let s = summaries_local
                                            .entry("tavily".to_string())
                                            .or_default();
                                        s.calls = s.calls.saturating_add(1);
                                        s.http_429 = s.http_429.saturating_add(http_429 as u64);
                                        s.elapsed_ms_sum =
                                            s.elapsed_ms_sum.saturating_add(elapsed_ms);
                                    }
                                    Err(e) => {
                                        return Err(McpError::internal_error(e.to_string(), None))
                                    }
                                }
                            }
                            other => {
                                attempts.push(serde_json::json!({"name": other, "ok": false, "error": "unknown provider"}));
                            }
                        }

                        // Never retry the same provider within one call.
                        remaining.retain(|x| x != &chosen);
                    }

                    let hint = if attempts.iter().any(|a| {
                        a.get("error")
                            .and_then(|v| v.as_str())
                            .is_some_and(|s| is_http_status(s, 429))
                    }) {
                        "At least one provider is rate-limiting (HTTP 429). Retry later; reduce max_results; or use urls=[...] to skip search."
                    } else {
                        "All configured providers failed. Inspect `providers` for per-provider errors; retry later or switch provider."
                    };

                    let mut payload = serde_json::json!({
                        "ok": false,
                        "provider": "auto",
                        "backend_provider": "fallback",
                        "query": query.clone(),
                        "query_key": Self::query_key(&query),
                        "max_results": max_results,
                        "request": { "provider": "auto", "auto_mode": auto_mode, "query": q.query, "query_key": Self::query_key(&q.query), "max_results": max_results, "language": q.language, "country": q.country },
                        "selection": { "requested_provider": "auto", "auto_mode": auto_mode, "selected_provider": "none", "mab": { "candidates": debug_rows0, "frontier": frontier0, "routing_context_used": routing_context_used, "routing_query_key": qk, "attempted_chain": attempted_chain } },
                        "providers": attempts,
                        "error": error_obj(
                            ErrorCode::SearchFailed,
                            "all configured providers failed (auto fallback)",
                            hint
                        )
                    });
                    add_envelope_fields(&mut payload, "web_search", t0.elapsed().as_millis());
                    return Ok(tool_result(payload));
                }
                "brave" => {
                    let pt0 = std::time::Instant::now();
                    let provider = match webpipe_local::search::BraveSearchProvider::from_env(
                        client.clone(),
                    ) {
                        Ok(p) => p,
                        Err(WebpipeError::NotConfigured(msg)) => {
                            self.stats_record_search_provider_qk(
                                "brave",
                                false,
                                0,
                                pt0.elapsed().as_millis() as u64,
                                Some(&msg),
                                qk.as_deref(),
                            );
                            let mut payload = serde_json::json!({
                                "ok": false,
                                "provider": "brave",
                                "query": query.clone(),
                                "max_results": max_results,
                                "request": { "provider": "brave", "auto_mode": auto_mode, "query": query.clone(), "max_results": max_results, "language": language, "country": country },
                                "error": error_obj(
                                    ErrorCode::NotConfigured,
                                    msg,
                                    "Set WEBPIPE_BRAVE_API_KEY (or BRAVE_SEARCH_API_KEY) in the webpipe MCP server environment."
                                )
                            });
                            add_envelope_fields(
                                &mut payload,
                                "web_search",
                                t0.elapsed().as_millis(),
                            );
                            return Ok(tool_result(payload));
                        }
                        Err(e) => return Err(McpError::internal_error(e.to_string(), None)),
                    };
                    match provider.search(&q).await {
                        Ok(r) => {
                            self.stats_record_search_provider_qk(
                                "brave",
                                true,
                                r.cost_units,
                                pt0.elapsed().as_millis() as u64,
                                None,
                                qk.as_deref(),
                            );
                            r
                        }
                        Err(e) => {
                            let msg = e.to_string();
                            self.stats_record_search_provider_qk(
                                "brave",
                                false,
                                0,
                                pt0.elapsed().as_millis() as u64,
                                Some(&msg),
                                qk.as_deref(),
                            );
                            let hint = search_failed_hint(
                                "brave",
                                &msg,
                                "Brave search failed. If this is a transient HTTP error, retry with smaller max_results; otherwise verify WEBPIPE_BRAVE_API_KEY.",
                            );
                            let mut payload = serde_json::json!({
                                "ok": false,
                                "provider": "brave",
                                "query": query.clone(),
                                "max_results": max_results,
                                "request": { "provider": "brave", "auto_mode": auto_mode, "query": query.clone(), "max_results": max_results, "language": language, "country": country },
                                "error": error_obj(
                                    ErrorCode::SearchFailed,
                                    msg,
                                    hint
                                )
                            });
                            add_envelope_fields(
                                &mut payload,
                                "web_search",
                                t0.elapsed().as_millis(),
                            );
                            return Ok(tool_result(payload));
                        }
                    }
                }
                "tavily" => {
                    let pt0 = std::time::Instant::now();
                    let provider = match webpipe_local::search::TavilySearchProvider::from_env(
                        client.clone(),
                    ) {
                        Ok(p) => p,
                        Err(WebpipeError::NotConfigured(msg)) => {
                            self.stats_record_search_provider_qk(
                                "tavily",
                                false,
                                0,
                                pt0.elapsed().as_millis() as u64,
                                Some(&msg),
                                qk.as_deref(),
                            );
                            let mut payload = serde_json::json!({
                                "ok": false,
                                "provider": "tavily",
                                "query": query.clone(),
                                "max_results": max_results,
                                "request": { "provider": "tavily", "auto_mode": auto_mode, "query": query.clone(), "max_results": max_results, "language": language, "country": country },
                                "error": error_obj(
                                    ErrorCode::NotConfigured,
                                    msg,
                                    "Set WEBPIPE_TAVILY_API_KEY (or TAVILY_API_KEY) in the webpipe MCP server environment."
                                )
                            });
                            add_envelope_fields(
                                &mut payload,
                                "web_search",
                                t0.elapsed().as_millis(),
                            );
                            return Ok(tool_result(payload));
                        }
                        Err(e) => return Err(McpError::internal_error(e.to_string(), None)),
                    };
                    match provider.search(&q).await {
                        Ok(r) => {
                            self.stats_record_search_provider_qk(
                                "tavily",
                                true,
                                r.cost_units,
                                pt0.elapsed().as_millis() as u64,
                                None,
                                qk.as_deref(),
                            );
                            r
                        }
                        Err(e) => {
                            // In this environment, Tavily can return HTTP 433 even when configured.
                            // Prefer a bounded fallback to Brave if available.
                            let msg = e.to_string();
                            self.stats_record_search_provider_qk(
                                "tavily",
                                false,
                                0,
                                pt0.elapsed().as_millis() as u64,
                                Some(&msg),
                                qk.as_deref(),
                            );
                            if msg.contains("HTTP 433") {
                                if let Ok(brave) =
                                    webpipe_local::search::BraveSearchProvider::from_env(
                                        client.clone(),
                                    )
                                {
                                    let r = match brave.search(&q).await {
                                        Ok(r) => {
                                            self.stats_record_search_provider_qk(
                                                "brave",
                                                true,
                                                r.cost_units,
                                                pt0.elapsed().as_millis() as u64,
                                                None,
                                                qk.as_deref(),
                                            );
                                            r
                                        }
                                        Err(e2) => {
                                            let msg2 = e2.to_string();
                                            self.stats_record_search_provider_qk(
                                                "brave",
                                                false,
                                                0,
                                                pt0.elapsed().as_millis() as u64,
                                                Some(&msg2),
                                                qk.as_deref(),
                                            );
                                            let mut payload = serde_json::json!({
                                                "ok": false,
                                                "provider": "brave",
                                                "query": query.clone(),
                                                "max_results": max_results,
                                                "request": { "provider": "tavily", "auto_mode": auto_mode, "query": query.clone(), "max_results": max_results, "language": language, "country": country },
                                                "fallback": {
                                                    "requested_provider": "tavily",
                                                    "reason": "tavily_http_433",
                                                    "tavily_error": msg
                                                },
                                                "error": error_obj(
                                                    ErrorCode::SearchFailed,
                                                    msg2,
                                                    "Fallback to Brave failed. Verify WEBPIPE_BRAVE_API_KEY or switch provider."
                                                )
                                            });
                                            add_envelope_fields(
                                                &mut payload,
                                                "web_search",
                                                t0.elapsed().as_millis(),
                                            );
                                            return Ok(tool_result(payload));
                                        }
                                    };
                                    // Carry evidence about the fallback.
                                    // (We don't change the top-level `provider` field; it indicates the backend used.)
                                    let mut payload = serde_json::json!({
                                        "ok": true,
                                        "provider": "tavily",
                                        "backend_provider": r.provider,
                                        "query": query.clone(),
                                        "max_results": max_results,
                                        "request": { "provider": "tavily", "auto_mode": auto_mode, "query": query.clone(), "max_results": max_results, "language": language, "country": country },
                                        "fallback": {
                                            "requested_provider": "tavily",
                                            "reason": "tavily_http_433",
                                            "tavily_error": msg
                                        },
                                        "cost_units": r.cost_units,
                                        "timings_ms": r.timings_ms,
                                        "results": r.results,
                                    });
                                    add_envelope_fields(
                                        &mut payload,
                                        "web_search",
                                        t0.elapsed().as_millis(),
                                    );
                                    return Ok(tool_result(payload));
                                }
                                // If Brave isn't configured, keep the error as a *tool-level* failure (not transport).
                                let mut payload = serde_json::json!({
                                    "ok": false,
                                    "provider": "tavily",
                                    "query": query.clone(),
                                    "max_results": max_results,
                                    "request": { "provider": "tavily", "auto_mode": auto_mode, "query": query.clone(), "max_results": max_results, "language": language, "country": country },
                                    "error": error_obj(
                                        ErrorCode::ProviderUnavailable,
                                        msg,
                                        "Tavily returned HTTP 433 in this environment. Configure WEBPIPE_BRAVE_API_KEY to allow automatic fallback, or switch provider."
                                    )
                                });
                                add_envelope_fields(
                                    &mut payload,
                                    "web_search",
                                    t0.elapsed().as_millis(),
                                );
                                return Ok(tool_result(payload));
                            }
                            let hint = search_failed_hint(
                                "tavily",
                                &msg,
                                "Tavily search failed. Retry with smaller max_results or switch provider.",
                            );
                            let mut payload = serde_json::json!({
                                "ok": false,
                                "provider": "tavily",
                                "query": query.clone(),
                                "max_results": max_results,
                                "request": { "provider": "tavily", "auto_mode": auto_mode, "query": query.clone(), "max_results": max_results, "language": language, "country": country },
                                "error": error_obj(
                                    ErrorCode::SearchFailed,
                                    msg,
                                    hint
                                )
                            });
                            add_envelope_fields(
                                &mut payload,
                                "web_search",
                                t0.elapsed().as_millis(),
                            );
                            return Ok(tool_result(payload));
                        }
                    }
                }
                "searxng" => {
                    let pt0 = std::time::Instant::now();
                    let provider = match webpipe_local::search::SearxngSearchProvider::from_env(
                        client.clone(),
                    ) {
                        Ok(p) => p,
                        Err(WebpipeError::NotConfigured(msg)) => {
                            self.stats_record_search_provider_qk(
                                "searxng",
                                false,
                                0,
                                pt0.elapsed().as_millis() as u64,
                                Some(&msg),
                                qk.as_deref(),
                            );
                            let mut payload = serde_json::json!({
                                "ok": false,
                                "provider": "searxng",
                                "query": query.clone(),
                                "max_results": max_results,
                                "request": { "provider": "searxng", "auto_mode": auto_mode, "query": query.clone(), "max_results": max_results, "language": language, "country": country },
                                "error": error_obj(
                                    ErrorCode::NotConfigured,
                                    msg,
                                    "Set WEBPIPE_SEARXNG_ENDPOINT (or WEBPIPE_SEARXNG_ENDPOINTS) in the webpipe MCP server environment."
                                )
                            });
                            add_envelope_fields(
                                &mut payload,
                                "web_search",
                                t0.elapsed().as_millis(),
                            );
                            return Ok(tool_result(payload));
                        }
                        Err(e) => return Err(McpError::internal_error(e.to_string(), None)),
                    };
                    match provider.search(&q).await {
                        Ok(r) => {
                            self.stats_record_search_provider_qk(
                                "searxng",
                                true,
                                r.cost_units,
                                pt0.elapsed().as_millis() as u64,
                                None,
                                qk.as_deref(),
                            );
                            r
                        }
                        Err(e) => {
                            let msg = e.to_string();
                            self.stats_record_search_provider_qk(
                                "searxng",
                                false,
                                0,
                                pt0.elapsed().as_millis() as u64,
                                Some(&msg),
                                qk.as_deref(),
                            );
                            let hint = search_failed_hint(
                                "searxng",
                                &msg,
                                "SearXNG search failed. Verify WEBPIPE_SEARXNG_ENDPOINT or switch provider.",
                            );
                            let mut payload = serde_json::json!({
                                "ok": false,
                                "provider": "searxng",
                                "query": query.clone(),
                                "max_results": max_results,
                                "request": { "provider": "searxng", "auto_mode": auto_mode, "query": query.clone(), "max_results": max_results, "language": language, "country": country },
                                "error": error_obj(
                                    ErrorCode::SearchFailed,
                                    msg,
                                    hint
                                )
                            });
                            add_envelope_fields(
                                &mut payload,
                                "web_search",
                                t0.elapsed().as_millis(),
                            );
                            return Ok(tool_result(payload));
                        }
                    }
                }
                other => {
                    let mut payload = serde_json::json!({
                        "ok": false,
                        "provider": other,
                        "query": query.clone(),
                        "max_results": max_results,
                        "request": { "provider": other, "auto_mode": auto_mode, "query": query.clone(), "max_results": max_results, "language": language, "country": country },
                        "error": error_obj(
                            ErrorCode::InvalidParams,
                            format!("unknown provider: {other}"),
                            "provider must be one of: auto, brave, tavily, searxng"
                        )
                    });
                    add_envelope_fields(&mut payload, "web_search", t0.elapsed().as_millis());
                    let md = web_search_markdown(&payload);
                    return Ok(tool_result_markdown_with_json(payload, md));
                }
            };

            let mut payload = serde_json::json!({
                "ok": true,
                "provider": provider_name,
                "backend_provider": resp.provider,
                "query": query,
                "query_key": Self::query_key(&query),
                "max_results": max_results,
                "request": { "provider": provider_name, "auto_mode": auto_mode, "query": q.query, "query_key": Self::query_key(&q.query), "max_results": max_results, "language": q.language, "country": q.country },
                "cost_units": resp.cost_units,
                "timings_ms": resp.timings_ms,
                "results": resp.results,
            });
            if provider_name.as_str() == "auto" {
                payload["selection"] = serde_json::json!({
                    "requested_provider": "auto",
                    "selected_provider": payload["backend_provider"].as_str().unwrap_or("unknown"),
                    "auto_mode": auto_mode
                });
            }
            add_envelope_fields(&mut payload, "web_search", t0.elapsed().as_millis());

            let md = web_search_markdown(&payload);
            Ok(tool_result_markdown_with_json(payload, md))
        }

        #[tool(
            description = "Best for: fast single-turn Q&A with live web citations via Perplexity Sonar. Not this for multi-source evidence with per-URL control — use search_evidence instead. Only visible when WEBPIPE_PERPLEXITY_API_KEY is configured. Output: answer text + citations[].",
            input_schema = Arc::new(tool_input_schema_draft07::<WebPerplexityArgs>()),
            annotations(title = "Perplexity (answer + citations)", read_only_hint = true, open_world_hint = true)
        )]
        async fn web_perplexity(
            &self,
            params: Parameters<Option<WebPerplexityArgs>>,
        ) -> Result<CallToolResult, McpError> {
            let args = params.0.unwrap_or_default();
            self.stats_inc_tool("web_perplexity");
            let t0 = std::time::Instant::now();

            let query = args.query.unwrap_or_default();
            let model = args.model.unwrap_or_else(|| "sonar".to_string());
            let timeout_ms = args.timeout_ms.unwrap_or(20_000).min(60_000);
            let no_network = args.no_network.unwrap_or(false);
            let max_tokens = args.max_tokens.unwrap_or(800).min(4_000);
            let max_answer_chars = args.max_answer_chars.unwrap_or(4_000).min(20_000);

            if query.trim().is_empty() {
                let mut payload = serde_json::json!({
                    "ok": false,
                    "query": query,
                    "model": model,
                    "error": error_obj(
                        ErrorCode::InvalidParams,
                        "query must be non-empty",
                        "Pass a non-empty question/prompt."
                    ),
                    "request": { "timeout_ms": timeout_ms, "no_network": no_network }
                });
                add_envelope_fields(&mut payload, "web_perplexity", t0.elapsed().as_millis());
                let md = web_perplexity_markdown(&payload);
                return Ok(tool_result_markdown_with_json(payload, md));
            }

            let pplx = match webpipe_local::perplexity::PerplexityClient::from_env(
                self.http.clone(),
            ) {
                Ok(c) => c,
                Err(e) => {
                    let mut payload = serde_json::json!({
                        "ok": false,
                        "query": query,
                        "model": model,
                        "error": error_obj(
                            ErrorCode::NotConfigured,
                            e.to_string(),
                            "Set WEBPIPE_PERPLEXITY_API_KEY (or PERPLEXITY_API_KEY) in the webpipe MCP server environment."
                        ),
                        "request": { "timeout_ms": timeout_ms, "no_network": no_network }
                    });
                    add_envelope_fields(&mut payload, "web_perplexity", t0.elapsed().as_millis());
                    let md = web_perplexity_markdown(&payload);
                    return Ok(tool_result_markdown_with_json(payload, md));
                }
            };

            let mut search_mode = args.search_mode.clone();
            if no_network {
                search_mode = Some("off".to_string());
            } else if search_mode.is_none() {
                search_mode = Some("web".to_string());
            }

            let sys = "Answer the user's question concisely. Include citations for any factual claims when available.";
            let req = webpipe_local::perplexity::ChatCompletionsRequest {
                model: model.clone(),
                messages: vec![
                    webpipe_local::perplexity::Message {
                        role: "system".to_string(),
                        content: sys.to_string(),
                    },
                    webpipe_local::perplexity::Message {
                        role: "user".to_string(),
                        content: query.clone(),
                    },
                ],
                max_tokens: Some(max_tokens),
                temperature: args.temperature,
                top_p: args.top_p,
                search_mode: search_mode.clone(),
                reasoning_effort: None,
            };

            // Apply timeout at the call site too (defense-in-depth).
            let resp = match tokio::time::timeout(
                std::time::Duration::from_millis(timeout_ms),
                pplx.chat_completions(req.clone()),
            )
            .await
            {
                Ok(Ok(r)) => r,
                Ok(Err(e)) => {
                    let mut payload = serde_json::json!({
                        "ok": false,
                        "query": query,
                        "model": model,
                        "error": error_obj(ErrorCode::ProviderUnavailable, e.to_string(), "Perplexity request failed."),
                        "request": { "timeout_ms": timeout_ms, "no_network": no_network, "search_mode": search_mode }
                    });
                    add_envelope_fields(&mut payload, "web_perplexity", t0.elapsed().as_millis());
                    let md = web_perplexity_markdown(&payload);
                    return Ok(tool_result_markdown_with_json(payload, md));
                }
                Err(_) => {
                    let mut payload = serde_json::json!({
                        "ok": false,
                        "query": query,
                        "model": model,
                        "error": error_obj(ErrorCode::ProviderUnavailable, "perplexity request timed out", "Increase timeout_ms or try again."),
                        "request": { "timeout_ms": timeout_ms, "no_network": no_network, "search_mode": search_mode }
                    });
                    add_envelope_fields(&mut payload, "web_perplexity", t0.elapsed().as_millis());
                    let md = web_perplexity_markdown(&payload);
                    return Ok(tool_result_markdown_with_json(payload, md));
                }
            };

            let answer0 = resp
                .choices
                .first()
                .map(|c| c.message.content.clone())
                .unwrap_or_default();
            let citations = resp.citations.clone().unwrap_or_default();
            let (answer, _n, clipped) = Self::truncate_to_chars(&answer0, max_answer_chars);

            let mut payload = serde_json::json!({
                "ok": true,
                "provider": "perplexity",
                "query": query,
                "model": model,
                "request": { "timeout_ms": timeout_ms, "no_network": no_network, "search_mode": search_mode },
                "answer": { "text": answer, "truncated": clipped, "citations": citations },
                "usage": resp.usage,
                "timings_ms": resp.timings_ms
            });
            add_envelope_fields(&mut payload, "web_perplexity", t0.elapsed().as_millis());
            let md = web_perplexity_markdown(&payload);
            Ok(tool_result_markdown_with_json(payload, md))
        }

        #[tool(
            description = "Best for: extracting readable text or ranked evidence chunks from a single URL you already have. Not this when you need to discover the right URL first — use search_evidence instead. Not this when you need raw bytes or status — use web_fetch. Output: top_chunks[] (ranked chunks, also at extract.chunks), extract.text_chars, extract.engine, warning_codes. Set include_text=true for full extracted text (bounded by max_chars).",
            input_schema = Arc::new(tool_input_schema_draft07::<WebExtractArgs>()),
            annotations(title = "Extract URL", read_only_hint = true, open_world_hint = true)
        )]
        async fn web_extract(
            &self,
            params: Parameters<Option<WebExtractArgs>>,
        ) -> Result<CallToolResult, McpError> {
            let args = params.0.unwrap_or_default();
            self.stats_inc_tool("web_extract");
            let width = args.width.unwrap_or(100).clamp(20, 240);
            let max_chars = args.max_chars.unwrap_or(20_000).min(200_000);
            let top_chunks = args.top_chunks.unwrap_or(5).min(50);
            let max_chunk_chars = args.max_chunk_chars.unwrap_or(500).min(5_000);
            let include_links = args.include_links.unwrap_or(false);
            let max_links = args.max_links.unwrap_or(50).min(500);
            // Default behavior: return full extracted text when no query is provided (users asked for “extract”),
            // but keep it off when query is provided (callers usually want bounded chunks).
            let include_text = args.include_text.unwrap_or(args.query.is_none());
            // Default to structure output for higher-quality chunk selection and better debugging.
            let include_structure = args.include_structure.unwrap_or(true);
            let max_outline_items = args.max_outline_items.unwrap_or(25).min(200);
            let max_blocks = args.max_blocks.unwrap_or(40).min(200);
            let max_block_chars = args.max_block_chars.unwrap_or(400).min(2000);
            let semantic_rerank = args.semantic_rerank.unwrap_or(false);
            let semantic_auto_fallback = args.semantic_auto_fallback.unwrap_or(true);
            let semantic_top_k = args.semantic_top_k.unwrap_or(5).min(50);
            let fetch_backend = args.fetch_backend.unwrap_or_else(|| "local".to_string());
            let no_network = args.no_network.unwrap_or(false);
            let cache_read_effective = args.cache_read.unwrap_or(true) || no_network;
            let cache_write_effective = if no_network {
                false
            } else {
                args.cache_write.unwrap_or(true)
            };
            let retry_on_truncation_arg = args.retry_on_truncation;
            let truncation_retry_max_bytes = args
                .truncation_retry_max_bytes
                .unwrap_or(10_000_000)
                .min(20_000_000);

            let url = args.url.unwrap_or_default();
            let t0 = std::time::Instant::now();
            if url.trim().is_empty() {
                let mut payload = serde_json::json!({
                    "ok": false,
                    "url": url,
                    "error": error_obj(
                        ErrorCode::InvalidParams,
                        "url must be non-empty",
                        "Pass an absolute URL like https://example.com."
                    ),
                    "request": {
                        "fetch_backend": fetch_backend,
                        "no_network": no_network,
                        "timeout_ms": args.timeout_ms.or(Some(20_000)),
                        "max_bytes": args.max_bytes.or(Some(5_000_000)),
                        "cache": { "read": cache_read_effective, "write": cache_write_effective, "ttl_s": args.cache_ttl_s },
                        "width": width,
                        "max_chars": max_chars,
                        "query": args.query,
                        "include_text": include_text,
                        "include_links": include_links,
                        "max_links": max_links,
                        "include_structure": include_structure,
                        "max_outline_items": max_outline_items,
                        "max_blocks": max_blocks,
                        "max_block_chars": max_block_chars
                    }
                });
                add_envelope_fields(&mut payload, "web_extract", t0.elapsed().as_millis());
                let md = web_extract_markdown(&payload);
                return Ok(tool_result_markdown_with_json(payload, md));
            }
            if fetch_backend.as_str() != "local"
                && fetch_backend.as_str() != "firecrawl"
                && fetch_backend.as_str() != "render"
            {
                let mut payload = serde_json::json!({
                    "ok": false,
                    "url": url,
                    "error": error_obj(
                        ErrorCode::InvalidParams,
                        "unknown fetch_backend",
                        "Allowed fetch_backends: local, firecrawl, render"
                    ),
                    "request": { "fetch_backend": fetch_backend }
                });
                add_envelope_fields(&mut payload, "web_extract", t0.elapsed().as_millis());
                let md = web_extract_markdown(&payload);
                return Ok(tool_result_markdown_with_json(payload, md));
            }

            if no_network && fetch_backend == "firecrawl" {
                let mut payload = serde_json::json!({
                    "ok": false,
                    "url": url,
                    "error": error_obj(
                        ErrorCode::NotSupported,
                        "no_network=true cannot be used with fetch_backend=\"firecrawl\"",
                        "Use fetch_backend=\"local\" with a warmed WEBPIPE_CACHE_DIR, or set no_network=false."
                    ),
                    "request": {
                        "fetch_backend": fetch_backend,
                        "no_network": no_network,
                        "cache": { "read": cache_read_effective, "write": cache_write_effective, "ttl_s": args.cache_ttl_s }
                    }
                });
                add_envelope_fields(&mut payload, "web_extract", t0.elapsed().as_millis());
                let md = web_extract_markdown(&payload);
                return Ok(tool_result_markdown_with_json(payload, md));
            }
            if no_network && fetch_backend == "render" {
                let mut payload = serde_json::json!({
                    "ok": false,
                    "url": url,
                    "error": error_obj(
                        ErrorCode::NotSupported,
                        "no_network=true cannot be used with fetch_backend=\"render\"",
                        "Render mode uses a headless browser and performs network requests. Use fetch_backend=\"local\" with a warmed WEBPIPE_CACHE_DIR, or set no_network=false."
                    ),
                    "request": { "fetch_backend": fetch_backend, "no_network": no_network }
                });
                add_envelope_fields(&mut payload, "web_extract", t0.elapsed().as_millis());
                let md = web_extract_markdown(&payload);
                return Ok(tool_result_markdown_with_json(payload, md));
            }
            if let Some(qs) = args.query.as_deref() {
                if qs.trim().is_empty() {
                    let mut payload = serde_json::json!({
                        "ok": false,
                        "url": url,
                        "error": error_obj(
                            ErrorCode::InvalidParams,
                            "query must be non-empty when provided",
                            "Either omit query, or pass a non-empty query string."
                        ),
                        "request": {
                            "fetch_backend": fetch_backend,
                            "timeout_ms": args.timeout_ms.or(Some(20_000)),
                            "max_bytes": args.max_bytes.or(Some(5_000_000)),
                            "cache": { "read": cache_read_effective, "write": cache_write_effective, "ttl_s": args.cache_ttl_s },
                            "width": width,
                            "max_chars": max_chars,
                            "query": args.query,
                            "include_text": include_text,
                            "include_links": include_links,
                            "max_links": max_links,
                            "include_structure": include_structure,
                            "max_outline_items": max_outline_items,
                            "max_blocks": max_blocks,
                            "max_block_chars": max_block_chars
                        }
                    });
                    add_envelope_fields(&mut payload, "web_extract", t0.elapsed().as_millis());
                    let md = web_extract_markdown(&payload);
                    return Ok(tool_result_markdown_with_json(payload, md));
                }
            }

            // Firecrawl mode: return markdown as extracted text, with limited link support.
            if fetch_backend == "firecrawl" {
                let timeout_ms = args.timeout_ms.unwrap_or(20_000);
                let max_age_ms = args.cache_ttl_s.map(|s| s.saturating_mul(1000));
                let fc = match webpipe_local::firecrawl::FirecrawlClient::from_env(
                    self.http.clone(),
                ) {
                    Ok(c) => c,
                    Err(e) => {
                        let msg = e.to_string();
                        self.stats_record_fetch_backend(
                            "firecrawl",
                            false,
                            t0.elapsed().as_millis() as u64,
                            Some(&msg),
                        );
                        let mut payload = serde_json::json!({
                            "ok": false,
                            "url": url,
                            "error": error_obj(
                                ErrorCode::NotConfigured,
                                msg,
                                "Set WEBPIPE_FIRECRAWL_API_KEY (or FIRECRAWL_API_KEY) to use fetch_backend=\"firecrawl\"."
                            ),
                            "request": { "fetch_backend": fetch_backend, "timeout_ms": timeout_ms }
                        });
                        add_envelope_fields(&mut payload, "web_extract", t0.elapsed().as_millis());
                        let md = web_extract_markdown(&payload);
                        return Ok(tool_result_markdown_with_json(payload, md));
                    }
                };

                let r = match fc.fetch_markdown(&url, timeout_ms, max_age_ms).await {
                    Ok(r) => r,
                    Err(e) => {
                        let msg = e.to_string();
                        self.stats_record_fetch_backend(
                            "firecrawl",
                            false,
                            t0.elapsed().as_millis() as u64,
                            Some(&msg),
                        );
                        let mut payload = serde_json::json!({
                            "ok": false,
                            "url": url,
                            "error": error_obj(
                                ErrorCode::FetchFailed,
                                msg,
                                "Firecrawl fetch failed. If this looks transient, retry later or reduce scope."
                            ),
                            "request": { "fetch_backend": fetch_backend, "timeout_ms": timeout_ms }
                        });
                        add_envelope_fields(&mut payload, "web_extract", t0.elapsed().as_millis());
                        let md = web_extract_markdown(&payload);
                        return Ok(tool_result_markdown_with_json(payload, md));
                    }
                };

                let md = r.markdown;
                let md_bytes = md.as_bytes().to_vec();
                let md_raw_bytes = Self::approx_bytes_len(&md);
                let extracted0 = webpipe_local::extract::ExtractedText {
                    engine: "firecrawl",
                    text: md,
                    warnings: Vec::new(),
                };
                let pipeline = webpipe_local::extract::extract_pipeline_from_extracted(
                    &md_bytes,
                    Some("text/markdown"),
                    url.as_str(),
                    extracted0,
                    webpipe_local::extract::ExtractPipelineCfg {
                        query: args.query.as_deref(),
                        width,
                        max_chars,
                        top_chunks,
                        max_chunk_chars,
                        include_structure,
                        max_outline_items,
                        max_blocks,
                        max_block_chars,
                    },
                );
                let extracted = pipeline.extracted;
                let text = extracted.text.clone();
                let n = pipeline.text_chars;
                let clipped = pipeline.text_truncated;
                let mut warnings: Vec<&'static str> = Vec::new();
                if clipped {
                    warnings.push("text_truncated_by_max_chars");
                }

                let mut payload = serde_json::json!({
                    "ok": true,
                    "fetch_backend": "firecrawl",
                    "url": url,
                    "final_url": url,
                    "status": 200,
                    "content_type": "text/markdown",
                    "bytes": md_raw_bytes,
                    "truncated": clipped,
                    "timings_ms": {
                        "total": t0.elapsed().as_millis(),
                        "firecrawl_fetch": r.elapsed_ms
                    }
                });
                add_envelope_fields(&mut payload, "web_extract", t0.elapsed().as_millis());
                payload["request"] = serde_json::json!({
                    "fetch_backend": "firecrawl",
                    "timeout_ms": timeout_ms,
                    "cache_ttl_s": args.cache_ttl_s,
                    "width": width,
                    "max_chars": max_chars,
                    "query": args.query,
                    "include_text": include_text,
                    "include_links": include_links,
                    "max_links": max_links,
                    "include_structure": include_structure,
                    "max_outline_items": max_outline_items,
                    "max_blocks": max_blocks,
                    "max_block_chars": max_block_chars
                });
                // Canonical output: extraction results live under payload.extract (no legacy mirrors).
                if !warnings.is_empty() {
                    payload["warnings"] = serde_json::json!(warnings);
                    let codes = warning_codes_from(&warnings);
                    payload["warning_codes"] = serde_json::json!(codes.clone());
                    payload["warning_hints"] = warning_hints_from(&codes);
                    self.stats_record_warnings(&warnings);
                }
                self.stats_record_fetch_backend(
                    "firecrawl",
                    true,
                    t0.elapsed().as_millis() as u64,
                    None,
                );
                // Canonical extract object.
                payload["extract"] = serde_json::json!({
                    "engine": "firecrawl",
                    "width": width,
                    "max_chars": max_chars,
                    "text_chars": n,
                    "text_truncated": clipped,
                    "top_chunks": top_chunks,
                    "max_chunk_chars": max_chunk_chars
                });
                if include_text {
                    payload["extract"]["text"] = serde_json::json!(text);
                }
                if include_structure {
                    if let Some(s) = pipeline.structure.as_ref() {
                        payload["extract"]["structure"] = serde_json::json!(s);
                    }
                }
                if let Some(q) = args.query.as_deref() {
                    if !q.trim().is_empty() {
                        payload["extract"]["chunks"] = serde_json::json!(pipeline.chunks);
                    }
                }
                if semantic_rerank
                    && args
                        .query
                        .as_deref()
                        .map(|s| !s.trim().is_empty())
                        .unwrap_or(false)
                {
                    let cands: Vec<(usize, usize, String)> = pipeline
                        .chunks
                        .iter()
                        .map(|c| (c.start_char, c.end_char, c.text.clone()))
                        .collect();
                    let q0 = args.query.clone().unwrap_or_default();
                    let semantic_timeout_ms = std::env::var("WEBPIPE_SEMANTIC_TIMEOUT_MS")
                        .ok()
                        .and_then(|s| s.trim().parse::<u64>().ok())
                        .unwrap_or(
                            args.timeout_ms
                                .unwrap_or(20_000)
                                .saturating_div(2)
                                .clamp(2_000, 8_000),
                        );
                    let sem = if semantic_timeout_ms == 0 {
                        webpipe_local::semantic::SemanticRerankResult {
                            ok: false,
                            backend: "unknown".to_string(),
                            model_id: None,
                            cache_hits: 0,
                            cache_misses: 0,
                            chunks: Vec::new(),
                            warnings: vec!["semantic_rerank_timeout"],
                        }
                    } else {
                        let handle = tokio::task::spawn_blocking(move || {
                            webpipe_local::semantic::semantic_rerank_chunks(
                                &q0,
                                &cands,
                                semantic_top_k,
                            )
                        });
                        match tokio::time::timeout(
                            std::time::Duration::from_millis(semantic_timeout_ms),
                            handle,
                        )
                        .await
                        {
                            Ok(join) => join.unwrap_or_else(|_| {
                                webpipe_local::semantic::SemanticRerankResult {
                                    ok: false,
                                    backend: "unknown".to_string(),
                                    model_id: None,
                                    cache_hits: 0,
                                    cache_misses: 0,
                                    chunks: Vec::new(),
                                    warnings: vec!["semantic_rerank_task_failed"],
                                }
                            }),
                            Err(_) => webpipe_local::semantic::SemanticRerankResult {
                                ok: false,
                                backend: "unknown".to_string(),
                                model_id: None,
                                cache_hits: 0,
                                cache_misses: 0,
                                chunks: Vec::new(),
                                warnings: vec!["semantic_rerank_timeout"],
                            },
                        }
                    };
                    for w in &sem.warnings {
                        warnings.push(*w);
                    }
                    payload["extract"]["semantic"] = serde_json::json!(sem);
                }
                if include_links {
                    payload["extract"]["links"] = serde_json::json!([]);
                    payload["extract"]["max_links"] = serde_json::json!(max_links);
                    warnings.push("links_unavailable_for_firecrawl");
                }
                if !warnings.is_empty() {
                    payload["warnings"] = serde_json::json!(warnings);
                    let codes = warning_codes_from(&warnings);
                    payload["warning_codes"] = serde_json::json!(codes.clone());
                    payload["warning_hints"] = warning_hints_from(&codes);
                    self.stats_record_warnings(&warnings);
                }
                return Ok(tool_result(payload));
            }

            // arXiv rewrite: /abs/<id> → /pdf/<id>.pdf
            let (url0, arxiv_attempts) = self.maybe_rewrite_arxiv_abs_url(&url);
            // GitHub issue rewrite: /issues/<n> → GitHub API JSON (no network)
            let (url0b, gh_issue_attempts, gh_issue_rewrote) =
                self.maybe_rewrite_github_issue_url(&url0);
            // GitHub release rewrite: /releases/... → GitHub API JSON (no network)
            let (url0c, gh_release_attempts, gh_release_rewrote) =
                self.maybe_rewrite_github_release_url(&url0b);
            // GitHub PR/commit rewrite: /pull/<n> or /commit/<sha> → .patch (no network)
            let (url1, gh_patch_attempts, gh_patch_warn) =
                self.maybe_rewrite_github_pr_or_commit_url(&url0c);
            // GitHub blob rewrite: /blob/<ref>/<path> → raw file (no network)
            let (url2, github_blob_attempts) = self.maybe_rewrite_github_blob_url(&url1);
            // Gist rewrite: gist page → /raw (no network)
            let (url3, gist_attempts) = self.maybe_rewrite_gist_url(&url2);
            // GitHub repo root rewrite: prefer raw README over GitHub app shell (network probe)
            let (fetch_url, github_repo_attempts) = self
                .maybe_rewrite_github_repo_url(
                    &url3,
                    args.timeout_ms.unwrap_or(20_000),
                    args.max_bytes.unwrap_or(5_000_000),
                    no_network,
                    args.cache_read.unwrap_or(true),
                    args.cache_write.unwrap_or(true),
                    args.cache_ttl_s,
                )
                .await;
            let arxiv_rewrote = url0 != url;
            let github_patch_rewrote = url1 != url0;
            let github_blob_rewrote = url2 != url1;
            let gist_rewrote = url3 != url2;
            let github_repo_rewrote = fetch_url != url3;
            let mut attempts_map = serde_json::Map::new();
            if let Some(a) = arxiv_attempts {
                attempts_map.insert("arxiv_rewrite".to_string(), a);
            }
            if let Some(a) = gh_issue_attempts {
                attempts_map.insert("github_issue_rewrite".to_string(), a);
            }
            if let Some(a) = gh_release_attempts {
                attempts_map.insert("github_release_rewrite".to_string(), a);
            }
            if let Some(a) = gh_patch_attempts {
                attempts_map.insert("github_patch_rewrite".to_string(), a);
            }
            if let Some(a) = github_blob_attempts {
                attempts_map.insert("github_blob_rewrite".to_string(), a);
            }
            if let Some(a) = gist_attempts {
                attempts_map.insert("gist_rewrite".to_string(), a);
            }
            if let Some(a) = github_repo_attempts {
                attempts_map.insert("github_repo_rewrite".to_string(), a);
            }

            let mut render_meta: Option<serde_json::Value> = None;

            let req = FetchRequest {
                url: fetch_url.clone(),
                timeout_ms: args.timeout_ms.or(Some(20_000)),
                max_bytes: args.max_bytes.or(Some(5_000_000)),
                headers: BTreeMap::new(),
                cache: FetchCachePolicy {
                    read: args.cache_read.unwrap_or(true) || no_network,
                    write: if no_network {
                        false
                    } else {
                        args.cache_write.unwrap_or(true)
                    },
                    ttl_s: args.cache_ttl_s,
                },
            };

            // Render mode (Playwright): execute JS-heavy pages in a headless browser and then run the
            // normal extraction pipeline on the resulting HTML.
            //
            // Safety defaults:
            // - Explicit opt-in via fetch_backend="render"
            // - no_network is rejected above
            // - privacy_mode=offline: only allow localhost URLs
            // - privacy_mode=anonymous: require a proxy for non-localhost URLs (fail closed)
            let resp = if fetch_backend == "render" {
                let pm = privacy_mode_from_env();
                if matches!(pm, PrivacyMode::Offline) && !is_localhost_url(fetch_url.as_str()) {
                    let mut payload = serde_json::json!({
                        "ok": false,
                        "url": url,
                        "error": error_obj(
                            ErrorCode::NotSupported,
                            "privacy_mode=offline blocks render fetches to non-localhost URLs",
                            "Use a localhost URL, or set WEBPIPE_PRIVACY_MODE=normal/anonymous."
                        ),
                        "request": { "fetch_backend": fetch_backend, "no_network": no_network }
                    });
                    add_envelope_fields(&mut payload, "web_extract", t0.elapsed().as_millis());
                    let md = web_extract_markdown(&payload);
                    return Ok(tool_result_markdown_with_json(payload, md));
                }

                let anon_proxy = anon_proxy_from_env();
                let proxy_for_render = if matches!(pm, PrivacyMode::Anonymous)
                    && !is_localhost_url(fetch_url.as_str())
                {
                    let Some(p) = anon_proxy.as_deref() else {
                        let mut payload = serde_json::json!({
                            "ok": false,
                            "url": url,
                            "error": error_obj(
                                ErrorCode::NotConfigured,
                                "anonymous mode requires a proxy for fetch_backend=\"render\"",
                                "Set WEBPIPE_ANON_PROXY (e.g. an HTTP proxy, or a Tor->HTTP proxy like Privoxy)."
                            ),
                            "request": { "fetch_backend": fetch_backend, "no_network": no_network }
                        });
                        add_envelope_fields(&mut payload, "web_extract", t0.elapsed().as_millis());
                        let md = web_extract_markdown(&payload);
                        return Ok(tool_result_markdown_with_json(payload, md));
                    };
                    let pl = p.trim().to_ascii_lowercase();
                    if pl.starts_with("socks5h://") {
                        let mut payload = serde_json::json!({
                            "ok": false,
                            "url": url,
                            "error": error_obj(
                                ErrorCode::NotSupported,
                                "socks5h:// proxies are not supported by Playwright proxy settings",
                                "Use an HTTP proxy endpoint, or convert Tor SOCKS to HTTP (e.g. via Privoxy), then set WEBPIPE_ANON_PROXY to that HTTP proxy URL."
                            ),
                            "request": { "fetch_backend": fetch_backend, "no_network": no_network }
                        });
                        add_envelope_fields(&mut payload, "web_extract", t0.elapsed().as_millis());
                        let md = web_extract_markdown(&payload);
                        return Ok(tool_result_markdown_with_json(payload, md));
                    }
                    Some(p)
                } else {
                    None
                };

                let timeout_ms = args.timeout_ms.unwrap_or(20_000);
                let pr = match webpipe_local::render_playwright::render_html_playwright(
                    fetch_url.as_str(),
                    timeout_ms,
                    proxy_for_render,
                )
                .await
                {
                    Ok(r) => r,
                    Err(e) => {
                        let msg = e.to_string();
                        self.stats_record_fetch_backend(
                            "render",
                            false,
                            t0.elapsed().as_millis() as u64,
                            Some(&msg),
                        );
                        let (code, hint) = match &e {
                            WebpipeError::InvalidUrl(_) => (
                                ErrorCode::InvalidUrl,
                                "Check that the URL is absolute (includes http/https) and is well-formed.",
                            ),
                            WebpipeError::NotConfigured(_) => (
                                ErrorCode::NotConfigured,
                                "Install Playwright for Node (and browsers), or set WEBPIPE_RENDER_DISABLE=1 to force-disable render in this environment.",
                            ),
                            WebpipeError::NotSupported(_) => (
                                ErrorCode::NotSupported,
                                "This operation is not supported by the current privacy mode/proxy configuration.",
                            ),
                            _ => (
                                ErrorCode::FetchFailed,
                                "Render fetch failed. Try a longer timeout_ms or use fetch_backend=\"local\" for non-JS pages.",
                            ),
                        };
                        let mut payload = serde_json::json!({
                            "ok": false,
                            "url": url,
                            "error": error_obj(code, msg, hint),
                            "request": { "fetch_backend": fetch_backend, "no_network": no_network, "timeout_ms": timeout_ms }
                        });
                        add_envelope_fields(&mut payload, "web_extract", t0.elapsed().as_millis());
                        let md = web_extract_markdown(&payload);
                        return Ok(tool_result_markdown_with_json(payload, md));
                    }
                };

                render_meta = Some(serde_json::json!({
                    "backend": "playwright",
                    "mode": pr.mode,
                    "proxy_configured": anon_proxy.is_some(),
                    "console_error_count": pr.console_error_count,
                    "elapsed_ms": pr.elapsed_ms
                }));

                webpipe_core::FetchResponse {
                    url: fetch_url.clone(),
                    final_url: pr.final_url,
                    status: pr.status.unwrap_or(200),
                    content_type: Some("text/html".to_string()),
                    headers: BTreeMap::new(),
                    bytes: pr.html.into_bytes(),
                    truncated: false,
                    source: webpipe_core::FetchSource::Network,
                    timings_ms: {
                        let mut m = BTreeMap::new();
                        m.insert("playwright_render".to_string(), pr.elapsed_ms as u128);
                        m
                    },
                }
            } else if no_network && !url_is_localhost(&req.url) {
                // Cache-only mode (no network): return ok=false on cache miss.
                match self.fetcher.cache_get(&req) {
                    Ok(Some(r)) => r,
                    Ok(None) => {
                        self.stats_record_fetch_backend(
                            "local",
                            false,
                            t0.elapsed().as_millis() as u64,
                            Some("cache_miss_no_network"),
                        );
                        let warns: Vec<&'static str> = vec!["no_network_may_require_warm_cache"];
                        let codes = warning_codes_from(&warns);
                        let mut payload = serde_json::json!({
                            "ok": false,
                            "url": url,
                            "error": error_obj(
                                ErrorCode::FetchFailed,
                                "cache miss in no_network mode",
                                "Warm the cache first (run without no_network), or set no_network=false."
                            ),
                            "request": { "fetch_backend": "local", "no_network": true, "cache": { "read": true, "write": false, "ttl_s": req.cache.ttl_s } },
                            "warnings": warns,
                            "warning_codes": codes.clone(),
                            "warning_hints": warning_hints_from(&codes)
                        });
                        add_envelope_fields(&mut payload, "web_extract", t0.elapsed().as_millis());
                        let md = web_extract_markdown(&payload);
                        return Ok(tool_result_markdown_with_json(payload, md));
                    }
                    Err(e) => {
                        let msg = e.to_string();
                        self.stats_record_fetch_backend(
                            "local",
                            false,
                            t0.elapsed().as_millis() as u64,
                            Some(&msg),
                        );
                        let mut payload = serde_json::json!({
                            "ok": false,
                            "url": url,
                            "error": error_obj(ErrorCode::CacheError, msg, "Cache read failed in no_network mode."),
                            "request": { "fetch_backend": "local", "no_network": true }
                        });
                        add_envelope_fields(&mut payload, "web_extract", t0.elapsed().as_millis());
                        let md = web_extract_markdown(&payload);
                        return Ok(tool_result_markdown_with_json(payload, md));
                    }
                }
            } else {
                let mut r = match self.fetcher.fetch(&req).await {
                    Ok(r) => r,
                    Err(e) => {
                        let msg = e.to_string();
                        self.stats_record_fetch_backend(
                            "local",
                            false,
                            t0.elapsed().as_millis() as u64,
                            Some(&msg),
                        );
                        let (code, hint) = match &e {
                            WebpipeError::InvalidUrl(_) => (
                                ErrorCode::InvalidUrl,
                                "Check that the URL is absolute (includes http/https) and is well-formed.",
                            ),
                            WebpipeError::Fetch(_) => (
                                ErrorCode::FetchFailed,
                                "Fetch failed. If this looks transient, retry with a smaller max_bytes and a larger timeout_ms; otherwise try a different URL.",
                            ),
                            WebpipeError::Cache(_) => (
                                ErrorCode::CacheError,
                                "Cache failed. Retry with cache_read=false/cache_write=false to isolate network vs cache issues.",
                            ),
                            WebpipeError::Search(_) => (
                                ErrorCode::UnexpectedError,
                                "Unexpected search error while fetching. This is likely a bug; include this payload when reporting.",
                            ),
                            WebpipeError::Llm(_) => (
                                ErrorCode::UnexpectedError,
                                "Unexpected LLM error while fetching. This is likely a bug; include this payload when reporting.",
                            ),
                            WebpipeError::NotConfigured(_) => (
                                ErrorCode::NotConfigured,
                                "This fetch backend should not require API keys; check your server environment and configuration.",
                            ),
                            WebpipeError::NotSupported(_) => (
                                ErrorCode::NotSupported,
                                "This operation is not supported by the current backend/config.",
                            ),
                        };
                        let mut payload = serde_json::json!({
                            "ok": false,
                            "url": url,
                            "error": error_obj(code, msg, hint),
                            "request": {
                                "fetch_backend": fetch_backend,
                                "no_network": no_network,
                                "timeout_ms": req.timeout_ms,
                                "max_bytes": req.max_bytes,
                                "cache": { "read": req.cache.read, "write": req.cache.write, "ttl_s": req.cache.ttl_s },
                                "width": width,
                                "max_chars": max_chars
                            }
                        });
                        add_envelope_fields(&mut payload, "web_extract", t0.elapsed().as_millis());
                        let md = web_extract_markdown(&payload);
                        return Ok(tool_result_markdown_with_json(payload, md));
                    }
                };

                // Optional bounded truncation retry (tail-recovery). Useful for PDFs (xref/trailer often at end).
                let retry_on_truncation = retry_on_truncation_arg.unwrap_or_else(|| {
                    Self::content_type_is_pdf(r.content_type.as_deref())
                        || Self::url_looks_like_pdf(r.final_url.as_str())
                        || webpipe_local::extract::bytes_look_like_pdf(&r.bytes)
                });
                if retry_on_truncation && r.truncated && !r.bytes.is_empty() {
                    let from = req.max_bytes.unwrap_or(0);
                    let to = (from.saturating_mul(5))
                        .max(1_000_000)
                        .min(truncation_retry_max_bytes);
                    if to > from && !no_network {
                        let mut req2 = req.clone();
                        req2.max_bytes = Some(to);
                        // Avoid reusing a truncated cache entry when trying to recover tail content.
                        req2.cache.read = false;
                        let t1 = std::time::Instant::now();
                        match self.fetcher.fetch(&req2).await {
                            Ok(r2) => {
                                attempts_map.insert(
                                    "truncation_retry".to_string(),
                                    serde_json::json!({
                                        "from_max_bytes": from,
                                        "to_max_bytes": to,
                                        "elapsed_ms": t1.elapsed().as_millis(),
                                        "ok": true,
                                        "status": r2.status,
                                        "bytes": r2.bytes.len(),
                                        "truncated": r2.truncated
                                    }),
                                );
                                // Prefer the larger response when it is not truncated or contains more bytes.
                                if (!r2.truncated && r.truncated) || r2.bytes.len() > r.bytes.len()
                                {
                                    r = r2;
                                }
                            }
                            Err(_) => {
                                attempts_map.insert(
                                    "truncation_retry".to_string(),
                                    serde_json::json!({
                                        "from_max_bytes": from,
                                        "to_max_bytes": to,
                                        "elapsed_ms": t1.elapsed().as_millis(),
                                        "ok": false,
                                        "error": "fetch_failed"
                                    }),
                                );
                            }
                        }
                    }
                }

                r
            };

            // From this point on, treat response parts as owned so we can offload extraction/chunking
            // to a blocking thread pool (avoid stalling the async runtime on big HTML/PDFs).
            let webpipe_core::FetchResponse {
                url: resp_url,
                final_url: resp_final_url,
                status: resp_status,
                content_type: resp_content_type,
                headers: _resp_headers,
                bytes: resp_bytes0,
                truncated: resp_body_truncated,
                source: _resp_source,
                timings_ms: resp_timings_ms,
            } = resp;
            let resp_bytes = std::sync::Arc::new(resp_bytes0);

            let pipeline0 = {
                let bytes = resp_bytes.clone();
                let ct = resp_content_type.clone();
                let final_url = resp_final_url.clone();
                let query = args.query.clone();
                let extract_timeout_ms_env = std::env::var("WEBPIPE_EXTRACT_PIPELINE_TIMEOUT_MS")
                    .ok()
                    .and_then(|s| s.trim().parse::<u64>().ok());
                let caller_timeout_ms = args.timeout_ms.unwrap_or(20_000);
                let pdf_like = Self::content_type_is_pdf(ct.as_deref())
                    || Self::url_looks_like_pdf(final_url.as_str())
                    || webpipe_local::extract::bytes_look_like_pdf(bytes.as_ref());
                let extract_timeout_ms_default = if pdf_like {
                    // PDFs can be slower to parse/extract than HTML. Keep a bounded but more
                    // permissive default so “arXiv PDF → evidence” works out of the box.
                    45_000
                } else {
                    // For HTML-ish content, stay within the per-call fetch timeout by default.
                    caller_timeout_ms.saturating_sub(1_000).clamp(4_000, 45_000)
                };
                let extract_timeout_ms =
                    extract_timeout_ms_env.unwrap_or(extract_timeout_ms_default);
                if extract_timeout_ms == 0 {
                    let mut payload = serde_json::json!({
                        "ok": false,
                        "kind": "web_extract",
                        "url": url,
                        "final_url": resp_final_url,
                        "status": resp_status,
                        "content_type": resp_content_type,
                        "error": error_obj(
                            ErrorCode::FetchFailed,
                            "extract pipeline timed out after 0ms".to_string(),
                            "Extraction hit a bounded timeout. Increase WEBPIPE_EXTRACT_PIPELINE_TIMEOUT_MS or reduce max_bytes/max_chars."
                        ),
                        "request": {
                            "fetch_backend": fetch_backend,
                            "no_network": no_network,
                            "timeout_ms": req.timeout_ms,
                            "max_bytes": req.max_bytes,
                            "cache": { "read": req.cache.read, "write": req.cache.write, "ttl_s": req.cache.ttl_s },
                            "width": width,
                            "max_chars": max_chars,
                            "query": args.query,
                            "include_text": include_text,
                            "include_links": include_links,
                            "max_links": max_links,
                            "include_structure": include_structure
                        },
                        "warnings": ["extract_pipeline_timeout"],
                        "warning_codes": ["extract_pipeline_timeout"],
                        "warning_hints": warning_hints_from(&["extract_pipeline_timeout"])
                    });
                    add_envelope_fields(&mut payload, "web_extract", t0.elapsed().as_millis());
                    let md = web_extract_markdown(&payload);
                    return Ok(tool_result_markdown_with_json(payload, md));
                }
                let handle = tokio::task::spawn_blocking(move || {
                    let extracted0 = webpipe_local::extract::best_effort_text_from_bytes(
                        &bytes,
                        ct.as_deref(),
                        final_url.as_str(),
                        width,
                        500,
                    );
                    webpipe_local::extract::extract_pipeline_from_extracted(
                        &bytes,
                        ct.as_deref(),
                        final_url.as_str(),
                        extracted0,
                        webpipe_local::extract::ExtractPipelineCfg {
                            query: query.as_deref(),
                            width,
                            max_chars,
                            top_chunks,
                            max_chunk_chars,
                            include_structure,
                            max_outline_items,
                            max_blocks,
                            max_block_chars,
                        },
                    )
                });
                match tokio::time::timeout(
                    std::time::Duration::from_millis(extract_timeout_ms),
                    handle,
                )
                .await
                {
                    Ok(join) => join.map_err(|e| {
                        McpError::internal_error(format!("extract pipeline join failed: {e}"), None)
                    })?,
                    Err(_) => {
                        let mut payload = serde_json::json!({
                            "ok": false,
                            "kind": "web_extract",
                            "url": url,
                            "final_url": resp_final_url,
                            "status": resp_status,
                            "content_type": resp_content_type,
                            "error": error_obj(
                                ErrorCode::FetchFailed,
                                format!("extract pipeline timed out after {extract_timeout_ms}ms"),
                                "Extraction hit a bounded timeout. HTML parsing and PDF extraction can be slow. Try increasing timeout_ms (e.g. 60000), reducing max_bytes/max_chars, switching fetch_backend, or setting WEBPIPE_EXTRACT_PIPELINE_TIMEOUT_MS."
                            ),
                            "request": {
                                "fetch_backend": fetch_backend,
                                "no_network": no_network,
                                "timeout_ms": req.timeout_ms,
                                "max_bytes": req.max_bytes,
                                "cache": { "read": req.cache.read, "write": req.cache.write, "ttl_s": req.cache.ttl_s },
                                "width": width,
                                "max_chars": max_chars,
                                "query": args.query,
                                "include_text": include_text,
                                "include_links": include_links,
                                "max_links": max_links,
                                "include_structure": include_structure
                            },
                            "warnings": ["extract_pipeline_timeout"],
                            "warning_codes": ["extract_pipeline_timeout"],
                            "warning_hints": warning_hints_from(&["extract_pipeline_timeout"])
                        });
                        add_envelope_fields(&mut payload, "web_extract", t0.elapsed().as_millis());
                        let md = web_extract_markdown(&payload);
                        return Ok(tool_result_markdown_with_json(payload, md));
                    }
                }
            };
            // Allow post-extraction fallbacks to replace pipeline + response parts.
            let mut resp_url = resp_url;
            let mut resp_final_url = resp_final_url;
            let mut resp_status = resp_status;
            let mut resp_content_type = resp_content_type;
            let mut resp_body_truncated = resp_body_truncated;
            let mut resp_timings_ms = resp_timings_ms;
            let mut resp_bytes = resp_bytes;

            let mut pipeline = pipeline0;
            #[cfg(feature = "vision-gemini")]
            let mut vision_model: Option<String> = None;
            #[cfg(not(feature = "vision-gemini"))]
            let vision_model: Option<String> = None;

            // Content-first: if PDF extraction fails (or panics) in the pure-Rust backend,
            // fall back to a higher-signal HTML artifact (bounded, conservative rewrites) and
            // extract evidence from that instead.
            //
            // This is bounded and only triggers when local PDF extraction produced degraded output.
            let mut pdf_html_fallback_used_warning: Option<&'static str> = None;
            if fetch_backend == "local" && !no_network {
                let pdf_extraction_degraded = pipeline.text_chars == 0
                    || pipeline.extracted.warnings.iter().any(|&w| {
                        matches!(
                            normalize_warning_code(w),
                            "pdf_extract_failed"
                                | "pdf_extract_panicked"
                                | "pdf_strings_fallback_used"
                        )
                    });
                if pdf_extraction_degraded {
                    #[derive(Clone)]
                    struct PdfHtmlFallbackSpec {
                        attempt_key: &'static str,
                        kind: &'static str,
                        warning_code: &'static str,
                        url: String,
                    }
                    let mut fb_specs: Vec<PdfHtmlFallbackSpec> = Vec::new();
                    if let Some(cands) =
                        webpipe_local::rewrite::arxiv_pdf_html_candidates(resp_final_url.as_str())
                    {
                        if let Some(u) = cands.into_iter().next() {
                            fb_specs.push(PdfHtmlFallbackSpec {
                                attempt_key: "arxiv_pdf_html_fallback",
                                kind: "arxiv_pdf_to_html_fallback",
                                warning_code: "arxiv_pdf_fallback_to_html",
                                url: u,
                            });
                        }
                    }
                    if let Some(cands) = webpipe_local::rewrite::openreview_pdf_api_candidates(
                        resp_final_url.as_str(),
                    ) {
                        if let Some(u) = cands.into_iter().next() {
                            fb_specs.push(PdfHtmlFallbackSpec {
                                attempt_key: "openreview_pdf_api_fallback",
                                kind: "openreview_pdf_to_api_fallback",
                                warning_code: "openreview_pdf_fallback_to_api",
                                url: u,
                            });
                        }
                    }
                    if let Some(cands) = webpipe_local::rewrite::openreview_pdf_forum_candidates(
                        resp_final_url.as_str(),
                    ) {
                        if let Some(u) = cands.into_iter().next() {
                            fb_specs.push(PdfHtmlFallbackSpec {
                                attempt_key: "openreview_pdf_forum_fallback",
                                kind: "openreview_pdf_to_forum_fallback",
                                warning_code: "openreview_pdf_fallback_to_forum",
                                url: u,
                            });
                        }
                    }

                    for fb in fb_specs {
                        let fallback_url = fb.url.clone();
                        let pdf_from_url = resp_final_url.clone();
                        let pdf_engine0 = pipeline.extracted.engine;
                        let pdf_text_chars0 = pipeline.text_chars;
                        let pdf_warn0: Vec<&'static str> = pipeline.extracted.warnings.clone();

                        let t_fb0 = std::time::Instant::now();
                        let fb_req = FetchRequest {
                            url: fallback_url.clone(),
                            timeout_ms: req.timeout_ms,
                            max_bytes: req.max_bytes,
                            headers: BTreeMap::new(),
                            cache: FetchCachePolicy {
                                read: req.cache.read,
                                write: req.cache.write,
                                ttl_s: req.cache.ttl_s,
                            },
                        };
                        match self.fetcher.fetch(&fb_req).await {
                            Ok(resp2) => {
                                let webpipe_core::FetchResponse {
                                    url: fb_resp_url,
                                    final_url: fb_final_url,
                                    status: fb_status,
                                    content_type: fb_content_type,
                                    headers: _fb_headers,
                                    bytes: fb_bytes0,
                                    truncated: fb_body_truncated,
                                    source: _fb_source,
                                    timings_ms: fb_timings_ms,
                                } = resp2;

                                // Run extraction pipeline on the fallback HTML.
                                let fb_bytes = std::sync::Arc::new(fb_bytes0);
                                let bytes2 = fb_bytes.clone();
                                let ct2 = fb_content_type.clone();
                                let final_url2 = fb_final_url.clone();
                                let query2 = args.query.clone();

                                let extract_timeout_ms =
                                    std::env::var("WEBPIPE_EXTRACT_PIPELINE_TIMEOUT_MS")
                                        .ok()
                                        .and_then(|s| s.trim().parse::<u64>().ok())
                                        .unwrap_or(
                                            args.timeout_ms
                                                .unwrap_or(20_000)
                                                .saturating_sub(1_000)
                                                .clamp(4_000, 45_000),
                                        );

                                let fb_pipeline_opt = if extract_timeout_ms == 0 {
                                    None
                                } else {
                                    let handle = tokio::task::spawn_blocking(move || {
                                        let extracted0 =
                                            webpipe_local::extract::best_effort_text_from_bytes(
                                                &bytes2,
                                                ct2.as_deref(),
                                                final_url2.as_str(),
                                                width,
                                                500,
                                            );
                                        webpipe_local::extract::extract_pipeline_from_extracted(
                                            &bytes2,
                                            ct2.as_deref(),
                                            final_url2.as_str(),
                                            extracted0,
                                            webpipe_local::extract::ExtractPipelineCfg {
                                                query: query2.as_deref(),
                                                width,
                                                max_chars,
                                                top_chunks,
                                                max_chunk_chars,
                                                include_structure,
                                                max_outline_items,
                                                max_blocks,
                                                max_block_chars,
                                            },
                                        )
                                    });
                                    match tokio::time::timeout(
                                        std::time::Duration::from_millis(extract_timeout_ms),
                                        handle,
                                    )
                                    .await
                                    {
                                        Ok(join) => join.ok(),
                                        Err(_) => None,
                                    }
                                };

                                if let Some(fb_pipeline) = fb_pipeline_opt {
                                    // Prefer fallback when it yields non-empty evidence and
                                    // is not obviously worse than the PDF attempt.
                                    let fb_pen =
                                        Self::warning_penalty(&fb_pipeline.extracted.warnings);
                                    let pdf_pen = Self::warning_penalty(&pdf_warn0);
                                    let fb_nonempty = fb_pipeline.text_chars >= 80
                                        || !fb_pipeline.chunks.is_empty();
                                    if fb_status < 400 && fb_nonempty && fb_pen <= pdf_pen {
                                        pdf_html_fallback_used_warning = Some(fb.warning_code);
                                        attempts_map.insert(
                                            fb.attempt_key.to_string(),
                                            serde_json::json!({
                                                "kind": fb.kind,
                                                "ok": true,
                                                "from": pdf_from_url,
                                                "to": fb_final_url,
                                                "status": fb_status,
                                                "elapsed_ms": t_fb0.elapsed().as_millis(),
                                                "pdf": {
                                                    "engine": pdf_engine0,
                                                    "text_chars": pdf_text_chars0,
                                                    "warning_codes": pdf_warn0
                                                }
                                            }),
                                        );

                                        // Replace response parts + pipeline so downstream payload uses
                                        // the higher-signal HTML evidence.
                                        resp_url = fb_resp_url;
                                        resp_final_url = fb_final_url;
                                        resp_status = fb_status;
                                        resp_content_type = fb_content_type;
                                        resp_body_truncated = fb_body_truncated;
                                        resp_timings_ms = fb_timings_ms;
                                        resp_bytes = fb_bytes;
                                        pipeline = fb_pipeline;
                                        break;
                                    } else {
                                        attempts_map.insert(
                                            fb.attempt_key.to_string(),
                                            serde_json::json!({
                                                "kind": fb.kind,
                                                "ok": false,
                                                "from": pdf_from_url,
                                                "to": fb_final_url,
                                                "status": fb_status,
                                                "elapsed_ms": t_fb0.elapsed().as_millis(),
                                                "error": "fallback_not_better"
                                            }),
                                        );
                                    }
                                } else {
                                    attempts_map.insert(
                                        fb.attempt_key.to_string(),
                                        serde_json::json!({
                                            "kind": fb.kind,
                                            "ok": false,
                                            "from": pdf_from_url,
                                            "to": fallback_url,
                                            "elapsed_ms": t_fb0.elapsed().as_millis(),
                                            "error": "fallback_extract_timeout_or_failed"
                                        }),
                                    );
                                }
                            }
                            Err(e) => {
                                attempts_map.insert(
                                    fb.attempt_key.to_string(),
                                    serde_json::json!({
                                        "kind": fb.kind,
                                        "ok": false,
                                        "from": pdf_from_url,
                                        "to": fallback_url,
                                        "elapsed_ms": t_fb0.elapsed().as_millis(),
                                        "error": e.to_string()
                                    }),
                                );
                            }
                        }
                    }
                }
            }

            // Opportunistic multimodal: if we fetched an image and local extraction produced no text,
            // optionally call Gemini Flash (feature-gated, opt-in via WEBPIPE_VISION).
            #[cfg(feature = "vision-gemini")]
            {
                let vision_mode = webpipe_local::vision_gemini::gemini_enabled_mode_from_env();
                let is_image_like = resp_content_type
                    .as_deref()
                    .unwrap_or("")
                    .to_ascii_lowercase()
                    .starts_with("image/")
                    || webpipe_local::extract::bytes_look_like_image(resp_bytes.as_ref());
                if !no_network
                    && vision_mode != "off"
                    && is_image_like
                    && pipeline.text_chars == 0
                    && !resp_bytes.is_empty()
                    && webpipe_local::vision_gemini::gemini_api_key_from_env().is_some()
                {
                    let mime0 = resp_content_type
                        .as_deref()
                        .unwrap_or("application/octet-stream");
                    let mime = mime0.split(';').next().unwrap_or(mime0).trim();

                    match webpipe_local::vision_gemini::gemini_image_to_text(
                        self.http.clone(),
                        resp_bytes.as_ref(),
                        mime,
                    )
                    .await
                    {
                        Ok((model_id, text)) => {
                            let ex = webpipe_local::extract::ExtractedText {
                                engine: "gemini_vision",
                                text,
                                warnings: vec!["gemini_used"],
                            };
                            let bytes = resp_bytes.clone();
                            let ct = resp_content_type.clone();
                            let final_url = resp_final_url.clone();
                            let query = args.query.clone();
                            match tokio::task::spawn_blocking(move || {
                                webpipe_local::extract::extract_pipeline_from_extracted(
                                    &bytes,
                                    ct.as_deref(),
                                    final_url.as_str(),
                                    ex,
                                    webpipe_local::extract::ExtractPipelineCfg {
                                        query: query.as_deref(),
                                        width,
                                        max_chars,
                                        top_chunks,
                                        max_chunk_chars,
                                        include_structure,
                                        max_outline_items,
                                        max_blocks,
                                        max_block_chars,
                                    },
                                )
                            })
                            .await
                            {
                                Ok(p2) => pipeline = p2,
                                Err(_) => {
                                    pipeline.extracted.warnings.push("gemini_pipeline_failed")
                                }
                            }
                            vision_model = Some(model_id);
                        }
                        Err(code) => {
                            if vision_mode == "strict" {
                                pipeline.extracted.warnings.push(code);
                            } else {
                                pipeline.extracted.warnings.push("gemini_failed");
                            }
                        }
                    }
                }
            }

            let extracted = pipeline.extracted;
            let text = extracted.text.clone();
            let n = pipeline.text_chars;
            let clipped = pipeline.text_truncated;
            let empty_extraction = n == 0 && !resp_bytes.is_empty();
            let is_pdf_like = Self::content_type_is_pdf(resp_content_type.as_deref())
                || Self::url_looks_like_pdf(resp_final_url.as_str())
                || webpipe_local::extract::bytes_look_like_pdf(&resp_bytes)
                || extracted.engine.starts_with("pdf-");
            let mut warnings: Vec<&'static str> = Vec::new();
            if resp_body_truncated {
                warnings.push("body_truncated_by_max_bytes");
            }
            if resp_timings_ms.contains_key("cache_get_timeout")
                || resp_timings_ms.contains_key("cache_put_timeout")
            {
                warnings.push("cache_io_timeout");
            }
            if attempts_map.contains_key("truncation_retry") {
                // Additive: indicates we attempted a tail-recovery fetch.
                // (The attempts_map entry contains details.)
                let ok = attempts_map
                    .get("truncation_retry")
                    .and_then(|v| v.get("ok"))
                    .and_then(|v| v.as_bool())
                    .unwrap_or(false);
                if ok {
                    warnings.push("truncation_retry_used");
                } else {
                    warnings.push("truncation_retry_failed");
                }
            }
            if clipped {
                warnings.push("text_truncated_by_max_chars");
            }
            if empty_extraction {
                warnings.push("empty_extraction");
            }
            for w in &extracted.warnings {
                warnings.push(*w);
            }
            if no_network && !url_is_localhost(&req.url) {
                warnings.push("cache_only");
            }
            let status_bad = resp_status >= 400;
            if status_bad && !warnings.contains(&"http_status_error") {
                warnings.push("http_status_error");
            }
            if resp_status == 429 && !warnings.contains(&"http_rate_limited") {
                warnings.push("http_rate_limited");
            }
            let title_opt = pipeline.structure.as_ref().and_then(|s| s.title.as_deref());
            if looks_like_js_challenge(resp_status, &text, title_opt)
                && !warnings.contains(&"blocked_by_js_challenge")
            {
                warnings.push("blocked_by_js_challenge");
            }
            if extracted.engine == "html_main"
                && resp_bytes.len() >= 50_000
                && pipeline
                    .structure
                    .as_ref()
                    .is_some_and(structure_looks_like_ui_shell)
                && extracted_text_looks_like_nav_shell(&text)
            {
                warnings.push("main_content_low_signal");
            }
            // Treat “script-only / no-semantic-tags” HTML as low-signal even when it yields a tiny
            // amount of text (e.g., inline JS).
            let ct0 = resp_content_type
                .as_deref()
                .unwrap_or("")
                .split(';')
                .next()
                .unwrap_or("")
                .trim()
                .to_ascii_lowercase();
            let html_like = ct0 == "text/html"
                || ct0 == "application/xhtml+xml"
                || webpipe_local::extract::bytes_look_like_html(resp_bytes.as_ref());
            if html_like
                && (extracted.engine.starts_with("html_")
                    || extracted.engine == "html2text"
                    || extracted.engine == "unknown")
                && n <= 200
                && pipeline
                    .structure
                    .as_ref()
                    .map(|s| s.blocks.is_empty() || s.structure_text.trim().is_empty())
                    .unwrap_or(true)
            {
                warnings.push("main_content_low_signal");
            }
            // Also treat “tiny but JS-shaped” extraction as low-signal, even if structure exists.
            if (extracted.engine.starts_with("html_") || extracted.engine == "html2text")
                && n <= 200
                && Self::looks_like_bundle_gunk(&text)
            {
                warnings.push("main_content_low_signal");
            }
            if arxiv_rewrote {
                warnings.push("arxiv_abs_rewritten_to_pdf");
            }
            if let Some(w) = pdf_html_fallback_used_warning {
                warnings.push(w);
            }
            if gh_issue_rewrote {
                warnings.push("github_issue_rewritten_to_api");
            }
            if gh_release_rewrote {
                warnings.push("github_release_rewritten_to_api");
            }
            if github_patch_rewrote {
                if let Some(w) = gh_patch_warn {
                    warnings.push(w);
                }
            }
            if github_blob_rewrote {
                warnings.push("github_blob_rewritten_to_raw");
            }
            if gist_rewrote {
                warnings.push("gist_rewritten_to_raw");
            }
            if github_repo_rewrote {
                warnings.push("github_repo_rewritten_to_raw_readme");
            }

            let mut payload = serde_json::json!({
                "ok": true,
                "fetch_backend": fetch_backend,
                "url": url,
                "fetched_url": resp_url,
                "final_url": resp_final_url,
                "status": resp_status,
                "content_type": resp_content_type,
                "bytes": resp_bytes.len(),
                "truncated": resp_body_truncated || clipped,
                "timings_ms": {
                    "total": t0.elapsed().as_millis()
                }
            });
            add_envelope_fields(&mut payload, "web_extract", t0.elapsed().as_millis());

            payload["request"] = serde_json::json!({
                "fetch_backend": fetch_backend,
                "no_network": no_network,
                "timeout_ms": req.timeout_ms,
                "max_bytes": req.max_bytes,
                "retry_on_truncation": retry_on_truncation_arg,
                "truncation_retry_max_bytes": truncation_retry_max_bytes,
                "cache": { "read": req.cache.read, "write": req.cache.write, "ttl_s": req.cache.ttl_s },
                "width": width,
                "max_chars": max_chars,
                "query": args.query,
                "include_text": include_text,
                "include_links": include_links,
                "max_links": max_links,
                "include_structure": include_structure,
                "max_outline_items": max_outline_items,
                "max_blocks": max_blocks,
                "max_block_chars": max_block_chars,
                "semantic_rerank": semantic_rerank,
                "semantic_auto_fallback": semantic_auto_fallback,
                "semantic_top_k": semantic_top_k
            });
            if let Some(m) = render_meta {
                payload["render"] = m;
            }

            if include_links && is_pdf_like {
                warnings.push("links_unavailable_for_pdf");
            }
            if !warnings.is_empty() {
                payload["warnings"] = serde_json::json!(warnings);
                let codes = warning_codes_from(&warnings);
                payload["warning_codes"] = serde_json::json!(codes.clone());
                payload["warning_hints"] = warning_hints_from(&codes);
            }

            // Canonical extract object (no legacy mirrors).
            payload["extract"] = serde_json::json!({
                "engine": extracted.engine,
                "width": width,
                "max_chars": max_chars,
                "text_chars": n,
                "text_truncated": clipped,
                "top_chunks": top_chunks,
                "max_chunk_chars": max_chunk_chars,
                "chunks": pipeline.chunks
            });
            if include_text {
                payload["extract"]["text"] = serde_json::json!(text);
            }
            // Deterministic quality scorecard (tail-risk detector).
            // Additive: safe for existing consumers.
            let quality = Self::quality_scorecard(
                args.query.as_deref(),
                &text,
                resp_status,
                extracted.engine,
                &warnings,
            );
            payload["extract"]["quality"] = quality;

            if let Some(vm) = vision_model.as_ref() {
                payload["extract"]["vision_model"] = serde_json::json!(vm);
            }

            if include_structure {
                if let Some(s) = pipeline.structure.as_ref() {
                    payload["extract"]["structure"] = serde_json::json!(s);
                }
            }

            if let Some(q) = payload["request"]["query"].as_str().map(|s| s.to_string()) {
                let auto_ok = semantic_auto_fallback && !semantic_rerank;
                if semantic_rerank {
                    let cands: Vec<(usize, usize, String)> = pipeline
                        .chunks
                        .iter()
                        .map(|c| (c.start_char, c.end_char, c.text.clone()))
                        .collect();
                    let q0 = q.clone();
                    let semantic_timeout_ms = std::env::var("WEBPIPE_SEMANTIC_TIMEOUT_MS")
                        .ok()
                        .and_then(|s| s.trim().parse::<u64>().ok())
                        .unwrap_or(
                            args.timeout_ms
                                .unwrap_or(20_000)
                                .saturating_div(2)
                                .clamp(2_000, 8_000),
                        );
                    let sem = if semantic_timeout_ms == 0 {
                        webpipe_local::semantic::SemanticRerankResult {
                            ok: false,
                            backend: "unknown".to_string(),
                            model_id: None,
                            cache_hits: 0,
                            cache_misses: 0,
                            chunks: Vec::new(),
                            warnings: vec!["semantic_rerank_timeout"],
                        }
                    } else {
                        let handle = tokio::task::spawn_blocking(move || {
                            webpipe_local::semantic::semantic_rerank_chunks(
                                &q0,
                                &cands,
                                semantic_top_k,
                            )
                        });
                        match tokio::time::timeout(
                            std::time::Duration::from_millis(semantic_timeout_ms),
                            handle,
                        )
                        .await
                        {
                            Ok(join) => join.unwrap_or_else(|_| {
                                webpipe_local::semantic::SemanticRerankResult {
                                    ok: false,
                                    backend: "unknown".to_string(),
                                    model_id: None,
                                    cache_hits: 0,
                                    cache_misses: 0,
                                    chunks: Vec::new(),
                                    warnings: vec!["semantic_rerank_task_failed"],
                                }
                            }),
                            Err(_) => webpipe_local::semantic::SemanticRerankResult {
                                ok: false,
                                backend: "unknown".to_string(),
                                model_id: None,
                                cache_hits: 0,
                                cache_misses: 0,
                                chunks: Vec::new(),
                                warnings: vec!["semantic_rerank_timeout"],
                            },
                        }
                    };
                    for w in &sem.warnings {
                        warnings.push(*w);
                    }
                    payload["extract"]["semantic"] = serde_json::json!(sem);
                } else if auto_ok
                    && Self::openrouter_api_key_from_env().is_some()
                    && n >= 1500
                    && !pipeline.chunks.is_empty()
                    && pipeline.chunks.iter().all(|c| c.score <= 1)
                    && extracted.engine != "unknown"
                    && extracted.engine != "html_hint"
                {
                    // Single bounded pass: only when lexical chunk scoring looks ineffective.
                    let cands: Vec<(usize, usize, String)> = pipeline
                        .chunks
                        .iter()
                        .map(|c| (c.start_char, c.end_char, c.text.clone()))
                        .collect();
                    let sem = self
                        .semantic_rerank_chunks_best(&q, &cands, semantic_top_k)
                        .await;
                    for w in &sem.warnings {
                        warnings.push(*w);
                    }
                    if sem.ok {
                        warnings.push("semantic_auto_fallback_used");
                        payload["warnings"] = serde_json::json!(warnings);
                        let codes = warning_codes_from(&warnings);
                        payload["warning_codes"] = serde_json::json!(codes.clone());
                        payload["warning_hints"] = warning_hints_from(&codes);
                    }
                    payload["extract"]["semantic"] = serde_json::json!(sem);
                }
            }

            if include_links {
                if is_pdf_like {
                    payload["extract"]["links"] = serde_json::json!([]);
                    payload["extract"]["max_links"] = serde_json::json!(max_links);
                } else {
                    let bytes = resp_bytes.clone();
                    let base_url = payload["final_url"].as_str().unwrap_or("").to_string();
                    let links_timeout_ms = std::env::var("WEBPIPE_LINKS_TIMEOUT_MS")
                        .ok()
                        .and_then(|s| s.trim().parse::<u64>().ok())
                        .unwrap_or(1_500);
                    let links = if links_timeout_ms == 0 {
                        warnings.push("links_timeout");
                        Vec::new()
                    } else {
                        let handle = tokio::task::spawn_blocking(move || {
                            let html = String::from_utf8_lossy(bytes.as_ref()).to_string();
                            webpipe_local::links::extract_links(
                                &html,
                                Some(base_url.as_str()),
                                max_links,
                            )
                        });
                        match tokio::time::timeout(
                            std::time::Duration::from_millis(links_timeout_ms),
                            handle,
                        )
                        .await
                        {
                            Ok(join) => join.unwrap_or_else(|_| Vec::new()),
                            Err(_) => {
                                warnings.push("links_timeout");
                                Vec::new()
                            }
                        }
                    };
                    payload["extract"]["links"] = serde_json::json!(links);
                    payload["extract"]["max_links"] = serde_json::json!(max_links);
                }
            }

            // Include any late-added warnings (e.g. links_timeout) in the final envelope.
            if !warnings.is_empty() {
                payload["warnings"] = serde_json::json!(warnings);
                let codes = warning_codes_from(&warnings);
                payload["warning_codes"] = serde_json::json!(codes.clone());
                payload["warning_hints"] = warning_hints_from(&codes);
            }

            self.stats_record_fetch_backend(
                fetch_backend.as_str(),
                true,
                t0.elapsed().as_millis() as u64,
                None,
            );
            payload["attempts"] = if attempts_map.is_empty() {
                serde_json::Value::Null
            } else {
                serde_json::Value::Object(attempts_map)
            };
            // Mirror extract.chunks as top_chunks at the top level for API consistency with
            // search_evidence, which returns top_chunks[] at the response root.
            // Agents can write `response.top_chunks` consistently for both tools.
            // extract.chunks remains at its current location for backward compat.
            if let Some(chunks) = payload["extract"].get("chunks").cloned() {
                payload["top_chunks"] = chunks;
            }

            let md = web_extract_markdown(&payload);
            Ok(tool_result_markdown_with_json(payload, md))
        }

        #[tool(
            description = "DEPRECATED: use web_extract instead (identical args and behavior). This alias exists for backward compatibility only and is excluded from the default toolset.",
            input_schema = Arc::new(tool_input_schema_draft07::<WebExtractArgs>()),
            annotations(title = "Page extract (alias)", read_only_hint = true, open_world_hint = true)
        )]
        async fn page_extract(
            &self,
            params: Parameters<Option<WebExtractArgs>>,
        ) -> Result<CallToolResult, McpError> {
            self.web_extract(params).await
        }

        #[tool(
            description = "DEPRECATED: use web_extract with include_text=true instead. This alias exists for backward compatibility only and is excluded from the default toolset.",
            input_schema = Arc::new(tool_input_schema_draft07::<WebExtractArgs>()),
            annotations(title = "Page extract (text)", read_only_hint = true, open_world_hint = true)
        )]
        async fn page_extract_text(
            &self,
            params: Parameters<Option<WebExtractArgs>>,
        ) -> Result<CallToolResult, McpError> {
            let mut args = params.0.unwrap_or_default();
            args.include_text = Some(true);
            self.web_extract(Parameters(Some(args))).await
        }
    }

    // ---- Prompts (MCP) ----
    //
    // Prompts are *templates* for client UIs (e.g. Cursor) to kick off common workflows.
    // They do not add new server-side capabilities; they only provide consistent, bounded
    // instructions that steer clients toward the best tools/defaults.
    //
    // MCP prompt surface: prompts/list + prompts/get (rmcp handles the protocol glue).

    #[derive(Debug, Deserialize, JsonSchema)]
    struct PromptSearchExtractArgs {
        /// What you want to find / answer.
        query: String,
        /// If true, the prompt will recommend cache-only behavior (no external network).
        #[serde(default)]
        no_network: Option<bool>,
    }

    #[derive(Debug, Deserialize, JsonSchema)]
    struct PromptOfflineCacheArgs {
        /// What you want to find / answer.
        query: String,
    }

    #[derive(Debug, Deserialize, JsonSchema)]
    struct PromptDeepResearchArgs {
        /// Research question.
        query: String,
        /// If true, request evidence in the output (URLs + top chunks).
        #[serde(default)]
        include_evidence: Option<bool>,
    }

    #[prompt_router]
    impl WebpipeMcp {
        #[prompt(
            name = "webpipe_search_extract",
            description = "Evidence-pack workflow: use `search_evidence` (Sources + excerpts + top chunks), then answer with URL citations."
        )]
        async fn prompt_webpipe_search_extract(
            &self,
            Parameters(args): Parameters<PromptSearchExtractArgs>,
        ) -> Result<Vec<PromptMessage>, McpError> {
            let no_network = args.no_network.unwrap_or(false);
            let sys = "You are a careful assistant. Use webpipe tools to gather evidence, then answer.\n\nRules:\n- Prefer `search_evidence` for most questions (it returns an evidence pack: Sources + excerpts + top chunks).\n- Keep bounds small (max_urls/top_chunks/max_chars) unless the user asks for exhaustive coverage.\n- Cite sources by URL (or by source index [1], [2] from the Markdown Sources list).\n- If evidence is insufficient, say so and suggest what to fetch/search next.";
            let user = if no_network {
                format!(
                    "Question:\n{}\n\nStay offline/cache-only:\n- Prefer `search_evidence` with no_network=true.\n  - If you know the sources: pass urls=[...]\n  - If you don’t: omit urls to search over WEBPIPE_CACHE_DIR (cache-corpus mode)\n- For any web_fetch/web_extract, set no_network=true (and keep bounds small).\n\nReturn an answer with citations (URLs).",
                    args.query.trim()
                )
            } else {
                format!(
                    "Question:\n{}\n\nUse `search_evidence` with bounded defaults (provider=\"auto\", auto_mode=\"fallback\", max_results≈5, max_urls≈3, top_chunks≈5).\n\nReturn an answer with citations (URLs).",
                    args.query.trim()
                )
            };
            // rmcp prompt messages support only User/Assistant roles (no System).
            // To avoid “fake system”, embed the system instructions into the user message.
            Ok(vec![PromptMessage::new_text(
                PromptMessageRole::User,
                format!("{sys}\n\n{user}"),
            )])
        }

        #[prompt(
            name = "webpipe_offline_cache_first",
            description = "Offline/cache-first workflow: use `search_evidence` (cache-corpus) over WEBPIPE_CACHE_DIR; avoid network."
        )]
        async fn prompt_webpipe_offline_cache_first(
            &self,
            Parameters(args): Parameters<PromptOfflineCacheArgs>,
        ) -> Result<Vec<PromptMessage>, McpError> {
            let sys = "You are a careful assistant. You must stay offline.\n\nRules:\n- Do not use network providers.\n- Prefer `search_evidence` with no_network=true (cache-only).\n- For any web_fetch/web_extract, set no_network=true.\n- If the cache is missing, say what needs to be warmed (do not guess).";
            let user = format!(
                "Question:\n{}\n\nStay offline: use WEBPIPE_CACHE_DIR (cache-only). Provide an answer with citations if possible.",
                args.query.trim()
            );
            Ok(vec![PromptMessage::new_text(
                PromptMessageRole::User,
                format!("{sys}\n\n{user}"),
            )])
        }

        #[prompt(
            name = "webpipe_deep_research",
            description = "Agentic research: gather evidence packs with `search_evidence`, then optionally synthesize."
        )]
        async fn prompt_webpipe_deep_research(
            &self,
            Parameters(args): Parameters<PromptDeepResearchArgs>,
        ) -> Result<Vec<PromptMessage>, McpError> {
            let include_evidence = args.include_evidence.unwrap_or(true);
            let sys = "You are a careful research assistant. Use webpipe tools to gather evidence and then (optionally) synthesize.\n\nRules:\n- Use `search_evidence` iteratively to gather 2–5 independent sources.\n- Keep bounds small unless asked.\n- Cite sources by URL.\n- If evidence is insufficient, say so and list the missing evidence.";
            let user = format!(
                "Research question:\n{}\n\nGather evidence with `search_evidence` (bounded defaults: max_results≈5, max_urls≈3, top_chunks≈5). Then return a concise answer. include_evidence={}: {}",
                args.query.trim(),
                include_evidence,
                if include_evidence { "include a short Sources list + the best supporting chunks." } else { "include sources only if essential; do not dump evidence." }
            );
            Ok(vec![PromptMessage::new_text(
                PromptMessageRole::User,
                format!("{sys}\n\n{user}"),
            )])
        }
    }

    impl rmcp::ServerHandler for WebpipeMcp {
        fn get_info(&self) -> ServerInfo {
            ServerInfo {
                instructions: Some(
                    "Local fetch/search plumbing for Cursor.\n\nUse `search_evidence` for most queries: it returns Sources + short excerpts + top chunks (bounded), plus a structured JSON payload.\n\nHard mode (render/unblock) is opt-in; outputs are schema-versioned."
                        .to_string(),
                ),
                capabilities: ServerCapabilities::builder()
                    .enable_tools()
                    .enable_prompts()
                    .enable_resources()
                    .build(),
                ..Default::default()
            }
        }

        fn list_tools(
            &self,
            _request: Option<PaginatedRequestParam>,
            _context: RequestContext<RoleServer>,
        ) -> impl std::future::Future<Output = Result<ListToolsResult, McpError>> + Send + '_
        {
            let toolset = mcp_toolset_from_env();
            let tools = self
                .tool_router
                .list_all()
                .into_iter()
                .filter(|t| mcp_tool_allowed(t.name.as_ref(), toolset))
                .collect::<Vec<_>>();
            std::future::ready(Ok(ListToolsResult::with_all_items(tools)))
        }

        fn call_tool(
            &self,
            request: rmcp::model::CallToolRequestParam,
            context: RequestContext<RoleServer>,
        ) -> impl std::future::Future<Output = Result<CallToolResult, McpError>> + Send + '_
        {
            use rmcp::handler::server::tool::ToolCallContext;
            let toolset = mcp_toolset_from_env();
            let name = request.name.to_string();
            async move {
                if !mcp_tool_allowed(name.as_str(), toolset) {
                    return Err(McpError::invalid_params(
                        format!(
                            "tool is not enabled in WEBPIPE_MCP_TOOLSET={}",
                            mcp_toolset_name(toolset)
                        ),
                        Some(serde_json::json!({
                            "code":"tool_not_enabled",
                            "tool": name,
                            "toolset": mcp_toolset_name(toolset),
                            "hint": "Set WEBPIPE_MCP_TOOLSET=debug to expose the full tool surface."
                        })),
                    ));
                }
                let ctx = ToolCallContext::new(self, request, context);
                self.tool_router.call(ctx).await
            }
        }

        fn list_prompts(
            &self,
            _request: Option<PaginatedRequestParam>,
            _context: RequestContext<RoleServer>,
        ) -> impl std::future::Future<Output = Result<ListPromptsResult, McpError>> + Send + '_
        {
            std::future::ready(Ok(ListPromptsResult::with_all_items(
                self.prompt_router.list_all(),
            )))
        }

        fn get_prompt(
            &self,
            request: GetPromptRequestParam,
            context: RequestContext<RoleServer>,
        ) -> impl std::future::Future<Output = Result<GetPromptResult, McpError>> + Send + '_
        {
            use rmcp::handler::server::prompt::PromptContext;
            async move {
                self.prompt_router
                    .get_prompt(PromptContext::new(
                        self,
                        request.name.to_string(),
                        request.arguments,
                        context,
                    ))
                    .await
            }
        }

        async fn list_resources(
            &self,
            _request: Option<PaginatedRequestParam>,
            _context: RequestContext<RoleServer>,
        ) -> Result<ListResourcesResult, McpError> {
            let resources: Vec<Resource> = vec![
                Annotated::new(
                    RawResource {
                        uri: "webpipe://meta".to_string(),
                        name: "meta.json".to_string(),
                        title: Some("Webpipe meta".to_string()),
                        description: Some(
                            "Same JSON payload as webpipe_meta (no secrets): capabilities, configured backends, defaults."
                                .to_string(),
                        ),
                        mime_type: Some("application/json".to_string()),
                        size: None,
                        icons: None,
                        meta: None,
                    },
                    None,
                ),
                Annotated::new(
                    RawResource {
                        uri: "webpipe://usage".to_string(),
                        name: "usage.json".to_string(),
                        title: Some("Webpipe usage".to_string()),
                        description: Some(
                            "Same JSON payload as webpipe_usage: in-process counts/cost units since start (no secrets)."
                                .to_string(),
                        ),
                        mime_type: Some("application/json".to_string()),
                        size: None,
                        icons: None,
                        meta: None,
                    },
                    None,
                ),
            ];
            Ok(ListResourcesResult::with_all_items(resources))
        }

        async fn read_resource(
            &self,
            request: ReadResourceRequestParam,
            _context: RequestContext<RoleServer>,
        ) -> Result<ReadResourceResult, McpError> {
            let uri = request.uri.trim().to_string();
            let (text, mime_type) = match uri.as_str() {
                "webpipe://meta" => {
                    let r = self.webpipe_meta(Parameters(None)).await?;
                    let v = r.structured_content.clone().ok_or_else(|| {
                        McpError::internal_error(
                            "webpipe_meta missing structured_content".to_string(),
                            None,
                        )
                    })?;
                    let s = serde_json::to_string(&v).unwrap_or_default();
                    (s, "application/json".to_string())
                }
                "webpipe://usage" => {
                    let r = self.webpipe_usage(Parameters(None)).await?;
                    let v = r.structured_content.clone().ok_or_else(|| {
                        McpError::internal_error(
                            "webpipe_usage missing structured_content".to_string(),
                            None,
                        )
                    })?;
                    let s = serde_json::to_string(&v).unwrap_or_default();
                    (s, "application/json".to_string())
                }
                _ => {
                    return Err(McpError::resource_not_found(
                        format!("unknown resource uri: {uri}"),
                        Some(serde_json::json!({
                            "known_uris": ["webpipe://meta", "webpipe://usage"]
                        })),
                    ));
                }
            };

            Ok(ReadResourceResult {
                contents: vec![ResourceContents::TextResourceContents {
                    uri,
                    mime_type: Some(mime_type),
                    text,
                    meta: None,
                }],
            })
        }
    }

    // `rmcp`'s stdio codec is line-delimited JSON-RPC (one JSON message per line) and it
    // rejects JSON-RPC *batch arrays* (a single line containing a JSON array), closing the
    // transport. Cursor may batch `tools/list`, `prompts/list`, and `resources/list`, which
    // showed up as “0 tools” in Cursor logs.
    //
    // This transport preserves the same wire format (newline-delimited JSON objects) but
    // additionally accepts a JSON array line and expands it into individual messages.
    #[cfg(feature = "stdio")]
    struct BatchLineStdioTransport {
        reader: tokio::io::BufReader<tokio::io::Stdin>,
        writer: std::sync::Arc<tokio::sync::Mutex<tokio::io::BufWriter<tokio::io::Stdout>>>,
        pending: std::collections::VecDeque<rmcp::service::RxJsonRpcMessage<rmcp::RoleServer>>,
    }

    #[cfg(feature = "stdio")]
    impl BatchLineStdioTransport {
        fn new() -> Self {
            Self {
                reader: tokio::io::BufReader::new(tokio::io::stdin()),
                writer: std::sync::Arc::new(tokio::sync::Mutex::new(tokio::io::BufWriter::new(
                    tokio::io::stdout(),
                ))),
                pending: std::collections::VecDeque::new(),
            }
        }
    }

    #[cfg(feature = "stdio")]
    impl rmcp::transport::Transport<rmcp::RoleServer> for BatchLineStdioTransport {
        type Error = std::io::Error;

        fn send(
            &mut self,
            item: rmcp::service::TxJsonRpcMessage<rmcp::RoleServer>,
        ) -> impl std::future::Future<Output = Result<(), Self::Error>> + Send + 'static {
            use tokio::io::AsyncWriteExt;
            let writer = self.writer.clone();
            async move {
                let mut w = writer.lock().await;
                let s = serde_json::to_string(&item)
                    .map_err(|e| std::io::Error::other(format!("json encode: {e}")))?;
                w.write_all(s.as_bytes()).await?;
                w.write_all(b"\n").await?;
                w.flush().await?;
                Ok(())
            }
        }

        fn receive(
            &mut self,
        ) -> impl std::future::Future<
            Output = Option<rmcp::service::RxJsonRpcMessage<rmcp::RoleServer>>,
        > + Send {
            use tokio::io::AsyncBufReadExt;
            async move {
                if let Some(m) = self.pending.pop_front() {
                    return Some(m);
                }

                let mut line = String::new();
                loop {
                    line.clear();
                    let n = match self.reader.read_line(&mut line).await {
                        Ok(n) => n,
                        Err(_) => return None,
                    };
                    if n == 0 {
                        return None;
                    }
                    let trimmed = line.trim();
                    if trimmed.is_empty() {
                        continue;
                    }

                    // Be forgiving: ignore non-JSON (incl. accidental LSP-style headers).
                    let val: serde_json::Value = match serde_json::from_str(trimmed) {
                        Ok(v) => v,
                        Err(_) => continue,
                    };

                    match val {
                        serde_json::Value::Array(items) => {
                            for v in items.into_iter() {
                                if let Ok(m) = serde_json::from_value::<
                                    rmcp::service::RxJsonRpcMessage<rmcp::RoleServer>,
                                >(v)
                                {
                                    self.pending.push_back(m);
                                }
                            }
                            if let Some(m) = self.pending.pop_front() {
                                return Some(m);
                            }
                            continue;
                        }
                        other => {
                            if let Ok(m) = serde_json::from_value::<
                                rmcp::service::RxJsonRpcMessage<rmcp::RoleServer>,
                            >(other)
                            {
                                return Some(m);
                            }
                            continue;
                        }
                    }
                }
            }
        }

        fn close(&mut self) -> impl std::future::Future<Output = Result<(), Self::Error>> + Send {
            use tokio::io::AsyncWriteExt;
            let writer = self.writer.clone();
            async move {
                let mut w = writer.lock().await;
                w.flush().await
            }
        }
    }

    pub(crate) async fn serve_stdio() -> Result<(), McpError> {
        let svc = WebpipeMcp::new()?;
        let running = svc
            .serve(BatchLineStdioTransport::new())
            .await
            .map_err(|e| McpError::internal_error(e.to_string(), None))?;
        // Keep the stdio server alive until the client closes.
        running
            .waiting()
            .await
            .map_err(|e| McpError::internal_error(e.to_string(), None))?;
        Ok(())
    }

    #[cfg(test)]
    mod tests {
        use super::*;
        use proptest::prelude::*;

        fn p<T>(v: T) -> Parameters<Option<T>> {
            Parameters(Some(v))
        }

        struct EnvGuard {
            // Hold the lock for the full test (env vars are process-global).
            _lock: std::sync::MutexGuard<'static, ()>,
            saved: Vec<(String, Option<String>)>,
        }

        impl EnvGuard {
            fn new(keys: &[&str]) -> Self {
                // If a prior test panicked while holding the lock, recover the guard so we
                // don't cascade failures behind a PoisonError. (Env is process-global anyway.)
                let lock = ENV_LOCK.lock().unwrap_or_else(|e| e.into_inner());
                let saved: Vec<(String, Option<String>)> = keys
                    .iter()
                    .map(|k| (k.to_string(), std::env::var(k).ok()))
                    .collect();
                for (k, _) in &saved {
                    std::env::remove_var(k);
                }
                Self { _lock: lock, saved }
            }

            fn set(&self, k: &str, v: &str) {
                std::env::set_var(k, v);
            }
        }

        impl Drop for EnvGuard {
            fn drop(&mut self) {
                for (k, v) in self.saved.drain(..) {
                    match v {
                        Some(val) => std::env::set_var(k, val),
                        None => std::env::remove_var(k),
                    }
                }
            }
        }

        fn payload_from_call_tool_result(r: &CallToolResult) -> serde_json::Value {
            // Prefer MCP `structured_content` (stable machine payload). Tool `content` may be
            // Markdown or other human-friendly text.
            if let Some(v) = r.structured_content.clone() {
                return v;
            }
            // Fallback: try to parse the first JSON-looking text item.
            for c in &r.content {
                let Some(t) = c.as_text() else { continue };
                if let Ok(v) = serde_json::from_str::<serde_json::Value>(&t.text) {
                    return v;
                }
            }
            panic!("tool result should contain structured_content or a JSON text item");
        }

        // Env vars are global; serialize tests that mutate them.
        static ENV_LOCK: std::sync::Mutex<()> = std::sync::Mutex::new(());
        const SEARCH_ENV_KEYS: [&str; 6] = [
            "WEBPIPE_BRAVE_API_KEY",
            "BRAVE_SEARCH_API_KEY",
            "WEBPIPE_TAVILY_API_KEY",
            "TAVILY_API_KEY",
            "WEBPIPE_SEARXNG_ENDPOINT",
            "WEBPIPE_ARXIV_ENDPOINT",
        ];
        const PERPLEXITY_ENV_KEYS: [&str; 3] = [
            "WEBPIPE_PERPLEXITY_API_KEY",
            "PERPLEXITY_API_KEY",
            "WEBPIPE_PERPLEXITY_ENDPOINT",
        ];

        #[test]
        fn web_search_extract_markdown_next_suggests_domains_allow_for_rust_when_forum_sources_present(
        ) {
            let payload = serde_json::json!({
                "ok": true,
                "query": "evoc Rust crate documentation",
                "mode": "search",
                "request": { "fetch_backend": "local" },
                "results": [
                    {
                        "url": "https://reddit.com/r/rust/comments/abc123/evoc/",
                        "final_url": "https://reddit.com/r/rust/comments/abc123/evoc/",
                        "status": 200,
                        "title": "evoc",
                        "warning_codes": []
                    }
                ],
                "top_chunks": [
                    {
                        "url": "https://reddit.com/r/rust/comments/abc123/evoc/",
                        "score": 1,
                        "text": "evoc discussion thread"
                    }
                ]
            });
            let md = web_search_extract_markdown(&payload);
            assert!(
                md.contains("domains_allow"),
                "expected markdown to suggest domains_allow; md={md}"
            );
        }

        #[test]
        fn web_search_extract_markdown_next_suggests_repo_ingest_for_github_repo_root_when_apiish_query(
        ) {
            let payload = serde_json::json!({
                "ok": true,
                "query": "API methods",
                "mode": "urls",
                "request": { "fetch_backend": "local" },
                "results": [
                    {
                        "url": "https://github.com/foo/bar",
                        "final_url": "https://raw.githubusercontent.com/foo/bar/main/README.md",
                        "status": 200,
                        "title": "bar",
                        "warning_codes": ["github_repo_rewritten_to_raw_readme"]
                    }
                ],
                "top_chunks": [
                    {
                        "url": "https://github.com/foo/bar",
                        "score": 1,
                        "text": "README excerpt"
                    }
                ]
            });
            let md = web_search_extract_markdown(&payload);
            assert!(
                md.contains("repo_ingest"),
                "expected markdown to suggest repo_ingest; md={md}"
            );
        }

        #[test]
        fn routing_context_query_key_prefers_contextual_window() {
            let env = EnvGuard::new(&[
                "WEBPIPE_ROUTING_CONTEXT",
                "WEBPIPE_ROUTING_MAX_CONTEXTS",
                "WEBPIPE_ROUTING_WINDOW",
            ]);
            env.set("WEBPIPE_ROUTING_CONTEXT", "query_key");
            env.set("WEBPIPE_ROUTING_MAX_CONTEXTS", "10");
            env.set("WEBPIPE_ROUTING_WINDOW", "10");

            let mcp = WebpipeMcp::new().expect("mcp new");

            // Seed contextual stats: brave is junky, tavily is clean.
            mcp.stats_record_search_provider_qk("brave", true, 1, 10, None, Some("qk1"));
            mcp.stats_set_last_search_outcome_junk_level_qk("brave", true, false, Some("qk1"));

            mcp.stats_record_search_provider_qk("tavily", true, 1, 10, None, Some("qk1"));
            mcp.stats_set_last_search_outcome_junk_level_qk("tavily", false, false, Some("qk1"));

            let (summaries, used) = mcp.snapshot_search_summaries_for_query_key(Some("qk1"));
            assert_eq!(used, "query_key");

            let arms = vec!["brave".to_string(), "tavily".to_string()];
            let cfg = muxer::MabConfig {
                exploration_c: 0.0,
                ..muxer::MabConfig::default()
            };
            let sel = muxer::select_mab(&arms, &summaries, &cfg);
            assert_eq!(sel.chosen, "tavily");
        }

        #[test]
        fn routing_context_query_key_is_bounded_by_max_contexts() {
            let env = EnvGuard::new(&[
                "WEBPIPE_ROUTING_CONTEXT",
                "WEBPIPE_ROUTING_MAX_CONTEXTS",
                "WEBPIPE_ROUTING_WINDOW",
            ]);
            env.set("WEBPIPE_ROUTING_CONTEXT", "query_key");
            env.set("WEBPIPE_ROUTING_MAX_CONTEXTS", "1");
            env.set("WEBPIPE_ROUTING_WINDOW", "10");

            let mcp = WebpipeMcp::new().expect("mcp new");

            mcp.stats_record_search_provider_qk("brave", true, 1, 10, None, Some("qk_a"));
            mcp.stats_record_search_provider_qk("brave", true, 1, 10, None, Some("qk_b"));

            let s = mcp.stats_lock();
            assert!(s.search_windows_by_query_key.len() <= 1);
        }

        #[test]
        fn compute_search_junk_label_hard_always_true() {
            let env = EnvGuard::new(&[
                "WEBPIPE_ROUTING_SOFT_JUNK_MIN",
                "WEBPIPE_ROUTING_SOFT_JUNK_FRAC",
            ]);
            env.set("WEBPIPE_ROUTING_SOFT_JUNK_MIN", "2");
            env.set("WEBPIPE_ROUTING_SOFT_JUNK_FRAC", "0.5");
            assert!(WebpipeMcp::compute_search_junk_label(1, 0, 3));
        }

        #[test]
        fn compute_search_junk_label_soft_respects_min_and_frac() {
            let env = EnvGuard::new(&[
                "WEBPIPE_ROUTING_SOFT_JUNK_MIN",
                "WEBPIPE_ROUTING_SOFT_JUNK_FRAC",
            ]);
            env.set("WEBPIPE_ROUTING_SOFT_JUNK_MIN", "2");
            env.set("WEBPIPE_ROUTING_SOFT_JUNK_FRAC", "0.5");
            // Below min_soft => false.
            assert!(!WebpipeMcp::compute_search_junk_label(0, 1, 3));
            // Meets min_soft but below frac => false (2/5 < 0.5).
            assert!(!WebpipeMcp::compute_search_junk_label(0, 2, 5));
            // Meets min_soft and frac => true (2/4 >= 0.5).
            assert!(WebpipeMcp::compute_search_junk_label(0, 2, 4));
        }

        #[test]
        fn hard_junk_sets_hard_flag_in_routing_window() {
            let env = EnvGuard::new(&[
                "WEBPIPE_ROUTING_CONTEXT",
                "WEBPIPE_ROUTING_MAX_CONTEXTS",
                "WEBPIPE_ROUTING_WINDOW",
            ]);
            env.set("WEBPIPE_ROUTING_CONTEXT", "query_key");
            env.set("WEBPIPE_ROUTING_MAX_CONTEXTS", "10");
            env.set("WEBPIPE_ROUTING_WINDOW", "10");

            let mcp = WebpipeMcp::new().expect("mcp new");
            mcp.stats_record_search_provider_qk("brave", true, 1, 10, None, Some("qk1"));
            mcp.stats_set_last_search_outcome_junk_level_qk("brave", true, true, Some("qk1"));

            let s = mcp.stats_lock();
            let per = s.search_windows_by_query_key.get("qk1").expect("qk1");
            let w = per.get("brave").expect("brave");
            let sum = w.summary();
            assert_eq!(sum.junk, 1);
            assert_eq!(sum.hard_junk, 1);
        }

        #[test]
        fn url_is_probably_auth_wall_is_reasonable() {
            assert!(url_looks_like_auth_or_challenge(
                "https://medium.com/m/signin?operation=login"
            ));
            assert!(url_looks_like_auth_or_challenge(
                "https://accounts.google.com/o/oauth2/v2/auth"
            ));
            assert!(!url_looks_like_auth_or_challenge(
                "https://example.com/docs/getting-started"
            ));
        }

        #[test]
        fn url_is_probably_promo_or_tracking_is_reasonable() {
            assert!(url_looks_like_promo_or_tracking(
                "https://vercel.com/home?utm_source=next-site&utm_medium=banner&utm_campaign=docs"
            ));
            assert!(url_looks_like_promo_or_tracking(
                "https://example.com/pricing?gclid=abc"
            ));
            assert!(!url_looks_like_promo_or_tracking(
                "https://nextjs.org/docs/app/getting-started/route-handlers"
            ));
        }

        #[test]
        fn filter_low_signal_chunks_drops_nextjs_bundle_gunk() {
            let chunks = vec![
                webpipe_local::extract::ScoredChunk {
                    start_char: 0,
                    end_char: 10,
                    score: 9,
                    text: "(self.__next_s=self.__next_s||[]).push([0,{\"suppressHydrationWarning\":true,\"children\":\"(function(a,b,c,d){var e;let f\"".to_string(),
                },
                webpipe_local::extract::ScoredChunk {
                    start_char: 11,
                    end_char: 42,
                    score: 3,
                    text: "Tool results may contain structured content.".to_string(),
                },
            ];
            let (out, filtered) = WebpipeMcp::filter_low_signal_chunks(chunks);
            assert!(filtered);
            assert_eq!(out.len(), 1);
            assert!(out[0].text.contains("structured content"));
        }

        #[test]
        fn filter_low_signal_chunks_fail_open_is_still_marked_filtered() {
            // If all chunks look like bundle/app-shell gunk, we must fail-open (return something),
            // but we should still return `filtered=true` so callers can react.
            let chunks = vec![
                webpipe_local::extract::ScoredChunk {
                    start_char: 0,
                    end_char: 50,
                    score: 9,
                    text: "webpackChunk.push([0,{\"suppressHydrationWarning\":true}]);".to_string(),
                },
                webpipe_local::extract::ScoredChunk {
                    start_char: 51,
                    end_char: 120,
                    score: 7,
                    text: "window.__NUXT__={};(function(a,b,c,d){var e,f,g;})();".to_string(),
                },
            ];
            let (out, filtered) = WebpipeMcp::filter_low_signal_chunks(chunks.clone());
            assert!(filtered, "fail-open should still be marked filtered");
            assert_eq!(
                out.len(),
                chunks.len(),
                "fail-open should keep original chunks"
            );
        }

        #[test]
        fn truncate_to_chars_is_utf8_safe_and_consistent() {
            let s = "aé🙂中";
            let (out, n, clipped) = WebpipeMcp::truncate_to_chars(s, 3);
            assert_eq!(n, out.chars().count());
            assert_eq!(n, 3);
            assert_eq!(out, "aé🙂");
            assert!(clipped);

            let (out2, n2, clipped2) = WebpipeMcp::truncate_to_chars(s, 10);
            assert_eq!(n2, out2.chars().count());
            assert_eq!(out2, s);
            assert!(!clipped2);
        }

        #[test]
        fn query_key_folds_common_greek_letters() {
            // Keep this deterministic and ASCII-only: query_key is used for stable keys.
            assert_eq!(
                WebpipeMcp::query_key("β-Gaussian"),
                Some("beta gaussian".to_string())
            );
            assert_eq!(
                WebpipeMcp::query_key("Ω(θ)"),
                Some("omega theta".to_string())
            );
        }

        proptest! {
            #[test]
            fn query_key_never_panics_for_arbitrary_unicode(s in any::<String>()) {
                let _ = WebpipeMcp::query_key(&s);
            }

            #[test]
            fn query_key_scrub_output_is_ascii_normalized(s in any::<String>()) {
                if let Some(k) = WebpipeMcp::query_key(&s) {
                    // scrub() is documented as ASCII-lowercase + whitespace-normalized.
                    prop_assert!(k.is_ascii());
                    prop_assert!(!k.starts_with(' '));
                    prop_assert!(!k.ends_with(' '));
                    prop_assert!(!k.contains("  "));
                    for ch in k.chars() {
                        prop_assert!(ch.is_ascii_lowercase() || ch.is_ascii_digit() || ch == ' ');
                    }
                }
            }

            #[test]
            fn pareto_selection_is_bounded_and_deterministic(
                urls in prop::collection::vec(any::<String>(), 0..30),
                scores in prop::collection::vec(any::<u64>(), 0..30),
                texts in prop::collection::vec(any::<String>(), 0..30),
                warnings in prop::collection::vec(0usize..6, 0..30),
                cache_hits in prop::collection::vec(any::<bool>(), 0..30),
                top_k in 1usize..10,
            ) {
                let n = urls.len().min(scores.len()).min(texts.len()).min(warnings.len()).min(cache_hits.len());
                let mut cands = Vec::new();
                for i in 0..n {
                    let start = i;
                    let end = i.saturating_add(1);
                    cands.push(ChunkCandidate{
                        url: urls[i].clone(),
                        score: scores[i],
                        start_char: start,
                        end_char: end,
                        text: texts[i].clone(),
                        warning_penalty: warnings[i] as i64,
                        cache_hit: cache_hits[i],
                    });
                }

                let a = WebpipeMcp::select_top_chunks(cands.clone(), top_k, "pareto");
                let b = WebpipeMcp::select_top_chunks(cands.clone(), top_k, "pareto");
                prop_assert!(a.len() <= top_k);
                prop_assert_eq!(a.len(), b.len());

                let sig = |c: &ChunkCandidate| (c.url.clone(), c.start_char, c.end_char, WebpipeMcp::score_key(c.score));
                let sa: Vec<_> = a.iter().map(sig).collect();
                let sb: Vec<_> = b.iter().map(sig).collect();
                prop_assert_eq!(sa, sb);
            }

            #[test]
            fn score_selection_is_sorted_and_deterministic(
                urls in prop::collection::vec(any::<String>(), 0..30),
                scores in prop::collection::vec(any::<u64>(), 0..30),
                texts in prop::collection::vec(any::<String>(), 0..30),
                warnings in prop::collection::vec(0usize..6, 0..30),
                cache_hits in prop::collection::vec(any::<bool>(), 0..30),
                top_k in 1usize..10,
            ) {
                let n = urls.len().min(scores.len()).min(texts.len()).min(warnings.len()).min(cache_hits.len());
                let mut cands = Vec::new();
                for i in 0..n {
                    let start = i;
                    let end = i.saturating_add(1);
                    cands.push(ChunkCandidate{
                        url: urls[i].clone(),
                        score: scores[i],
                        start_char: start,
                        end_char: end,
                        text: texts[i].clone(),
                        warning_penalty: warnings[i] as i64,
                        cache_hit: cache_hits[i],
                    });
                }

                let a = WebpipeMcp::select_top_chunks(cands.clone(), top_k, "score");
                let b = WebpipeMcp::select_top_chunks(cands.clone(), top_k, "score");

                prop_assert!(a.len() <= top_k);
                prop_assert_eq!(a.len(), b.len());

                // Deterministic signature.
                let sig = |c: &ChunkCandidate| (c.url.clone(), c.start_char, c.end_char, WebpipeMcp::score_key(c.score));
                let sa: Vec<_> = a.iter().map(sig).collect();
                let sb: Vec<_> = b.iter().map(sig).collect();
                prop_assert_eq!(sa, sb);

                // Sorted by (score desc, warning_penalty asc, url asc, start_char asc) per implementation.
                for w in a.windows(2) {
                    let x = &w[0];
                    let y = &w[1];
                    let kx = (
                        -(WebpipeMcp::score_key(x.score)),
                        x.warning_penalty,
                        x.url.clone(),
                        x.start_char,
                    );
                    let ky = (
                        -(WebpipeMcp::score_key(y.score)),
                        y.warning_penalty,
                        y.url.clone(),
                        y.start_char,
                    );
                    prop_assert!(kx <= ky);
                }
            }

            #[test]
            fn error_obj_has_stable_shape(msg in any::<String>(), hint in any::<String>()) {
                let codes = [
                    ErrorCode::InvalidParams,
                    ErrorCode::InvalidUrl,
                    ErrorCode::NotConfigured,
                    ErrorCode::NotSupported,
                    ErrorCode::ProviderUnavailable,
                    ErrorCode::FetchFailed,
                    ErrorCode::SearchFailed,
                    ErrorCode::CacheError,
                    ErrorCode::UnexpectedError,
                ];

                for code in codes {
                    let v = error_obj(code, &msg, &hint);
                    prop_assert!(v.is_object());
                    prop_assert_eq!(v.get("code").and_then(|x| x.as_str()), Some(code.as_str()));
                    prop_assert!(v.get("message").and_then(|x| x.as_str()).is_some());
                    prop_assert!(v.get("hint").and_then(|x| x.as_str()).is_some());
                    prop_assert!(v.get("retryable").and_then(|x| x.as_bool()).is_some());
                }
            }
        }

        #[tokio::test]
        async fn web_search_auto_not_configured_has_stable_shape_and_selection() {
            let _env = EnvGuard::new(&SEARCH_ENV_KEYS);

            let svc = WebpipeMcp::new().expect("new");
            let r = svc
                .web_search(p(WebSearchArgs {
                    query: Some("q".to_string()),
                    provider: Some("auto".to_string()),
                    auto_mode: Some("fallback".to_string()),
                    max_results: Some(3),
                    language: None,
                    country: None,
                    ..Default::default()
                }))
                .await
                .expect("call");
            let v = payload_from_call_tool_result(&r);
            assert_eq!(v["schema_version"].as_u64(), Some(2));
            assert_eq!(v["kind"].as_str(), Some("web_search"));
            assert_eq!(v["ok"].as_bool(), Some(false));
            assert_eq!(v["provider"].as_str(), Some("auto"));
            assert_eq!(v["request"]["provider"].as_str(), Some("auto"));
            assert_eq!(v["query"].as_str(), Some("q"));
            assert_eq!(v["max_results"].as_u64(), Some(3));
            assert_eq!(
                v["error"]["code"].as_str(),
                Some(ErrorCode::NotConfigured.as_str())
            );
            assert_eq!(v["selection"]["requested_provider"].as_str(), Some("auto"));
            assert_eq!(v["selection"]["selected_provider"].as_str(), Some("none"));
        }

        #[tokio::test]
        async fn web_search_explicit_provider_not_configured_echoes_provider_and_query() {
            let _env = EnvGuard::new(&SEARCH_ENV_KEYS);

            let svc = WebpipeMcp::new().expect("new");

            // brave (not configured)
            let r1 = svc
                .web_search(p(WebSearchArgs {
                    query: Some("q1".to_string()),
                    provider: Some("brave".to_string()),
                    auto_mode: None,
                    max_results: Some(1),
                    language: None,
                    country: None,
                    ..Default::default()
                }))
                .await
                .expect("call brave");
            let v1 = payload_from_call_tool_result(&r1);
            assert_eq!(v1["ok"].as_bool(), Some(false));
            assert_eq!(v1["provider"].as_str(), Some("brave"));
            assert_eq!(v1["request"]["provider"].as_str(), Some("brave"));
            assert_eq!(v1["query"].as_str(), Some("q1"));
            assert_eq!(
                v1["error"]["code"].as_str(),
                Some(ErrorCode::NotConfigured.as_str())
            );

            // tavily (not configured)
            let r2 = svc
                .web_search(p(WebSearchArgs {
                    query: Some("q2".to_string()),
                    provider: Some("tavily".to_string()),
                    auto_mode: None,
                    max_results: Some(1),
                    language: None,
                    country: None,
                    ..Default::default()
                }))
                .await
                .expect("call tavily");
            let v2 = payload_from_call_tool_result(&r2);
            assert_eq!(v2["ok"].as_bool(), Some(false));
            assert_eq!(v2["provider"].as_str(), Some("tavily"));
            assert_eq!(v2["request"]["provider"].as_str(), Some("tavily"));
            assert_eq!(v2["query"].as_str(), Some("q2"));
            assert_eq!(
                v2["error"]["code"].as_str(),
                Some(ErrorCode::NotConfigured.as_str())
            );
        }

        #[tokio::test]
        async fn web_deep_research_not_configured_is_stable_and_offline_friendly() {
            // Remove Perplexity key(s), and also remove search keys to prove we can run offline mode.
            let mut keys = Vec::new();
            keys.extend_from_slice(&SEARCH_ENV_KEYS);
            keys.extend_from_slice(&PERPLEXITY_ENV_KEYS);
            let _env = EnvGuard::new(&keys);

            // Local fixture server for deterministic fetch/extract.
            use axum::{routing::get, Router};
            use std::net::SocketAddr;
            let app = Router::new().route(
                "/",
                get(|| async {
                    (
                        [("content-type", "text/html")],
                        "<html><body><h1>Hello</h1><p>world</p></body></html>",
                    )
                }),
            );
            let listener = tokio::net::TcpListener::bind("127.0.0.1:0").await.unwrap();
            let addr: SocketAddr = listener.local_addr().unwrap();
            tokio::spawn(async move {
                axum::serve(listener, app).await.expect("axum serve");
            });
            let url = format!("http://{}/", addr);

            let svc = WebpipeMcp::new().expect("new");
            let r = svc
                .web_deep_research(Parameters(Some(WebDeepResearchArgs {
                    query: "test question".to_string(),
                    audit: None,
                    synthesize: None,
                    urls: Some(vec![url]),
                    provider: Some("auto".to_string()),
                    auto_mode: Some("fallback".to_string()),
                    selection_mode: None,
                    exploration: None,
                    fetch_backend: Some("local".to_string()),
                    no_network: Some(false),
                    max_results: Some(3),
                    max_urls: Some(1),
                    timeout_ms: Some(5_000),
                    max_bytes: Some(200_000),
                    width: Some(80),
                    max_chars: Some(5_000),
                    top_chunks: Some(3),
                    max_chunk_chars: Some(200),
                    include_links: Some(false),
                    max_links: Some(10),
                    arxiv_mode: None,
                    arxiv_max_papers: None,
                    arxiv_categories: None,
                    arxiv_years: None,
                    arxiv_timeout_ms: None,
                    papers_mode: None,
                    papers_backends: None,
                    papers_max_papers: None,
                    papers_years: None,
                    papers_timeout_ms: None,
                    papers_include_abstract: None,
                    model: Some("sonar-deep-research".to_string()),
                    llm_model: None,
                    search_mode: None,
                    reasoning_effort: Some("medium".to_string()),
                    max_tokens: Some(200),
                    temperature: Some(0.0),
                    top_p: None,
                    max_answer_chars: Some(2_000),
                    include_evidence: Some(true),
                    now_epoch_s: Some(1700000000),
                    llm_backend: None,
                })))
                .await
                .expect("call");

            let v = payload_from_call_tool_result(&r);
            assert_eq!(v["schema_version"].as_u64(), Some(2));
            assert_eq!(v["kind"].as_str(), Some("web_deep_research"));
            assert_eq!(v["ok"].as_bool(), Some(false));
            assert_eq!(
                v["error"]["code"].as_str(),
                Some(ErrorCode::NotConfigured.as_str())
            );
            // Prove evidence exists and is successful, so failure is clearly "LLM not configured".
            assert_eq!(v["evidence"]["ok"].as_bool(), Some(true));
            assert_eq!(v["evidence"]["kind"].as_str(), Some("web_search_extract"));
        }

        #[tokio::test]
        async fn web_deep_research_ollama_local_succeeds_with_no_network_if_cache_is_warmed() {
            let mut keys = Vec::new();
            keys.extend_from_slice(&SEARCH_ENV_KEYS);
            keys.extend_from_slice(&PERPLEXITY_ENV_KEYS);
            keys.extend_from_slice(&[
                "WEBPIPE_CACHE_DIR",
                "WEBPIPE_OLLAMA_ENABLE",
                "WEBPIPE_OLLAMA_BASE_URL",
                "WEBPIPE_OLLAMA_MODEL",
            ]);
            let env = EnvGuard::new(&keys);

            // Pre-warm cache (strictly offline): write a cached fetch response that matches
            // the FetchRequest knobs used by web_search_extract inside web_deep_research.
            let tmp = tempfile::tempdir().expect("tempdir");
            env.set("WEBPIPE_CACHE_DIR", tmp.path().to_str().unwrap());

            let url = "http://example.invalid/fixture".to_string();
            let html = "<html><body><h1>Hello</h1><p>cached world</p></body></html>";
            let cache = webpipe_local::FsCache::new(tmp.path().to_path_buf());
            let req = FetchRequest {
                url: url.clone(),
                timeout_ms: Some(2_000),
                max_bytes: Some(200_000),
                headers: BTreeMap::new(),
                cache: FetchCachePolicy {
                    read: true,
                    write: true,
                    ttl_s: Some(60),
                },
            };
            cache
                .put(
                    &req,
                    &webpipe_core::FetchResponse {
                        url: url.clone(),
                        final_url: url.clone(),
                        status: 200,
                        content_type: Some("text/html".to_string()),
                        headers: BTreeMap::new(),
                        bytes: html.as_bytes().to_vec(),
                        truncated: false,
                        source: FetchSource::Network,
                        timings_ms: BTreeMap::new(),
                    },
                )
                .expect("cache put");

            // Local Ollama stub server (localhost-only) for deterministic synthesis.
            use axum::{routing::post, Json, Router};
            use std::net::SocketAddr;
            let app = Router::new().route(
                "/api/chat",
                post(|_body: Json<serde_json::Value>| async move {
                    Json(serde_json::json!({
                        "message": { "role": "assistant", "content": "Local synthesis ok." }
                    }))
                }),
            );
            let listener = tokio::net::TcpListener::bind("127.0.0.1:0").await.unwrap();
            let addr: SocketAddr = listener.local_addr().unwrap();
            tokio::spawn(async move {
                axum::serve(listener, app).await.expect("axum serve");
            });
            env.set("WEBPIPE_OLLAMA_ENABLE", "true");
            env.set("WEBPIPE_OLLAMA_BASE_URL", &format!("http://{addr}"));
            env.set("WEBPIPE_OLLAMA_MODEL", "test-model");

            let svc = WebpipeMcp::new().expect("new");
            let r = svc
                .web_deep_research(Parameters(Some(WebDeepResearchArgs {
                    query: "test question".to_string(),
                    audit: None,
                    synthesize: None,
                    urls: Some(vec![url]),
                    provider: Some("auto".to_string()),
                    auto_mode: Some("fallback".to_string()),
                    selection_mode: None,
                    exploration: None,
                    fetch_backend: Some("local".to_string()),
                    no_network: Some(true),
                    max_results: Some(1),
                    max_urls: Some(1),
                    timeout_ms: Some(5_000),
                    max_bytes: Some(200_000),
                    width: Some(80),
                    max_chars: Some(5_000),
                    top_chunks: Some(2),
                    max_chunk_chars: Some(200),
                    include_links: Some(false),
                    max_links: Some(10),
                    arxiv_mode: None,
                    arxiv_max_papers: None,
                    arxiv_categories: None,
                    arxiv_years: None,
                    arxiv_timeout_ms: None,
                    papers_mode: None,
                    papers_backends: None,
                    papers_max_papers: None,
                    papers_years: None,
                    papers_timeout_ms: None,
                    papers_include_abstract: None,
                    model: Some("sonar-deep-research".to_string()),
                    llm_model: None,
                    search_mode: None,
                    reasoning_effort: Some("medium".to_string()),
                    max_tokens: Some(200),
                    temperature: Some(0.0),
                    top_p: None,
                    max_answer_chars: Some(2_000),
                    include_evidence: Some(true),
                    now_epoch_s: Some(1700000000),
                    llm_backend: Some("ollama".to_string()),
                })))
                .await
                .expect("call");

            let v = payload_from_call_tool_result(&r);
            assert_eq!(v["schema_version"].as_u64(), Some(2));
            assert_eq!(v["kind"].as_str(), Some("web_deep_research"));
            assert_eq!(v["ok"].as_bool(), Some(true));
            assert_eq!(v["provider"].as_str(), Some("ollama"));
            assert_eq!(v["request"]["no_network"].as_bool(), Some(true));
            assert_eq!(v["evidence"]["ok"].as_bool(), Some(true));
            assert_eq!(v["evidence"]["kind"].as_str(), Some("web_search_extract"));
            assert!(v["warnings"]
                .as_array()
                .unwrap_or(&vec![])
                .iter()
                .any(|x| x.as_str() == Some("llm_ollama_used")));
            assert!(v["answer"]["text"]
                .as_str()
                .unwrap_or("")
                .contains("Local synthesis ok"));
        }

        #[tokio::test]
        async fn web_deep_research_openai_compat_local_succeeds_with_no_network_if_cache_is_warmed()
        {
            let mut keys = Vec::new();
            keys.extend_from_slice(&SEARCH_ENV_KEYS);
            keys.extend_from_slice(&PERPLEXITY_ENV_KEYS);
            keys.extend_from_slice(&[
                "WEBPIPE_CACHE_DIR",
                "WEBPIPE_OPENAI_COMPAT_BASE_URL",
                "WEBPIPE_OPENAI_COMPAT_API_KEY",
                "WEBPIPE_OPENAI_COMPAT_MODEL",
            ]);
            let env = EnvGuard::new(&keys);

            let tmp = tempfile::tempdir().expect("tempdir");
            env.set("WEBPIPE_CACHE_DIR", tmp.path().to_str().unwrap());

            let url = "http://example.invalid/fixture".to_string();
            let html = "<html><body><h1>Hello</h1><p>cached world</p></body></html>";
            let cache = webpipe_local::FsCache::new(tmp.path().to_path_buf());
            let req = FetchRequest {
                url: url.clone(),
                timeout_ms: Some(2_000),
                max_bytes: Some(200_000),
                headers: BTreeMap::new(),
                cache: FetchCachePolicy {
                    read: true,
                    write: true,
                    ttl_s: Some(60),
                },
            };
            cache
                .put(
                    &req,
                    &webpipe_core::FetchResponse {
                        url: url.clone(),
                        final_url: url.clone(),
                        status: 200,
                        content_type: Some("text/html".to_string()),
                        headers: BTreeMap::new(),
                        bytes: html.as_bytes().to_vec(),
                        truncated: false,
                        source: FetchSource::Network,
                        timings_ms: BTreeMap::new(),
                    },
                )
                .expect("cache put");

            // OpenAI-compatible local stub.
            use axum::{routing::post, Json, Router};
            use std::net::SocketAddr;
            let app = Router::new().route(
                "/v1/chat/completions",
                post(|_body: Json<serde_json::Value>| async move {
                    Json(serde_json::json!({
                        "choices": [
                            { "message": { "role": "assistant", "content": "OpenAI-compat synthesis ok." } }
                        ]
                    }))
                }),
            );
            let listener = tokio::net::TcpListener::bind("127.0.0.1:0").await.unwrap();
            let addr: SocketAddr = listener.local_addr().unwrap();
            tokio::spawn(async move {
                axum::serve(listener, app).await.expect("axum serve");
            });
            env.set("WEBPIPE_OPENAI_COMPAT_BASE_URL", &format!("http://{addr}"));
            env.set("WEBPIPE_OPENAI_COMPAT_MODEL", "local/test");

            let svc = WebpipeMcp::new().expect("new");
            let r = svc
                .web_deep_research(Parameters(Some(WebDeepResearchArgs {
                    query: "test question".to_string(),
                    audit: None,
                    synthesize: None,
                    urls: Some(vec![url]),
                    provider: Some("auto".to_string()),
                    auto_mode: Some("fallback".to_string()),
                    selection_mode: None,
                    exploration: None,
                    fetch_backend: Some("local".to_string()),
                    no_network: Some(true),
                    max_results: Some(1),
                    max_urls: Some(1),
                    timeout_ms: Some(5_000),
                    max_bytes: Some(200_000),
                    width: Some(80),
                    max_chars: Some(5_000),
                    top_chunks: Some(2),
                    max_chunk_chars: Some(200),
                    include_links: Some(false),
                    max_links: Some(10),
                    arxiv_mode: None,
                    arxiv_max_papers: None,
                    arxiv_categories: None,
                    arxiv_years: None,
                    arxiv_timeout_ms: None,
                    papers_mode: None,
                    papers_backends: None,
                    papers_max_papers: None,
                    papers_years: None,
                    papers_timeout_ms: None,
                    papers_include_abstract: None,
                    model: Some("sonar-deep-research".to_string()),
                    llm_model: Some("local/test".to_string()),
                    search_mode: None,
                    reasoning_effort: Some("medium".to_string()),
                    max_tokens: Some(200),
                    temperature: Some(0.0),
                    top_p: None,
                    max_answer_chars: Some(2_000),
                    include_evidence: Some(true),
                    now_epoch_s: Some(1700000000),
                    llm_backend: Some("openai_compat".to_string()),
                })))
                .await
                .expect("call");

            let v = payload_from_call_tool_result(&r);
            assert_eq!(v["ok"].as_bool(), Some(true));
            assert_eq!(v["provider"].as_str(), Some("openai_compat"));
            assert!(v.get("evidence_pack").is_some());
            assert!(v["answer"]["text"]
                .as_str()
                .unwrap_or("")
                .contains("OpenAI-compat synthesis ok"));
        }

        #[tokio::test]
        async fn web_deep_research_openai_compat_rejects_non_localhost_in_no_network_mode() {
            let mut keys = Vec::new();
            keys.extend_from_slice(&SEARCH_ENV_KEYS);
            keys.extend_from_slice(&PERPLEXITY_ENV_KEYS);
            keys.extend_from_slice(&[
                "WEBPIPE_OPENAI_COMPAT_BASE_URL",
                "WEBPIPE_OPENAI_COMPAT_MODEL",
            ]);
            let env = EnvGuard::new(&keys);
            env.set("WEBPIPE_OPENAI_COMPAT_BASE_URL", "https://example.com");
            env.set("WEBPIPE_OPENAI_COMPAT_MODEL", "x");

            let svc = WebpipeMcp::new().expect("new");
            let r = svc
                .web_deep_research(Parameters(Some(WebDeepResearchArgs {
                    query: "q".to_string(),
                    audit: None,
                    synthesize: None,
                    urls: Some(vec!["http://example.invalid/fixture".to_string()]),
                    provider: Some("auto".to_string()),
                    auto_mode: Some("fallback".to_string()),
                    selection_mode: None,
                    exploration: None,
                    fetch_backend: Some("local".to_string()),
                    no_network: Some(true),
                    max_results: Some(1),
                    max_urls: Some(1),
                    timeout_ms: Some(2_000),
                    max_bytes: Some(200_000),
                    width: Some(80),
                    max_chars: Some(2_000),
                    top_chunks: Some(1),
                    max_chunk_chars: Some(200),
                    include_links: Some(false),
                    max_links: Some(10),
                    arxiv_mode: None,
                    arxiv_max_papers: None,
                    arxiv_categories: None,
                    arxiv_years: None,
                    arxiv_timeout_ms: None,
                    papers_mode: None,
                    papers_backends: None,
                    papers_max_papers: None,
                    papers_years: None,
                    papers_timeout_ms: None,
                    papers_include_abstract: None,
                    model: Some("sonar-deep-research".to_string()),
                    llm_model: Some("x".to_string()),
                    search_mode: None,
                    reasoning_effort: None,
                    max_tokens: None,
                    temperature: None,
                    top_p: None,
                    max_answer_chars: Some(500),
                    include_evidence: Some(false),
                    now_epoch_s: Some(1700000000),
                    llm_backend: Some("openai_compat".to_string()),
                })))
                .await
                .expect("call");

            let v = payload_from_call_tool_result(&r);
            assert_eq!(v["ok"].as_bool(), Some(false));
            assert_eq!(
                v["error"]["code"].as_str(),
                Some(ErrorCode::NotSupported.as_str())
            );
        }

        #[tokio::test]
        async fn web_deep_research_audit_rejects_search_mode() {
            let mut keys = Vec::new();
            keys.extend_from_slice(&SEARCH_ENV_KEYS);
            keys.extend_from_slice(&PERPLEXITY_ENV_KEYS);
            let _env = EnvGuard::new(&keys);

            let svc = WebpipeMcp::new().expect("new");
            let r = svc
                .web_deep_research(Parameters(Some(WebDeepResearchArgs {
                    query: "q".to_string(),
                    audit: Some(true),
                    synthesize: Some(false),
                    urls: Some(vec!["http://example.invalid/fixture".to_string()]),
                    provider: Some("auto".to_string()),
                    auto_mode: Some("fallback".to_string()),
                    selection_mode: Some("score".to_string()),
                    exploration: Some("balanced".to_string()),
                    fetch_backend: Some("local".to_string()),
                    no_network: Some(true),
                    max_results: Some(1),
                    max_urls: Some(1),
                    timeout_ms: Some(2_000),
                    max_bytes: Some(200_000),
                    width: Some(80),
                    max_chars: Some(2_000),
                    top_chunks: Some(1),
                    max_chunk_chars: Some(200),
                    include_links: Some(false),
                    max_links: Some(10),
                    arxiv_mode: Some("off".to_string()),
                    arxiv_max_papers: None,
                    arxiv_categories: None,
                    arxiv_years: None,
                    arxiv_timeout_ms: None,
                    papers_mode: None,
                    papers_backends: None,
                    papers_max_papers: None,
                    papers_years: None,
                    papers_timeout_ms: None,
                    papers_include_abstract: None,
                    model: Some("sonar-deep-research".to_string()),
                    llm_model: None,
                    search_mode: Some("on".to_string()),
                    reasoning_effort: None,
                    max_tokens: None,
                    temperature: None,
                    top_p: None,
                    max_answer_chars: Some(500),
                    include_evidence: Some(true),
                    now_epoch_s: Some(1700000000),
                    llm_backend: Some("auto".to_string()),
                })))
                .await
                .expect("call");
            let v = payload_from_call_tool_result(&r);
            assert_eq!(v["ok"].as_bool(), Some(false));
            assert_eq!(
                v["error"]["code"].as_str(),
                Some(ErrorCode::NotSupported.as_str())
            );
        }

        #[tokio::test]
        async fn web_deep_research_audit_requires_include_evidence() {
            let mut keys = Vec::new();
            keys.extend_from_slice(&SEARCH_ENV_KEYS);
            keys.extend_from_slice(&PERPLEXITY_ENV_KEYS);
            let _env = EnvGuard::new(&keys);

            let svc = WebpipeMcp::new().expect("new");
            let r = svc
                .web_deep_research(Parameters(Some(WebDeepResearchArgs {
                    query: "q".to_string(),
                    audit: Some(true),
                    synthesize: Some(false),
                    urls: Some(vec!["http://example.invalid/fixture".to_string()]),
                    provider: Some("auto".to_string()),
                    auto_mode: Some("fallback".to_string()),
                    selection_mode: Some("score".to_string()),
                    exploration: Some("balanced".to_string()),
                    fetch_backend: Some("local".to_string()),
                    no_network: Some(true),
                    max_results: Some(1),
                    max_urls: Some(1),
                    timeout_ms: Some(2_000),
                    max_bytes: Some(200_000),
                    width: Some(80),
                    max_chars: Some(2_000),
                    top_chunks: Some(1),
                    max_chunk_chars: Some(200),
                    include_links: Some(false),
                    max_links: Some(10),
                    arxiv_mode: Some("off".to_string()),
                    arxiv_max_papers: None,
                    arxiv_categories: None,
                    arxiv_years: None,
                    arxiv_timeout_ms: None,
                    papers_mode: None,
                    papers_backends: None,
                    papers_max_papers: None,
                    papers_years: None,
                    papers_timeout_ms: None,
                    papers_include_abstract: None,
                    model: Some("sonar-deep-research".to_string()),
                    llm_model: None,
                    search_mode: None,
                    reasoning_effort: None,
                    max_tokens: None,
                    temperature: None,
                    top_p: None,
                    max_answer_chars: Some(500),
                    include_evidence: Some(false),
                    now_epoch_s: Some(1700000000),
                    llm_backend: Some("auto".to_string()),
                })))
                .await
                .expect("call");
            let v = payload_from_call_tool_result(&r);
            assert_eq!(v["ok"].as_bool(), Some(false));
            assert_eq!(
                v["error"]["code"].as_str(),
                Some(ErrorCode::InvalidParams.as_str())
            );
        }

        #[tokio::test]
        async fn web_deep_research_audit_fails_closed_when_perplexity_rejects_off_mode() {
            let mut keys = Vec::new();
            keys.extend_from_slice(&SEARCH_ENV_KEYS);
            keys.extend_from_slice(&PERPLEXITY_ENV_KEYS);
            let env = EnvGuard::new(&keys);

            // Local HTML evidence server (so evidence gathering succeeds).
            use axum::{routing::get, Router};
            use std::net::SocketAddr;
            let html_app = Router::new().route(
                "/",
                get(|| async {
                    (
                        [(axum::http::header::CONTENT_TYPE, "text/html")],
                        "<html><body><h1>Evidence</h1><p>Some text.</p></body></html>",
                    )
                }),
            );
            let html_listener = tokio::net::TcpListener::bind("127.0.0.1:0").await.unwrap();
            let html_addr: SocketAddr = html_listener.local_addr().unwrap();
            tokio::spawn(async move {
                axum::serve(html_listener, html_app)
                    .await
                    .expect("axum serve");
            });
            let url = format!("http://{}/", html_addr);

            // Local Perplexity stub:
            // - if search_mode == "off": return HTTP 400 (forcing the "off rejected" path)
            // - otherwise: return a minimal successful shape
            use axum::{routing::post, Json};
            let pplx_app = Router::new().route(
                "/chat/completions",
                post(|body: Json<serde_json::Value>| async move {
                    let sm = body
                        .get("search_mode")
                        .and_then(|v| v.as_str())
                        .unwrap_or("");
                    if sm == "off" {
                        return (
                            axum::http::StatusCode::BAD_REQUEST,
                            "search_mode off not supported",
                        )
                            .into_response();
                    }
                    Json(serde_json::json!({
                        "choices": [
                            { "message": { "role": "assistant", "content": "ok" } }
                        ],
                        "citations": []
                    }))
                    .into_response()
                }),
            );
            use axum::response::IntoResponse;
            let pplx_listener = tokio::net::TcpListener::bind("127.0.0.1:0").await.unwrap();
            let pplx_addr: SocketAddr = pplx_listener.local_addr().unwrap();
            tokio::spawn(async move {
                axum::serve(pplx_listener, pplx_app)
                    .await
                    .expect("axum serve");
            });

            env.set("WEBPIPE_PERPLEXITY_API_KEY", "dummy");
            env.set(
                "WEBPIPE_PERPLEXITY_ENDPOINT",
                &format!("http://{pplx_addr}/chat/completions"),
            );

            let svc = WebpipeMcp::new().expect("new");
            let r = svc
                .web_deep_research(Parameters(Some(WebDeepResearchArgs {
                    query: "q".to_string(),
                    audit: Some(true),
                    synthesize: Some(true),
                    urls: Some(vec![url]),
                    provider: Some("auto".to_string()),
                    auto_mode: Some("fallback".to_string()),
                    selection_mode: Some("score".to_string()),
                    exploration: Some("balanced".to_string()),
                    fetch_backend: Some("local".to_string()),
                    no_network: Some(false),
                    max_results: Some(1),
                    max_urls: Some(1),
                    timeout_ms: Some(2_000),
                    max_bytes: Some(200_000),
                    width: Some(80),
                    max_chars: Some(2_000),
                    top_chunks: Some(1),
                    max_chunk_chars: Some(200),
                    include_links: Some(false),
                    max_links: Some(10),
                    arxiv_mode: Some("off".to_string()),
                    arxiv_max_papers: None,
                    arxiv_categories: None,
                    arxiv_years: None,
                    arxiv_timeout_ms: None,
                    papers_mode: None,
                    papers_backends: None,
                    papers_max_papers: None,
                    papers_years: None,
                    papers_timeout_ms: None,
                    papers_include_abstract: None,
                    model: Some("sonar-deep-research".to_string()),
                    llm_model: None,
                    search_mode: None,
                    reasoning_effort: None,
                    max_tokens: None,
                    temperature: None,
                    top_p: None,
                    max_answer_chars: Some(500),
                    include_evidence: Some(true),
                    now_epoch_s: Some(1700000000),
                    llm_backend: Some("perplexity".to_string()),
                })))
                .await
                .expect("call");

            let v = payload_from_call_tool_result(&r);
            assert_eq!(v["ok"].as_bool(), Some(false));
            assert_eq!(
                v["error"]["code"].as_str(),
                Some(ErrorCode::NotSupported.as_str())
            );
            assert!(v.get("evidence_pack").is_some());
        }

        #[tokio::test]
        async fn web_search_auto_fallback_uses_tavily_when_brave_is_rate_limited() {
            let mut keys = Vec::new();
            keys.extend_from_slice(&SEARCH_ENV_KEYS);
            keys.extend_from_slice(&["WEBPIPE_BRAVE_ENDPOINT", "WEBPIPE_TAVILY_ENDPOINT"]);
            let env = EnvGuard::new(&keys);

            // Local stub search endpoints.
            use axum::{routing::get, routing::post, Json, Router};
            use std::net::SocketAddr;
            let app = Router::new()
                // Brave returns 429.
                .route(
                    "/brave",
                    get(|| async {
                        (
                            axum::http::StatusCode::TOO_MANY_REQUESTS,
                            "Too Many Requests",
                        )
                    }),
                )
                // Tavily returns a minimal success shape.
                .route(
                    "/tavily",
                    post(|_body: Json<serde_json::Value>| async move {
                        Json(serde_json::json!({
                            "results": [
                                {"url":"https://example.com","title":"Example","content":"Hello"}
                            ],
                            "usage": { "credits": 1 }
                        }))
                    }),
                );
            let listener = tokio::net::TcpListener::bind("127.0.0.1:0").await.unwrap();
            let addr: SocketAddr = listener.local_addr().unwrap();
            tokio::spawn(async move {
                axum::serve(listener, app).await.expect("axum serve");
            });

            env.set("WEBPIPE_BRAVE_API_KEY", "dummy");
            env.set("WEBPIPE_TAVILY_API_KEY", "dummy");
            env.set("WEBPIPE_BRAVE_ENDPOINT", &format!("http://{addr}/brave"));
            env.set("WEBPIPE_TAVILY_ENDPOINT", &format!("http://{addr}/tavily"));

            let svc = WebpipeMcp::new().expect("new");
            let r = svc
                .web_search(p(WebSearchArgs {
                    provider: Some("auto".to_string()),
                    auto_mode: Some("fallback".to_string()),
                    query: Some("q".to_string()),
                    country: None,
                    language: None,
                    max_results: Some(1),
                    ..Default::default()
                }))
                .await
                .expect("call");
            let v = payload_from_call_tool_result(&r);
            assert_eq!(v["ok"].as_bool(), Some(true));
            assert_eq!(v["provider"].as_str(), Some("auto"));
            assert_eq!(v["backend_provider"].as_str(), Some("tavily"));
            assert!(v["warnings"]
                .as_array()
                .unwrap_or(&vec![])
                .iter()
                .any(|x| x.as_str() == Some("provider_failover")));
            assert!(v["providers"].is_array());
        }

        #[tokio::test]
        async fn web_search_auto_fallback_can_choose_tavily_first_when_brave_is_unhealthy() {
            let mut keys = Vec::new();
            keys.extend_from_slice(&SEARCH_ENV_KEYS);
            keys.extend_from_slice(&["WEBPIPE_BRAVE_ENDPOINT", "WEBPIPE_TAVILY_ENDPOINT"]);
            keys.extend_from_slice(&["WEBPIPE_ROUTING_CONTEXT", "WEBPIPE_ROUTING_WINDOW"]);
            let env = EnvGuard::new(&keys);
            env.set("WEBPIPE_ROUTING_CONTEXT", "global");
            env.set("WEBPIPE_ROUTING_WINDOW", "50");

            // Local stub search endpoints: both succeed.
            use axum::{routing::get, routing::post, Json, Router};
            use std::net::SocketAddr;
            let app = Router::new()
                .route(
                    "/brave",
                    get(|| async {
                        (
                            [(axum::http::header::CONTENT_TYPE, "application/json")],
                            r#"{"web":{"results":[{"url":"https://example.com","title":"Example","description":"Hello"}]}}"#,
                        )
                    }),
                )
                .route(
                    "/tavily",
                    post(|_body: Json<serde_json::Value>| async move {
                        Json(serde_json::json!({
                            "results": [
                                {"url":"https://example.com","title":"Example","content":"Hello"}
                            ],
                            "usage": { "credits": 1 }
                        }))
                    }),
                );
            let listener = tokio::net::TcpListener::bind("127.0.0.1:0").await.unwrap();
            let addr: SocketAddr = listener.local_addr().unwrap();
            tokio::spawn(async move {
                axum::serve(listener, app).await.expect("axum serve");
            });

            env.set("WEBPIPE_BRAVE_API_KEY", "dummy");
            env.set("WEBPIPE_TAVILY_API_KEY", "dummy");
            env.set("WEBPIPE_BRAVE_ENDPOINT", &format!("http://{addr}/brave"));
            env.set("WEBPIPE_TAVILY_ENDPOINT", &format!("http://{addr}/tavily"));

            let svc = WebpipeMcp::new().expect("new");

            // Seed global stats: Brave is currently rate-limiting; Tavily is healthy.
            for _ in 0..10 {
                svc.stats_record_search_provider_qk("brave", false, 0, 10, Some("HTTP 429"), None);
            }
            for _ in 0..10 {
                svc.stats_record_search_provider_qk("tavily", true, 1, 10, None, None);
            }

            let r = svc
                .web_search(p(WebSearchArgs {
                    provider: Some("auto".to_string()),
                    auto_mode: Some("fallback".to_string()),
                    query: Some("q".to_string()),
                    country: None,
                    language: None,
                    max_results: Some(1),
                    ..Default::default()
                }))
                .await
                .expect("call");
            let v = payload_from_call_tool_result(&r);
            assert_eq!(v["ok"].as_bool(), Some(true));
            assert_eq!(v["provider"].as_str(), Some("auto"));
            assert_eq!(v["backend_provider"].as_str(), Some("tavily"));
            // We should not need to fail over if Tavily is chosen first.
            assert!(!v["warnings"]
                .as_array()
                .unwrap_or(&vec![])
                .iter()
                .any(|x| x.as_str() == Some("provider_failover")));
        }

        #[tokio::test]
        async fn web_deep_research_can_include_arxiv_evidence_without_synthesis() {
            let mut keys = Vec::new();
            keys.extend_from_slice(&SEARCH_ENV_KEYS);
            keys.extend_from_slice(&["WEBPIPE_ARXIV_ENDPOINT"]);
            let env = EnvGuard::new(&keys);

            // Local stub Atom feed.
            use axum::{routing::get, Router};
            use std::net::SocketAddr;
            let atom = r#"
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">
  <opensearch:totalResults>1</opensearch:totalResults>
  <entry>
    <id>http://arxiv.org/abs/0805.3415v1</id>
    <updated>2008-05-22T00:00:00Z</updated>
    <published>2008-05-22T00:00:00Z</published>
    <title>Test Paper</title>
    <summary>This is a test abstract.</summary>
    <author><name>Alice</name></author>
    <category term="cs.CL" />
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" />
    <link rel="related" type="application/pdf" href="https://arxiv.org/pdf/0805.3415.pdf" />
  </entry>
</feed>
"#;
            let atom = atom.to_string();
            let app = Router::new().route(
                "/api/query",
                get(move || {
                    let atom = atom.clone();
                    async move {
                        (
                            [(axum::http::header::CONTENT_TYPE, "application/atom+xml")],
                            atom,
                        )
                    }
                }),
            );
            let listener = tokio::net::TcpListener::bind("127.0.0.1:0").await.unwrap();
            let addr: SocketAddr = listener.local_addr().unwrap();
            tokio::spawn(async move {
                axum::serve(listener, app).await.expect("axum serve");
            });
            env.set(
                "WEBPIPE_ARXIV_ENDPOINT",
                &format!("http://{addr}/api/query"),
            );

            // Also provide urls to avoid needing web_search_extract network calls.
            let svc = WebpipeMcp::new().expect("new");
            let r = svc
                .web_deep_research(Parameters(Some(WebDeepResearchArgs {
                    query: "paper about something".to_string(),
                    audit: None,
                    synthesize: Some(false),
                    urls: Some(vec!["http://example.invalid/fixture".to_string()]),
                    provider: Some("auto".to_string()),
                    auto_mode: Some("fallback".to_string()),
                    selection_mode: Some("score".to_string()),
                    exploration: Some("balanced".to_string()),
                    fetch_backend: Some("local".to_string()),
                    no_network: Some(true),
                    max_results: Some(1),
                    max_urls: Some(1),
                    timeout_ms: Some(2_000),
                    max_bytes: Some(200_000),
                    width: Some(80),
                    max_chars: Some(2_000),
                    top_chunks: Some(1),
                    max_chunk_chars: Some(200),
                    include_links: Some(false),
                    max_links: Some(10),
                    arxiv_mode: Some("on".to_string()),
                    arxiv_max_papers: Some(1),
                    arxiv_categories: None,
                    arxiv_years: None,
                    arxiv_timeout_ms: Some(2_000),
                    papers_mode: None,
                    papers_backends: None,
                    papers_max_papers: None,
                    papers_years: None,
                    papers_timeout_ms: None,
                    papers_include_abstract: None,
                    model: Some("sonar-deep-research".to_string()),
                    llm_model: None,
                    search_mode: None,
                    reasoning_effort: None,
                    max_tokens: None,
                    temperature: None,
                    top_p: None,
                    max_answer_chars: Some(500),
                    include_evidence: Some(true),
                    now_epoch_s: Some(1700000000),
                    llm_backend: Some("auto".to_string()),
                })))
                .await
                .expect("call");
            let v = payload_from_call_tool_result(&r);
            assert_eq!(v["ok"].as_bool(), Some(true));
            assert_eq!(v["answer"]["skipped"].as_bool(), Some(true));
            let ev = &v["evidence"];
            assert!(ev.get("arxiv").is_some());
            // In no_network mode, ArXiv should be reported as skipped.
            assert_eq!(ev["arxiv"]["ok"].as_bool(), Some(false));
        }

        #[tokio::test]
        async fn web_deep_research_includes_arxiv_papers_when_enabled() {
            let mut keys = Vec::new();
            keys.extend_from_slice(&SEARCH_ENV_KEYS);
            keys.extend_from_slice(&["WEBPIPE_ARXIV_ENDPOINT", "WEBPIPE_CACHE_DIR"]);
            let env = EnvGuard::new(&keys);

            // Local HTML evidence server.
            use axum::{routing::get, Router};
            use std::net::SocketAddr;
            let html_app = Router::new().route(
                "/",
                get(|| async {
                    (
                        [(axum::http::header::CONTENT_TYPE, "text/html")],
                        "<html><body><h1>Evidence</h1><p>Some text.</p></body></html>",
                    )
                }),
            );
            let html_listener = tokio::net::TcpListener::bind("127.0.0.1:0").await.unwrap();
            let html_addr: SocketAddr = html_listener.local_addr().unwrap();
            tokio::spawn(async move {
                axum::serve(html_listener, html_app)
                    .await
                    .expect("axum serve");
            });
            let url = format!("http://{}/", html_addr);

            // Local ArXiv Atom endpoint.
            let atom = r#"
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">
  <opensearch:totalResults>1</opensearch:totalResults>
  <entry>
    <id>http://arxiv.org/abs/0805.3415v1</id>
    <updated>2008-05-22T00:00:00Z</updated>
    <published>2008-05-22T00:00:00Z</published>
    <title>Test Paper</title>
    <summary>This is a test abstract.</summary>
    <author><name>Alice</name></author>
    <category term="cs.CL" />
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" />
    <link rel="related" type="application/pdf" href="https://arxiv.org/pdf/0805.3415.pdf" />
  </entry>
</feed>
"#;
            let atom = atom.to_string();
            let arxiv_app = Router::new().route(
                "/api/query",
                get(move || {
                    let atom = atom.clone();
                    async move {
                        (
                            [(axum::http::header::CONTENT_TYPE, "application/atom+xml")],
                            atom,
                        )
                    }
                }),
            );
            let arxiv_listener = tokio::net::TcpListener::bind("127.0.0.1:0").await.unwrap();
            let arxiv_addr: SocketAddr = arxiv_listener.local_addr().unwrap();
            tokio::spawn(async move {
                axum::serve(arxiv_listener, arxiv_app)
                    .await
                    .expect("axum serve");
            });
            env.set(
                "WEBPIPE_ARXIV_ENDPOINT",
                &format!("http://{arxiv_addr}/api/query"),
            );

            let svc = WebpipeMcp::new().expect("new");
            let r = svc
                .web_deep_research(Parameters(Some(WebDeepResearchArgs {
                    query: "paper about something".to_string(),
                    audit: None,
                    synthesize: Some(false),
                    urls: Some(vec![url]),
                    provider: Some("auto".to_string()),
                    auto_mode: Some("fallback".to_string()),
                    selection_mode: Some("score".to_string()),
                    exploration: Some("balanced".to_string()),
                    fetch_backend: Some("local".to_string()),
                    no_network: Some(false),
                    max_results: Some(1),
                    max_urls: Some(1),
                    timeout_ms: Some(2_000),
                    max_bytes: Some(200_000),
                    width: Some(80),
                    max_chars: Some(2_000),
                    top_chunks: Some(1),
                    max_chunk_chars: Some(200),
                    include_links: Some(false),
                    max_links: Some(10),
                    arxiv_mode: Some("on".to_string()),
                    arxiv_max_papers: Some(1),
                    arxiv_categories: None,
                    arxiv_years: None,
                    arxiv_timeout_ms: Some(2_000),
                    papers_mode: None,
                    papers_backends: None,
                    papers_max_papers: None,
                    papers_years: None,
                    papers_timeout_ms: None,
                    papers_include_abstract: None,
                    model: Some("sonar-deep-research".to_string()),
                    llm_model: None,
                    search_mode: None,
                    reasoning_effort: None,
                    max_tokens: None,
                    temperature: None,
                    top_p: None,
                    max_answer_chars: Some(500),
                    include_evidence: Some(true),
                    now_epoch_s: Some(1700000000),
                    llm_backend: Some("auto".to_string()),
                })))
                .await
                .expect("call");
            let v = payload_from_call_tool_result(&r);
            assert_eq!(v["ok"].as_bool(), Some(true));
            let ev = &v["evidence"];
            assert_eq!(ev["arxiv"]["ok"].as_bool(), Some(true));
            assert!(ev["arxiv"]["papers"]
                .as_array()
                .map(|a| !a.is_empty())
                .unwrap_or(false));
        }

        #[tokio::test]
        async fn web_deep_research_success_includes_evidence_pack_when_include_evidence_true() {
            let mut keys = Vec::new();
            keys.extend_from_slice(&SEARCH_ENV_KEYS);
            keys.extend_from_slice(&PERPLEXITY_ENV_KEYS);
            keys.extend_from_slice(&[
                "WEBPIPE_OLLAMA_ENABLE",
                "WEBPIPE_OLLAMA_BASE_URL",
                "WEBPIPE_OLLAMA_MODEL",
            ]);
            let env = EnvGuard::new(&keys);

            // Local HTML evidence server.
            use axum::{routing::get, Router};
            use std::net::SocketAddr;
            let html_app = Router::new().route(
                "/",
                get(|| async {
                    (
                        [(axum::http::header::CONTENT_TYPE, "text/html")],
                        "<html><body><h1>Evidence</h1><p>Some text.</p></body></html>",
                    )
                }),
            );
            let html_listener = tokio::net::TcpListener::bind("127.0.0.1:0").await.unwrap();
            let html_addr: SocketAddr = html_listener.local_addr().unwrap();
            tokio::spawn(async move {
                axum::serve(html_listener, html_app)
                    .await
                    .expect("axum serve");
            });
            let url = format!("http://{}/", html_addr);

            // Local Ollama stub.
            use axum::{routing::post, Json};
            let ollama_app = Router::new().route(
                "/api/chat",
                post(|_body: Json<serde_json::Value>| async move {
                    Json(serde_json::json!({
                        "message": { "role": "assistant", "content": "Local synthesis ok." }
                    }))
                }),
            );
            let ollama_listener = tokio::net::TcpListener::bind("127.0.0.1:0").await.unwrap();
            let ollama_addr: SocketAddr = ollama_listener.local_addr().unwrap();
            tokio::spawn(async move {
                axum::serve(ollama_listener, ollama_app)
                    .await
                    .expect("axum serve");
            });
            env.set("WEBPIPE_OLLAMA_ENABLE", "true");
            env.set("WEBPIPE_OLLAMA_BASE_URL", &format!("http://{ollama_addr}"));
            env.set("WEBPIPE_OLLAMA_MODEL", "test-model");

            let svc = WebpipeMcp::new().expect("new");
            let r = svc
                .web_deep_research(Parameters(Some(WebDeepResearchArgs {
                    query: "paper about something".to_string(),
                    audit: None,
                    synthesize: Some(true),
                    urls: Some(vec![url]),
                    provider: Some("auto".to_string()),
                    auto_mode: Some("fallback".to_string()),
                    selection_mode: Some("score".to_string()),
                    exploration: Some("balanced".to_string()),
                    fetch_backend: Some("local".to_string()),
                    no_network: Some(false),
                    max_results: Some(1),
                    max_urls: Some(1),
                    timeout_ms: Some(5_000),
                    max_bytes: Some(200_000),
                    width: Some(80),
                    max_chars: Some(2_000),
                    top_chunks: Some(2),
                    max_chunk_chars: Some(200),
                    include_links: Some(false),
                    max_links: Some(10),
                    arxiv_mode: Some("off".to_string()),
                    arxiv_max_papers: None,
                    arxiv_categories: None,
                    arxiv_years: None,
                    arxiv_timeout_ms: None,
                    papers_mode: None,
                    papers_backends: None,
                    papers_max_papers: None,
                    papers_years: None,
                    papers_timeout_ms: None,
                    papers_include_abstract: None,
                    model: Some("sonar-deep-research".to_string()),
                    llm_model: None,
                    search_mode: None,
                    reasoning_effort: None,
                    max_tokens: None,
                    temperature: None,
                    top_p: None,
                    max_answer_chars: Some(500),
                    include_evidence: Some(true),
                    now_epoch_s: Some(1700000000),
                    llm_backend: Some("ollama".to_string()),
                })))
                .await
                .expect("call");
            let v = payload_from_call_tool_result(&r);
            assert_eq!(v["ok"].as_bool(), Some(true));
            assert!(v.get("evidence").is_some());
            assert!(v.get("evidence_pack").is_some());
            assert_eq!(
                v["evidence_pack"]["kind"].as_str(),
                Some("webpipe_evidence_pack")
            );
            // Evidence pack should not contain raw extracted text.
            if let Some(r0) = v["evidence_pack"]["results"]
                .as_array()
                .and_then(|a| a.first())
            {
                assert!(r0.get("text").is_none());
                assert!(r0.get("extract").and_then(|e| e.get("text")).is_none());
            }
        }

        #[tokio::test]
        async fn web_search_extract_rejects_unknown_fetch_backend() {
            let _env = EnvGuard::new(&["WEBPIPE_CACHE_DIR"]);

            // Local fixture server.
            use axum::{routing::get, Router};
            use std::net::SocketAddr;
            let app = Router::new().route(
                "/",
                get(|| async {
                    (
                        [(axum::http::header::CONTENT_TYPE, "text/html")],
                        "<html>hi</html>",
                    )
                }),
            );
            let listener = tokio::net::TcpListener::bind("127.0.0.1:0").await.unwrap();
            let addr: SocketAddr = listener.local_addr().unwrap();
            tokio::spawn(async move {
                axum::serve(listener, app).await.expect("axum serve");
            });
            let url = format!("http://{}/", addr);

            let svc = WebpipeMcp::new().expect("new");
            let r = svc
                .web_search_extract(p(WebSearchExtractArgs {
                    query: Some("q".to_string()),
                    urls: Some(vec![url]),
                    url_selection_mode: None,
                    provider: Some("auto".to_string()),
                    auto_mode: Some("fallback".to_string()),
                    selection_mode: None,
                    fetch_backend: Some("nope".to_string()),
                    no_network: Some(false),
                    firecrawl_fallback_on_empty_extraction: Some(false),
                    firecrawl_fallback_on_low_signal: Some(false),
                    render_fallback_on_empty_extraction: Some(false),
                    render_fallback_on_low_signal: Some(false),
                    max_results: Some(1),
                    max_urls: Some(1),
                    timeout_ms: Some(2_000),
                    max_bytes: Some(200_000),
                    width: Some(80),
                    max_chars: Some(2_000),
                    top_chunks: Some(3),
                    max_chunk_chars: Some(200),
                    include_links: Some(false),
                    include_structure: None,
                    max_outline_items: None,
                    max_blocks: None,
                    max_block_chars: None,
                    semantic_rerank: None,
                    semantic_top_k: None,
                    max_links: Some(10),
                    include_text: Some(false),
                    cache_read: Some(true),
                    cache_write: Some(true),
                    cache_ttl_s: None,
                    exploration: None,
                    agentic: Some(false),
                    agentic_selector: Some("lexical".to_string()),
                    agentic_max_search_rounds: None,
                    agentic_frontier_max: None,
                    planner_max_calls: None,
                    retry_on_truncation: None,
                    truncation_retry_max_bytes: None,
                    compact: None,
                    ..Default::default()
                }))
                .await
                .expect("call");

            let v = payload_from_call_tool_result(&r);
            assert_eq!(v["kind"].as_str(), Some("web_search_extract"));
            assert_eq!(v["ok"].as_bool(), Some(false));
            assert_eq!(
                v["error"]["code"].as_str(),
                Some(ErrorCode::InvalidParams.as_str())
            );
        }

        #[tokio::test]
        async fn web_search_extract_include_links_extracts_links_for_html_main_pages() {
            let _env = EnvGuard::new(&["WEBPIPE_CACHE_DIR"]);

            // Local fixture server: include a nav with many links and an article with a few key links.
            // This shape tends to trigger html_main extraction, but regardless, link extraction should
            // come from raw HTML.
            use axum::{routing::get, Router};
            use std::net::SocketAddr;
            let html = r#"
<html>
  <head><title>Test</title></head>
  <body>
    <nav>
      <a href="/a">A</a><a href="/b">B</a><a href="/c">C</a><a href="/d">D</a>
      <a href="/e">E</a><a href="/f">F</a><a href="/g">G</a><a href="/h">H</a>
    </nav>
    <article>
      <h1>Main content</h1>
      <p>See <a href="https://example.com/x">X</a> and <a href="/y">Y</a>.</p>
    </article>
  </body>
</html>
"#;
            let html = html.to_string();
            let app = Router::new().route(
                "/",
                get(move || {
                    let html = html.clone();
                    async move { ([(axum::http::header::CONTENT_TYPE, "text/html")], html) }
                }),
            );
            let listener = tokio::net::TcpListener::bind("127.0.0.1:0").await.unwrap();
            let addr: SocketAddr = listener.local_addr().unwrap();
            tokio::spawn(async move {
                axum::serve(listener, app).await.expect("axum serve");
            });
            let url = format!("http://{}/", addr);

            let svc = WebpipeMcp::new().expect("new");
            let r = svc
                .web_search_extract(p(WebSearchExtractArgs {
                    query: Some("main content".to_string()),
                    urls: Some(vec![url]),
                    provider: Some("auto".to_string()),
                    auto_mode: Some("fallback".to_string()),
                    selection_mode: Some("score".to_string()),
                    url_selection_mode: Some("auto".to_string()),
                    fetch_backend: Some("local".to_string()),
                    no_network: Some(false),
                    firecrawl_fallback_on_empty_extraction: Some(false),
                    firecrawl_fallback_on_low_signal: Some(false),
                    render_fallback_on_empty_extraction: Some(false),
                    render_fallback_on_low_signal: Some(false),
                    max_results: Some(1),
                    max_urls: Some(1),
                    timeout_ms: Some(2_000),
                    max_bytes: Some(200_000),
                    width: Some(80),
                    max_chars: Some(2_000),
                    top_chunks: Some(2),
                    max_chunk_chars: Some(200),
                    include_links: Some(true),
                    include_structure: Some(false),
                    max_outline_items: None,
                    max_blocks: None,
                    max_block_chars: None,
                    semantic_rerank: None,
                    semantic_top_k: None,
                    max_links: Some(5),
                    include_text: Some(false),
                    // Avoid cross-test cache interference (this test asserts on parsed links).
                    cache_read: Some(false),
                    cache_write: Some(false),
                    cache_ttl_s: None,
                    exploration: None,
                    agentic: Some(false),
                    agentic_selector: Some("lexical".to_string()),
                    agentic_max_search_rounds: None,
                    agentic_frontier_max: None,
                    planner_max_calls: None,
                    retry_on_truncation: None,
                    truncation_retry_max_bytes: None,
                    compact: Some(true),
                    ..Default::default()
                }))
                .await
                .expect("call");

            let v = payload_from_call_tool_result(&r);
            assert_eq!(v["ok"].as_bool(), Some(true));
            let one = v["results"].as_array().unwrap()[0].clone();
            let links = one["extract"]["links"].as_array().unwrap();
            assert!(!links.is_empty());
            // Bounded.
            assert!(links.len() <= 5);
        }

        #[tokio::test]
        async fn web_search_extract_agentic_compact_summary_includes_stuck_and_frontier_stats() {
            let _env = EnvGuard::new(&["WEBPIPE_CACHE_DIR"]);

            // Two pages: one is "stuck" (empty body) and links to the other.
            use axum::{routing::get, Router};
            use std::net::SocketAddr;
            let page_a =
                r#"<html><head><title>Login</title></head><body>Just a moment...</body></html>"#
                    .to_string();
            let page_b = r#"<html><body><article><h1>Install</h1><p>Install instructions.</p></article></body></html>"#.to_string();
            let app = Router::new()
                .route(
                    "/a",
                    get(move || {
                        let page_a = page_a.clone();
                        async move { ([(axum::http::header::CONTENT_TYPE, "text/html")], page_a) }
                    }),
                )
                .route(
                    "/b",
                    get(move || {
                        let page_b = page_b.clone();
                        async move { ([(axum::http::header::CONTENT_TYPE, "text/html")], page_b) }
                    }),
                );
            let listener = tokio::net::TcpListener::bind("127.0.0.1:0").await.unwrap();
            let addr: SocketAddr = listener.local_addr().unwrap();
            tokio::spawn(async move {
                axum::serve(listener, app).await.expect("axum serve");
            });
            let url_a = format!("http://{addr}/a");

            let svc = WebpipeMcp::new().expect("new");
            let r = svc
                .web_search_extract(p(WebSearchExtractArgs {
                    query: Some("install".to_string()),
                    urls: Some(vec![url_a]),
                    provider: Some("auto".to_string()),
                    auto_mode: Some("fallback".to_string()),
                    selection_mode: Some("score".to_string()),
                    url_selection_mode: Some("auto".to_string()),
                    fetch_backend: Some("local".to_string()),
                    no_network: Some(false),
                    firecrawl_fallback_on_empty_extraction: Some(false),
                    firecrawl_fallback_on_low_signal: Some(false),
                    render_fallback_on_empty_extraction: Some(false),
                    render_fallback_on_low_signal: Some(false),
                    max_results: Some(1),
                    max_urls: Some(2),
                    timeout_ms: Some(2_000),
                    max_bytes: Some(200_000),
                    width: Some(80),
                    max_chars: Some(2_000),
                    top_chunks: Some(2),
                    max_chunk_chars: Some(200),
                    include_links: Some(false),
                    include_structure: Some(false),
                    max_outline_items: None,
                    max_blocks: None,
                    max_block_chars: None,
                    semantic_rerank: None,
                    semantic_top_k: None,
                    max_links: Some(10),
                    include_text: Some(false),
                    cache_read: Some(true),
                    cache_write: Some(true),
                    cache_ttl_s: None,
                    exploration: None,
                    agentic: Some(true),
                    agentic_selector: Some("lexical".to_string()),
                    agentic_max_search_rounds: Some(2),
                    agentic_frontier_max: Some(50),
                    planner_max_calls: None,
                    retry_on_truncation: None,
                    truncation_retry_max_bytes: None,
                    compact: Some(true),
                    ..Default::default()
                }))
                .await
                .expect("call");

            let v = payload_from_call_tool_result(&r);
            assert_eq!(v["ok"].as_bool(), Some(true));
            assert_eq!(v["agentic"]["enabled"].as_bool(), Some(true));
            assert!(v["agentic"].get("search_rounds").is_some());
            assert!(v["agentic"].get("stuck_events").is_some());
            assert!(v["agentic"].get("frontier_added_total").is_some());
            assert!(v["agentic"].get("frontier_len_final").is_some());
            assert!(v["agentic"].get("urls_fetched").is_some());
        }

        #[tokio::test]
        async fn web_search_extract_retry_on_truncation_can_recover_tail_content() {
            let _env = EnvGuard::new(&["WEBPIPE_CACHE_DIR"]);

            // Local fixture: a large HTML document with the key sentence near the end.
            use axum::{routing::get, Router};
            use std::net::SocketAddr;
            let prefix = "<html><body><article><h1>Docs</h1><p>Intro.</p>";
            let filler = "x".repeat(20_000);
            let tail = "<p>TAIL_SENTINEL: Route Handlers let you create custom request handlers.</p></article></body></html>";
            let body = format!("{prefix}{filler}{tail}");
            let app = Router::new().route(
                "/",
                get(move || {
                    let body = body.clone();
                    async move { ([(axum::http::header::CONTENT_TYPE, "text/html")], body) }
                }),
            );
            let listener = tokio::net::TcpListener::bind("127.0.0.1:0").await.unwrap();
            let addr: SocketAddr = listener.local_addr().unwrap();
            tokio::spawn(async move {
                axum::serve(listener, app).await.expect("axum serve");
            });
            let url = format!("http://{}/", addr);

            let svc = WebpipeMcp::new().expect("new");
            let r = svc
                .web_search_extract(p(WebSearchExtractArgs {
                    query: Some("route handlers".to_string()),
                    urls: Some(vec![url]),
                    url_selection_mode: Some("auto".to_string()),
                    provider: Some("auto".to_string()),
                    auto_mode: Some("fallback".to_string()),
                    selection_mode: Some("score".to_string()),
                    fetch_backend: Some("local".to_string()),
                    no_network: Some(false),
                    firecrawl_fallback_on_empty_extraction: Some(false),
                    firecrawl_fallback_on_low_signal: Some(false),
                    render_fallback_on_empty_extraction: Some(false),
                    render_fallback_on_low_signal: Some(false),
                    max_results: Some(1),
                    max_urls: Some(1),
                    timeout_ms: Some(2_000),
                    // Force truncation on first attempt.
                    max_bytes: Some(2_000),
                    retry_on_truncation: Some(true),
                    truncation_retry_max_bytes: Some(50_000),
                    width: Some(80),
                    // Keep enough extracted text to include tail content once truncation retry succeeds.
                    max_chars: Some(60_000),
                    top_chunks: Some(3),
                    max_chunk_chars: Some(300),
                    include_links: Some(false),
                    include_structure: Some(false),
                    max_outline_items: None,
                    max_blocks: None,
                    max_block_chars: None,
                    semantic_rerank: None,
                    semantic_top_k: None,
                    max_links: Some(10),
                    include_text: Some(false),
                    cache_read: Some(false),
                    cache_write: Some(false),
                    cache_ttl_s: None,
                    exploration: None,
                    agentic: Some(false),
                    agentic_selector: Some("lexical".to_string()),
                    agentic_max_search_rounds: None,
                    agentic_frontier_max: None,
                    planner_max_calls: None,
                    // This test asserts per-URL diagnostics ("attempts") which are omitted in compact mode.
                    compact: Some(false),
                    ..Default::default()
                }))
                .await
                .expect("call");

            let v = payload_from_call_tool_result(&r);
            assert_eq!(v["ok"].as_bool(), Some(true));
            let one = v["results"].as_array().unwrap()[0].clone();
            assert!(one.get("attempts").is_some());
            assert!(one["attempts"].get("local_retry").is_some());
            let found_tail = v["top_chunks"]
                .as_array()
                .unwrap_or(&vec![])
                .iter()
                .any(|c| {
                    c.get("text")
                        .and_then(|t| t.as_str())
                        .unwrap_or("")
                        .contains("TAIL_SENTINEL")
                });
            assert!(found_tail);
        }

        #[tokio::test]
        async fn web_search_extract_retry_on_truncation_can_retry_beyond_5mb_default_cap() {
            let _env = EnvGuard::new(&["WEBPIPE_CACHE_DIR"]);

            // Regression: urls-mode sequential hydration has its own truncation retry logic.
            // It must be able to retry beyond 5MB when max_bytes=5_000_000 (default), otherwise
            // retry_on_truncation=true is ineffective on large HTML pages.
            use axum::{routing::get, Router};
            use std::net::SocketAddr;

            // Build a body slightly above 5MB.
            let prefix = "<html><body><main><h1>Big</h1><p>Intro</p>";
            let tail = "<p>TAIL_SENTINEL_5MB</p></main></body></html>";
            let target_len = 6_200_000usize;
            let mut filler_len = target_len.saturating_sub(prefix.len() + tail.len());
            // Keep filler non-empty so the response is actually larger than max_bytes.
            filler_len = filler_len.max(1);
            let filler = "x".repeat(filler_len);
            let body = format!("{prefix}{filler}{tail}");
            assert!(body.len() > 5_000_000, "fixture must exceed 5MB");

            let app = Router::new().route(
                "/",
                get(move || {
                    let body = body.clone();
                    async move { ([(axum::http::header::CONTENT_TYPE, "text/html")], body) }
                }),
            );
            let listener = tokio::net::TcpListener::bind("127.0.0.1:0").await.unwrap();
            let addr: SocketAddr = listener.local_addr().unwrap();
            tokio::spawn(async move {
                axum::serve(listener, app).await.expect("axum serve");
            });
            let url = format!("http://{}/", addr);

            let svc = WebpipeMcp::new().expect("new");
            let r = svc
                .web_search_extract(p(WebSearchExtractArgs {
                    query: Some("tail sentinel".to_string()),
                    urls: Some(vec![url]),
                    url_selection_mode: Some("auto".to_string()),
                    provider: Some("auto".to_string()),
                    auto_mode: Some("fallback".to_string()),
                    selection_mode: Some("score".to_string()),
                    fetch_backend: Some("local".to_string()),
                    no_network: Some(false),
                    firecrawl_fallback_on_empty_extraction: Some(false),
                    firecrawl_fallback_on_low_signal: Some(false),
                    render_fallback_on_empty_extraction: Some(false),
                    render_fallback_on_low_signal: Some(false),
                    max_results: Some(1),
                    max_urls: Some(1),
                    timeout_ms: Some(5_000),
                    // Default-ish: 5MB fetch cap.
                    max_bytes: Some(5_000_000),
                    retry_on_truncation: Some(true),
                    // Explicitly allow retry to exceed 5MB.
                    truncation_retry_max_bytes: Some(10_000_000),
                    width: Some(80),
                    // Keep bounded; we only care about attempts and bytes/truncation status.
                    max_chars: Some(8_000),
                    top_chunks: Some(1),
                    max_chunk_chars: Some(400),
                    include_links: Some(false),
                    include_structure: Some(false),
                    max_outline_items: None,
                    max_blocks: None,
                    max_block_chars: None,
                    semantic_rerank: None,
                    semantic_top_k: None,
                    max_links: Some(10),
                    include_text: Some(false),
                    cache_read: Some(false),
                    cache_write: Some(false),
                    cache_ttl_s: None,
                    exploration: None,
                    agentic: Some(false),
                    agentic_selector: Some("lexical".to_string()),
                    agentic_max_search_rounds: None,
                    agentic_frontier_max: None,
                    planner_max_calls: None,
                    // Need per-URL attempts for the assertion.
                    compact: Some(false),
                    ..Default::default()
                }))
                .await
                .expect("call");

            let v = payload_from_call_tool_result(&r);
            assert_eq!(v["ok"].as_bool(), Some(true));
            let one = v["results"].as_array().unwrap()[0].clone();
            assert!(one.get("attempts").is_some());
            assert!(
                one["attempts"].get("local_retry").is_some(),
                "expected local_retry attempt when max_bytes=5MB; attempts={}",
                one["attempts"]
            );
            assert!(
                one["bytes"].as_u64().unwrap_or(0) > 5_000_000,
                "expected bytes to exceed 5MB after retry; got bytes={}",
                one["bytes"]
            );
            assert_eq!(
                one["truncated"].as_bool(),
                Some(false),
                "expected truncated=false after retry; got truncated={}",
                one["truncated"]
            );
        }

        #[tokio::test]
        async fn web_search_extract_urls_mode_without_query_still_returns_top_chunks() {
            let _env = EnvGuard::new(&["WEBPIPE_CACHE_DIR"]);

            use axum::{routing::get, Router};
            use std::net::SocketAddr;
            let app = Router::new().route(
                "/",
                get(|| async {
                    (
                        [(axum::http::header::CONTENT_TYPE, "text/html")],
                        "<html><body><main><h1>Title</h1>\
                         <p>This paragraph is long enough to be selected as a default chunk even without a query.</p>\
                         <p>Second paragraph is also long enough to be useful as evidence for URL-only mode.</p>\
                         </main></body></html>",
                    )
                }),
            );
            let listener = tokio::net::TcpListener::bind("127.0.0.1:0").await.unwrap();
            let addr: SocketAddr = listener.local_addr().unwrap();
            tokio::spawn(async move {
                axum::serve(listener, app).await.expect("axum serve");
            });
            let url = format!("http://{}/", addr);

            let svc = WebpipeMcp::new().expect("new");
            let r = svc
                .web_search_extract(p(WebSearchExtractArgs {
                    query: None,
                    urls: Some(vec![url]),
                    url_selection_mode: Some("auto".to_string()),
                    provider: Some("auto".to_string()),
                    auto_mode: Some("fallback".to_string()),
                    selection_mode: Some("score".to_string()),
                    fetch_backend: Some("local".to_string()),
                    no_network: Some(false),
                    firecrawl_fallback_on_empty_extraction: Some(false),
                    firecrawl_fallback_on_low_signal: Some(false),
                    render_fallback_on_empty_extraction: Some(false),
                    render_fallback_on_low_signal: Some(false),
                    max_results: Some(1),
                    max_urls: Some(1),
                    timeout_ms: Some(2_000),
                    max_bytes: Some(200_000),
                    retry_on_truncation: Some(false),
                    truncation_retry_max_bytes: None,
                    width: Some(80),
                    max_chars: Some(5_000),
                    top_chunks: Some(2),
                    max_chunk_chars: Some(300),
                    include_links: Some(false),
                    include_structure: Some(false),
                    max_outline_items: None,
                    max_blocks: None,
                    max_block_chars: None,
                    semantic_rerank: None,
                    semantic_top_k: None,
                    max_links: Some(10),
                    include_text: Some(false),
                    cache_read: Some(false),
                    cache_write: Some(false),
                    cache_ttl_s: None,
                    exploration: None,
                    agentic: Some(false),
                    agentic_selector: Some("lexical".to_string()),
                    agentic_max_search_rounds: None,
                    agentic_frontier_max: None,
                    planner_max_calls: None,
                    compact: Some(true),
                    ..Default::default()
                }))
                .await
                .expect("call");

            let v = payload_from_call_tool_result(&r);
            assert_eq!(v["ok"].as_bool(), Some(true));
            let tc = v["top_chunks"].as_array().expect("top_chunks array");
            assert!(!tc.is_empty());
            assert!(tc[0]
                .get("text")
                .and_then(|x| x.as_str())
                .unwrap_or("")
                .contains("long enough"));
        }

        #[tokio::test]
        async fn web_search_extract_agentic_hops_to_query_relevant_link_when_max_urls_ge_2() {
            let _env = EnvGuard::new(&["WEBPIPE_CACHE_DIR"]);

            // Local fixture: a docs landing page with multiple links, only one of which
            // contains both query tokens ("route" and "handlers") in the URL and anchor text.
            use axum::{routing::get, Router};
            use std::net::SocketAddr;
            let landing_html = r#"
<html><body>
  <main>
    <h1>Docs</h1>
    <p>Welcome to the docs landing page.</p>
    <nav>
      <a href="/docs/app/getting-started/routing">Routing</a>
      <a href="/docs/app/getting-started/route-handlers">Route Handlers</a>
      <a href="/docs/app/getting-started/installation">Installation</a>
    </nav>
  </main>
</body></html>
"#;
            let routing_html = r#"
<html><body><main>
  <h1>Routing</h1>
  <p>This page is about routing but does not talk about handlers.</p>
</main></body></html>
"#;
            let handlers_html = r#"
<html><body><main>
  <h1>Route Handlers</h1>
  <p>Route Handlers let you create custom request handlers for a given route.</p>
</main></body></html>
"#;

            let app = Router::new()
                .route(
                    "/docs",
                    get({
                        let landing_html = landing_html.to_string();
                        move || {
                            let landing_html = landing_html.clone();
                            async move {
                                (
                                    [(axum::http::header::CONTENT_TYPE, "text/html")],
                                    landing_html,
                                )
                            }
                        }
                    }),
                )
                .route(
                    "/docs/app/getting-started/routing",
                    get({
                        let routing_html = routing_html.to_string();
                        move || {
                            let routing_html = routing_html.clone();
                            async move {
                                (
                                    [(axum::http::header::CONTENT_TYPE, "text/html")],
                                    routing_html,
                                )
                            }
                        }
                    }),
                )
                .route(
                    "/docs/app/getting-started/route-handlers",
                    get({
                        let handlers_html = handlers_html.to_string();
                        move || {
                            let handlers_html = handlers_html.clone();
                            async move {
                                (
                                    [(axum::http::header::CONTENT_TYPE, "text/html")],
                                    handlers_html,
                                )
                            }
                        }
                    }),
                );

            let listener = tokio::net::TcpListener::bind("127.0.0.1:0").await.unwrap();
            let addr: SocketAddr = listener.local_addr().unwrap();
            tokio::spawn(async move {
                axum::serve(listener, app).await.expect("axum serve");
            });
            let landing_url = format!("http://{}/docs", addr);
            let handlers_url = format!("http://{}/docs/app/getting-started/route-handlers", addr);

            let svc = WebpipeMcp::new().expect("new");
            let r = svc
                .web_search_extract(p(WebSearchExtractArgs {
                    query: Some("route handlers".to_string()),
                    urls: Some(vec![landing_url]),
                    url_selection_mode: Some("auto".to_string()),
                    provider: Some("auto".to_string()),
                    auto_mode: Some("fallback".to_string()),
                    selection_mode: Some("score".to_string()),
                    fetch_backend: Some("local".to_string()),
                    no_network: Some(false),
                    firecrawl_fallback_on_empty_extraction: Some(false),
                    firecrawl_fallback_on_low_signal: Some(false),
                    render_fallback_on_empty_extraction: Some(false),
                    render_fallback_on_low_signal: Some(false),
                    max_results: Some(1),
                    max_urls: Some(2),
                    timeout_ms: Some(2_000),
                    max_bytes: Some(200_000),
                    retry_on_truncation: Some(false),
                    truncation_retry_max_bytes: None,
                    width: Some(80),
                    max_chars: Some(10_000),
                    top_chunks: Some(5),
                    max_chunk_chars: Some(300),
                    include_links: Some(false),
                    include_structure: Some(false),
                    max_outline_items: None,
                    max_blocks: None,
                    max_block_chars: None,
                    semantic_rerank: None,
                    semantic_top_k: None,
                    max_links: Some(50),
                    include_text: Some(false),
                    cache_read: Some(false),
                    cache_write: Some(false),
                    cache_ttl_s: None,
                    exploration: None,
                    agentic: Some(true),
                    agentic_selector: Some("lexical".to_string()),
                    agentic_max_search_rounds: Some(1),
                    agentic_frontier_max: Some(200),
                    planner_max_calls: Some(0),
                    compact: Some(true),
                    ..Default::default()
                }))
                .await
                .expect("call");

            let v = payload_from_call_tool_result(&r);
            assert_eq!(v["ok"].as_bool(), Some(true));

            // We should have fetched two pages: landing + route-handlers.
            let results = v["results"].as_array().expect("results array");
            assert_eq!(results.len(), 2);
            assert!(results.iter().any(|x| {
                x.get("final_url")
                    .and_then(|u| u.as_str())
                    .map(|u| u == handlers_url)
                    .unwrap_or(false)
            }));

            // And the best chunk should come from the route-handlers page.
            let top_chunks = v["top_chunks"].as_array().expect("top_chunks array");
            assert!(top_chunks.iter().any(|c| {
                c.get("url")
                    .and_then(|u| u.as_str())
                    .map(|u| u == handlers_url)
                    .unwrap_or(false)
                    && c.get("text")
                        .and_then(|t| t.as_str())
                        .unwrap_or("")
                        .to_ascii_lowercase()
                        .contains("custom request handlers")
            }));
        }

        #[tokio::test]
        async fn web_search_extract_agentic_can_hop_via_anchor_text_even_when_parent_has_no_hits() {
            let _env = EnvGuard::new(&["WEBPIPE_CACHE_DIR"]);

            // The landing page does NOT contain the query tokens in its text (so parent_relevance==0),
            // but it links out with informative anchor text. Agentic selection should still hop.
            use axum::{routing::get, Router};
            use std::net::SocketAddr;

            let landing_html = r#"
<html><body>
  <main>
    <h1>Docs</h1>
    <p>Welcome. This page talks about installation and getting started only.</p>
    <nav>
      <a href="/x1">Route Handlers</a>
      <a href="/route-handlers">Click</a>
    </nav>
  </main>
</body></html>
"#;
            let x1_html = r#"
<html><body><main>
  <h1>Route Handlers</h1>
  <p>TAIL_SENTINEL: Route Handlers let you create custom request handlers for a given route.</p>
</main></body></html>
"#;
            let route_handlers_html = r#"
<html><body><main>
  <h1>Misc</h1>
  <p>This page URL looks relevant but the content is not.</p>
</main></body></html>
"#;

            let app =
                Router::new()
                    .route(
                        "/docs",
                        get({
                            let landing_html = landing_html.to_string();
                            move || {
                                let landing_html = landing_html.clone();
                                async move {
                                    (
                                        [(axum::http::header::CONTENT_TYPE, "text/html")],
                                        landing_html,
                                    )
                                }
                            }
                        }),
                    )
                    .route(
                        "/x1",
                        get({
                            let x1_html = x1_html.to_string();
                            move || {
                                let x1_html = x1_html.clone();
                                async move {
                                    ([(axum::http::header::CONTENT_TYPE, "text/html")], x1_html)
                                }
                            }
                        }),
                    )
                    .route(
                        "/route-handlers",
                        get({
                            let route_handlers_html = route_handlers_html.to_string();
                            move || {
                                let route_handlers_html = route_handlers_html.clone();
                                async move {
                                    (
                                        [(axum::http::header::CONTENT_TYPE, "text/html")],
                                        route_handlers_html,
                                    )
                                }
                            }
                        }),
                    );

            let listener = tokio::net::TcpListener::bind("127.0.0.1:0").await.unwrap();
            let addr: SocketAddr = listener.local_addr().unwrap();
            tokio::spawn(async move {
                axum::serve(listener, app).await.expect("axum serve");
            });
            let landing_url = format!("http://{}/docs", addr);
            let x1_url = format!("http://{}/x1", addr);

            let svc = WebpipeMcp::new().expect("new");
            let r = svc
                .web_search_extract(p(WebSearchExtractArgs {
                    query: Some("route handlers".to_string()),
                    urls: Some(vec![landing_url]),
                    url_selection_mode: Some("auto".to_string()),
                    provider: Some("auto".to_string()),
                    auto_mode: Some("fallback".to_string()),
                    selection_mode: Some("score".to_string()),
                    fetch_backend: Some("local".to_string()),
                    no_network: Some(false),
                    firecrawl_fallback_on_empty_extraction: Some(false),
                    firecrawl_fallback_on_low_signal: Some(false),
                    render_fallback_on_empty_extraction: Some(false),
                    render_fallback_on_low_signal: Some(false),
                    max_results: Some(1),
                    max_urls: Some(2),
                    timeout_ms: Some(2_000),
                    max_bytes: Some(200_000),
                    retry_on_truncation: Some(false),
                    truncation_retry_max_bytes: None,
                    width: Some(80),
                    max_chars: Some(10_000),
                    top_chunks: Some(5),
                    max_chunk_chars: Some(300),
                    include_links: Some(false),
                    include_structure: Some(false),
                    max_outline_items: None,
                    max_blocks: None,
                    max_block_chars: None,
                    semantic_rerank: None,
                    semantic_top_k: None,
                    max_links: Some(50),
                    include_text: Some(false),
                    cache_read: Some(false),
                    cache_write: Some(false),
                    cache_ttl_s: None,
                    exploration: None,
                    agentic: Some(true),
                    agentic_selector: Some("lexical".to_string()),
                    agentic_max_search_rounds: Some(1),
                    agentic_frontier_max: Some(200),
                    planner_max_calls: Some(0),
                    compact: Some(true),
                    ..Default::default()
                }))
                .await
                .expect("call");

            let v = payload_from_call_tool_result(&r);
            assert_eq!(v["ok"].as_bool(), Some(true));
            let results = v["results"].as_array().expect("results array");
            assert_eq!(results.len(), 2);
            assert!(results.iter().any(|x| {
                x.get("final_url")
                    .and_then(|u| u.as_str())
                    .map(|u| u == x1_url)
                    .unwrap_or(false)
            }));
            let top_chunks = v["top_chunks"].as_array().expect("top_chunks array");
            assert!(top_chunks.iter().any(|c| {
                c.get("url")
                    .and_then(|u| u.as_str())
                    .map(|u| u == x1_url)
                    .unwrap_or(false)
                    && c.get("text")
                        .and_then(|t| t.as_str())
                        .unwrap_or("")
                        .contains("TAIL_SENTINEL")
            }));
        }

        #[tokio::test]
        async fn web_extract_warns_on_empty_extraction_for_html() {
            let _env = EnvGuard::new(&["WEBPIPE_CACHE_DIR"]);

            // HTML that is likely to extract to empty text.
            use axum::{routing::get, Router};
            use std::net::SocketAddr;
            let app = Router::new().route(
                "/",
                get(|| async {
                    (
                        [(axum::http::header::CONTENT_TYPE, "text/html")],
                        "<html><head><script>var x = 1;</script></head><body></body></html>",
                    )
                }),
            );
            let listener = tokio::net::TcpListener::bind("127.0.0.1:0").await.unwrap();
            let addr: SocketAddr = listener.local_addr().unwrap();
            tokio::spawn(async move {
                axum::serve(listener, app).await.expect("axum serve");
            });
            let url = format!("http://{}/", addr);

            let svc = WebpipeMcp::new().expect("new");
            let r = svc
                .web_extract(p(WebExtractArgs {
                    url: Some(url),
                    fetch_backend: Some("local".to_string()),
                    no_network: Some(false),
                    width: Some(80),
                    max_chars: Some(2_000),
                    query: None,
                    top_chunks: Some(3),
                    max_chunk_chars: Some(200),
                    include_text: Some(false),
                    include_links: Some(false),
                    max_links: Some(10),
                    timeout_ms: Some(2_000),
                    max_bytes: Some(200_000),
                    retry_on_truncation: None,
                    truncation_retry_max_bytes: None,
                    cache_read: Some(true),
                    cache_write: Some(true),
                    cache_ttl_s: None,
                    include_structure: None,
                    max_outline_items: None,
                    max_blocks: None,
                    max_block_chars: None,
                    semantic_rerank: None,
                    semantic_auto_fallback: None,
                    semantic_top_k: None,
                }))
                .await
                .expect("call");

            let v = payload_from_call_tool_result(&r);
            assert_eq!(v["kind"].as_str(), Some("web_extract"));
            assert_eq!(v["ok"].as_bool(), Some(true));
            // Script-only HTML should be treated as "no useful main content" (either empty or low-signal),
            // without requiring a specific extraction engine behavior.
            let n = v
                .get("extract")
                .and_then(|e| e.get("text_chars"))
                .and_then(|x| x.as_u64())
                .unwrap_or(0);
            assert!(
                n <= 200,
                "unexpectedly large text_chars={n} for script-only HTML"
            );
            let empty = Vec::<serde_json::Value>::new();
            let warns: Vec<&str> = v["warnings"]
                .as_array()
                .unwrap_or(&empty)
                .iter()
                .filter_map(|x| x.as_str())
                .collect();
            assert!(
                warns.contains(&"empty_extraction")
                    || warns.contains(&"main_content_low_signal")
                    || warns.contains(&"unsupported_content_no_text"),
                "expected an empty/low-signal warning; got warnings={warns:?}"
            );
        }

        #[tokio::test]
        async fn web_fetch_warns_on_body_truncation() {
            let _env = EnvGuard::new(&["WEBPIPE_CACHE_DIR"]);

            // Large HTML body to force truncation by max_bytes.
            use axum::{routing::get, Router};
            use std::net::SocketAddr;
            let big = "x".repeat(10_000);
            let app = Router::new().route(
                "/",
                get(move || {
                    let body = big.clone();
                    async move { ([(axum::http::header::CONTENT_TYPE, "text/html")], body) }
                }),
            );
            let listener = tokio::net::TcpListener::bind("127.0.0.1:0").await.unwrap();
            let addr: SocketAddr = listener.local_addr().unwrap();
            tokio::spawn(async move {
                axum::serve(listener, app).await.expect("axum serve");
            });
            let url = format!("http://{}/", addr);

            let svc = WebpipeMcp::new().expect("new");
            let r = svc
                .web_fetch(p(WebFetchArgs {
                    url: Some(url),
                    fetch_backend: Some("local".to_string()),
                    no_network: Some(false),
                    timeout_ms: Some(2_000),
                    max_bytes: Some(200),
                    max_text_chars: Some(1_000),
                    headers: None,
                    cache_read: Some(false),
                    cache_write: Some(false),
                    cache_ttl_s: None,
                    include_text: Some(false),
                    include_headers: Some(false),
                }))
                .await
                .expect("call");

            let v = payload_from_call_tool_result(&r);
            assert_eq!(v["kind"].as_str(), Some("web_fetch"));
            assert_eq!(v["ok"].as_bool(), Some(true));
            assert_eq!(v["truncated"].as_bool(), Some(true));
            assert!(v["warnings"]
                .as_array()
                .unwrap_or(&vec![])
                .iter()
                .any(|x| x.as_str() == Some("body_truncated_by_max_bytes")));
            assert!(v["warning_codes"]
                .as_array()
                .unwrap_or(&vec![])
                .iter()
                .any(|x| x.as_str() == Some("body_truncated_by_max_bytes")));
        }

        #[tokio::test]
        async fn web_search_extract_firecrawl_fallback_on_empty_extraction_is_bounded() {
            // This is a fully offline test: we stand up one local server that:
            // - serves an HTML page that extracts to empty text
            // - acts as a fake Firecrawl v2 scrape endpoint returning markdown
            //
            // Then we run web_search_extract with fetch_backend="local" and
            // firecrawl_fallback_on_empty_extraction=true, and assert it uses the fallback
            // for that URL only (no global behavior change).
            let _env = EnvGuard::new(&[
                "WEBPIPE_CACHE_DIR",
                "WEBPIPE_FIRECRAWL_API_KEY",
                "WEBPIPE_FIRECRAWL_ENDPOINT_V2",
            ]);
            _env.set("WEBPIPE_FIRECRAWL_API_KEY", "test-key");

            use axum::{routing::get, routing::post, Json, Router};
            use serde_json::json;
            use std::net::SocketAddr;

            let app = Router::new()
                .route(
                    "/page",
                    get(|| async {
                        (
                            [(axum::http::header::CONTENT_TYPE, "text/html")],
                            "<html><head><script>var x = 1;</script></head><body></body></html>",
                        )
                    }),
                )
                .route(
                    "/v2/scrape",
                    post(|_body: Json<serde_json::Value>| async move {
                        Json(json!({ "success": true, "data": { "markdown": "# Hi\n\nHello world" } }))
                    }),
                );
            let listener = tokio::net::TcpListener::bind("127.0.0.1:0").await.unwrap();
            let addr: SocketAddr = listener.local_addr().unwrap();
            tokio::spawn(async move {
                axum::serve(listener, app).await.expect("axum serve");
            });

            _env.set(
                "WEBPIPE_FIRECRAWL_ENDPOINT_V2",
                &format!("http://{}/v2/scrape", addr),
            );

            let url = format!("http://{}/page", addr);

            let svc = WebpipeMcp::new().expect("new");
            let r = svc
                .web_search_extract(p(WebSearchExtractArgs {
                    query: Some("hello".to_string()),
                    urls: Some(vec![url.clone()]),
                    url_selection_mode: None,
                    provider: Some("auto".to_string()),
                    auto_mode: Some("merge".to_string()),
                    selection_mode: None,
                    fetch_backend: Some("local".to_string()),
                    no_network: Some(false),
                    firecrawl_fallback_on_empty_extraction: Some(true),
                    firecrawl_fallback_on_low_signal: Some(false),
                    render_fallback_on_empty_extraction: Some(false),
                    render_fallback_on_low_signal: Some(false),
                    max_results: Some(1),
                    max_urls: Some(1),
                    timeout_ms: Some(2_000),
                    max_bytes: Some(200_000),
                    width: Some(80),
                    max_chars: Some(2_000),
                    top_chunks: Some(3),
                    max_chunk_chars: Some(200),
                    include_links: Some(true),
                    include_structure: None,
                    max_outline_items: None,
                    max_blocks: None,
                    max_block_chars: None,
                    semantic_rerank: None,
                    semantic_top_k: None,
                    max_links: Some(10),
                    include_text: Some(true),
                    cache_read: Some(false),
                    cache_write: Some(false),
                    cache_ttl_s: None,
                    exploration: None,
                    agentic: Some(false),
                    agentic_selector: Some("lexical".to_string()),
                    agentic_max_search_rounds: None,
                    agentic_frontier_max: None,
                    planner_max_calls: None,
                    retry_on_truncation: None,
                    truncation_retry_max_bytes: None,
                    // This test asserts debug-ish per-URL fields that are omitted in compact mode.
                    compact: Some(false),
                    ..Default::default()
                }))
                .await
                .expect("call");

            let v = payload_from_call_tool_result(&r);
            assert_eq!(v["kind"].as_str(), Some("web_search_extract"));
            assert_eq!(v["ok"].as_bool(), Some(true));
            assert_eq!(
                v["request"]["firecrawl_fallback_on_empty_extraction"].as_bool(),
                Some(true)
            );

            let results = v["results"].as_array().expect("results array");
            assert_eq!(results.len(), 1);
            let one = &results[0];
            assert_eq!(one["ok"].as_bool(), Some(true));
            // Per-URL fetch_backend reflects the *used* backend after fallback.
            assert_eq!(one["fetch_backend"].as_str(), Some("firecrawl"));
            assert!(one["warnings"]
                .as_array()
                .unwrap()
                .iter()
                .any(|x| x.as_str() == Some("firecrawl_fallback_on_empty_extraction")));
            assert!(one["warning_codes"]
                .as_array()
                .unwrap()
                .iter()
                .any(|x| x.as_str() == Some("firecrawl_fallback_on_empty_extraction")));
            assert!(one["extract"]["text"]
                .as_str()
                .unwrap_or("")
                .contains("Hello world"));
            // Link extraction should be disabled in fallback mode.
            assert_eq!(one["extract"]["links"].as_array().unwrap().len(), 0);
            assert!(one.get("attempts").is_some());
        }

        #[tokio::test]
        async fn web_extract_firecrawl_bytes_are_raw_markdown_bytes() {
            // Offline: fake Firecrawl v2 endpoint returning a fixed markdown payload.
            let _env = EnvGuard::new(&[
                "WEBPIPE_CACHE_DIR",
                "WEBPIPE_FIRECRAWL_API_KEY",
                "WEBPIPE_FIRECRAWL_ENDPOINT_V2",
            ]);
            _env.set("WEBPIPE_FIRECRAWL_API_KEY", "test-key");

            use axum::{routing::post, Json, Router};
            use serde_json::json;
            use std::net::SocketAddr;

            let md = "# Hi\n\nHello world";
            let expected_bytes = md.len() as u64;

            let app = Router::new().route(
                "/v2/scrape",
                post(move |_body: Json<serde_json::Value>| {
                    let md0 = md.to_string();
                    async move { Json(json!({ "success": true, "data": { "markdown": md0 } })) }
                }),
            );
            let listener = tokio::net::TcpListener::bind("127.0.0.1:0").await.unwrap();
            let addr: SocketAddr = listener.local_addr().unwrap();
            tokio::spawn(async move {
                axum::serve(listener, app).await.expect("axum serve");
            });

            _env.set(
                "WEBPIPE_FIRECRAWL_ENDPOINT_V2",
                &format!("http://{}/v2/scrape", addr),
            );

            let svc = WebpipeMcp::new().expect("new");
            let r = svc
                .web_extract(p(WebExtractArgs {
                    url: Some("https://example.com/".to_string()),
                    fetch_backend: Some("firecrawl".to_string()),
                    no_network: Some(false),
                    width: Some(80),
                    max_chars: Some(2_000),
                    query: None,
                    top_chunks: Some(3),
                    max_chunk_chars: Some(200),
                    include_text: Some(true),
                    include_links: Some(true),
                    max_links: Some(10),
                    timeout_ms: Some(2_000),
                    max_bytes: Some(200_000),
                    retry_on_truncation: None,
                    truncation_retry_max_bytes: None,
                    cache_read: Some(false),
                    cache_write: Some(false),
                    cache_ttl_s: None,
                    include_structure: Some(false),
                    max_outline_items: None,
                    max_blocks: None,
                    max_block_chars: None,
                    semantic_rerank: None,
                    semantic_auto_fallback: None,
                    semantic_top_k: None,
                }))
                .await
                .expect("call");

            let v = payload_from_call_tool_result(&r);
            assert_eq!(v["kind"].as_str(), Some("web_extract"));
            assert_eq!(v["ok"].as_bool(), Some(true));
            assert_eq!(v["fetch_backend"].as_str(), Some("firecrawl"));
            assert_eq!(v["bytes"].as_u64(), Some(expected_bytes));
            assert_eq!(v["extract"]["text_chars"].as_u64(), Some(expected_bytes));
        }

        #[tokio::test]
        async fn warning_codes_normalize_links_unavailable_for_firecrawl() {
            // Offline: Firecrawl path always has links unavailable (we return []), so it emits
            // canonical warning code `links_unavailable`.
            let _env = EnvGuard::new(&[
                "WEBPIPE_CACHE_DIR",
                "WEBPIPE_FIRECRAWL_API_KEY",
                "WEBPIPE_FIRECRAWL_ENDPOINT_V2",
            ]);
            _env.set("WEBPIPE_FIRECRAWL_API_KEY", "test-key");

            use axum::{routing::post, Json, Router};
            use serde_json::json;
            use std::net::SocketAddr;

            let app = Router::new().route(
                "/v2/scrape",
                post(|_body: Json<serde_json::Value>| async move {
                    Json(json!({ "success": true, "data": { "markdown": "# Hi\n\nHello world" } }))
                }),
            );
            let listener = tokio::net::TcpListener::bind("127.0.0.1:0").await.unwrap();
            let addr: SocketAddr = listener.local_addr().unwrap();
            tokio::spawn(async move {
                axum::serve(listener, app).await.expect("axum serve");
            });

            _env.set(
                "WEBPIPE_FIRECRAWL_ENDPOINT_V2",
                &format!("http://{}/v2/scrape", addr),
            );

            let svc = WebpipeMcp::new().expect("new");
            let r = svc
                .web_extract(p(WebExtractArgs {
                    url: Some("https://example.com/".to_string()),
                    fetch_backend: Some("firecrawl".to_string()),
                    no_network: Some(false),
                    width: Some(80),
                    max_chars: Some(2_000),
                    query: None,
                    top_chunks: Some(3),
                    max_chunk_chars: Some(200),
                    include_text: Some(true),
                    include_links: Some(true),
                    max_links: Some(10),
                    timeout_ms: Some(2_000),
                    max_bytes: Some(200_000),
                    retry_on_truncation: None,
                    truncation_retry_max_bytes: None,
                    cache_read: Some(false),
                    cache_write: Some(false),
                    cache_ttl_s: None,
                    include_structure: Some(false),
                    max_outline_items: None,
                    max_blocks: None,
                    max_block_chars: None,
                    semantic_rerank: None,
                    semantic_auto_fallback: None,
                    semantic_top_k: None,
                }))
                .await
                .expect("call");

            let v = payload_from_call_tool_result(&r);
            let empty: Vec<serde_json::Value> = vec![];
            let codes = v["warning_codes"].as_array().unwrap_or(&empty);
            assert!(
                codes
                    .iter()
                    .any(|x| x.as_str() == Some("links_unavailable")),
                "expected links_unavailable in warning_codes"
            );
        }

        #[tokio::test]
        async fn webpipe_meta_provider_auto_order_matrix() {
            // This test encodes the brave-first policy in executable form.
            let _env = EnvGuard::new(&[
                "WEBPIPE_BRAVE_API_KEY",
                "BRAVE_SEARCH_API_KEY",
                "WEBPIPE_TAVILY_API_KEY",
                "TAVILY_API_KEY",
                "WEBPIPE_FIRECRAWL_API_KEY",
                "FIRECRAWL_API_KEY",
                "WEBPIPE_CACHE_DIR",
            ]);

            let svc = WebpipeMcp::new().expect("new");

            // none configured
            let v0 = payload_from_call_tool_result(
                &svc.webpipe_meta(Parameters(None)).await.expect("meta"),
            );
            assert_eq!(v0["ok"].as_bool(), Some(true));
            assert_eq!(
                v0["defaults"]["web_search"]["provider"].as_str(),
                Some("brave")
            );
            assert_eq!(
                v0["defaults"]["web_search"]["provider_auto_order"]
                    .as_array()
                    .unwrap()
                    .len(),
                0
            );
            assert_eq!(
                v0["configured"]["providers"]["brave"].as_bool(),
                Some(false)
            );
            assert_eq!(
                v0["configured"]["providers"]["tavily"].as_bool(),
                Some(false)
            );
            // Opportunistic tooling capability surface should exist (even when all keys are missing).
            assert!(v0["local_tools"].as_object().is_some());
            assert!(v0["local_tools"]["yt_dlp"].is_boolean());
            assert!(v0["local_tools"]["pandoc"].is_boolean());
            assert!(v0["local_tools"]["ffmpeg"].is_boolean());
            assert!(v0["local_tools"]["tesseract"].is_boolean());
            assert!(v0["local_tools"]["pdftotext"].is_boolean());
            assert!(v0["local_tools"]["mutool"].is_boolean());
            assert!(v0["configured"]["vision"]["gemini"].is_boolean());
            // Knob list is names-only and stable.
            assert!(v0["supported"]["knobs"]
                .as_array()
                .unwrap()
                .iter()
                .any(|x| x.as_str() == Some("WEBPIPE_PDF_SHELLOUT")));
            assert!(v0["supported"]["knobs"]
                .as_array()
                .unwrap()
                .iter()
                .any(|x| x.as_str() == Some("WEBPIPE_VISION")));

            // brave only
            _env.set("WEBPIPE_BRAVE_API_KEY", "x");
            let v1 = payload_from_call_tool_result(
                &svc.webpipe_meta(Parameters(None)).await.expect("meta"),
            );
            assert_eq!(
                v1["defaults"]["web_search"]["provider_auto_order"]
                    .as_array()
                    .unwrap()
                    .iter()
                    .filter_map(|x| x.as_str())
                    .collect::<Vec<_>>(),
                vec!["brave"]
            );
            assert_eq!(v1["configured"]["providers"]["brave"].as_bool(), Some(true));
            assert_eq!(
                v1["configured"]["providers"]["tavily"].as_bool(),
                Some(false)
            );

            // tavily only (clear brave, set tavily)
            std::env::remove_var("WEBPIPE_BRAVE_API_KEY");
            _env.set("WEBPIPE_TAVILY_API_KEY", "x");
            let v2 = payload_from_call_tool_result(
                &svc.webpipe_meta(Parameters(None)).await.expect("meta"),
            );
            assert_eq!(
                v2["defaults"]["web_search"]["provider_auto_order"]
                    .as_array()
                    .unwrap()
                    .iter()
                    .filter_map(|x| x.as_str())
                    .collect::<Vec<_>>(),
                vec!["tavily"]
            );
            assert_eq!(
                v2["configured"]["providers"]["brave"].as_bool(),
                Some(false)
            );
            assert_eq!(
                v2["configured"]["providers"]["tavily"].as_bool(),
                Some(true)
            );

            // both configured (brave-first ordering)
            _env.set("WEBPIPE_BRAVE_API_KEY", "x");
            let v3 = payload_from_call_tool_result(
                &svc.webpipe_meta(Parameters(None)).await.expect("meta"),
            );
            assert_eq!(
                v3["defaults"]["web_search"]["provider_auto_order"]
                    .as_array()
                    .unwrap()
                    .iter()
                    .filter_map(|x| x.as_str())
                    .collect::<Vec<_>>(),
                vec!["brave", "tavily"]
            );
            assert_eq!(v3["configured"]["providers"]["brave"].as_bool(), Some(true));
            assert_eq!(
                v3["configured"]["providers"]["tavily"].as_bool(),
                Some(true)
            );

            // Supported fetch_backends should include render (Playwright).
            let empty: Vec<serde_json::Value> = vec![];
            let fbs: Vec<&str> = v3["supported"]["fetch_backends"]
                .as_array()
                .unwrap_or(&empty)
                .iter()
                .filter_map(|x| x.as_str())
                .collect();
            assert!(fbs.contains(&"local"));
            assert!(fbs.contains(&"firecrawl"));
            assert!(fbs.contains(&"render"));
        }

        #[tokio::test]
        async fn web_seed_urls_contract_is_stable_and_bounded() {
            let svc = WebpipeMcp::new().expect("new");
            let r = svc
                .web_seed_urls(p(WebSeedUrlsArgs {
                    max: Some(2),
                    include_ids: Some(true),
                    ids_only: Some(false),
                }))
                .await
                .expect("seed urls");
            let v = payload_from_call_tool_result(&r);
            assert_eq!(v["ok"].as_bool(), Some(true));
            assert_eq!(v["kind"].as_str(), Some("web_seed_urls"));
            assert_eq!(v["schema_version"].as_u64(), Some(2));
            assert_eq!(v["max"].as_u64(), Some(2));
            assert_eq!(v["ids"].as_array().map(|a| a.len()), Some(2));
            let seeds = v["seeds"].as_array().expect("seeds array");
            assert_eq!(seeds.len(), 2);
            assert!(seeds[0].get("url").and_then(|x| x.as_str()).is_some());
        }

        #[tokio::test]
        async fn web_seed_search_extract_works_on_local_urls() {
            use axum::{http::header, routing::get, Router};
            use std::net::SocketAddr;

            async fn serve(app: Router) -> SocketAddr {
                let listener = tokio::net::TcpListener::bind("127.0.0.1:0").await.unwrap();
                let addr: SocketAddr = listener.local_addr().unwrap();
                tokio::spawn(async move {
                    axum::serve(listener, app).await.unwrap();
                });
                addr
            }

            // Local “seed docs” with distinctive tokens.
            let app = Router::new()
                .route(
                    "/a",
                    get(|| async {
                        (
                            [(header::CONTENT_TYPE, "text/markdown")],
                            "# A\n\nseedtoken_a unique_a",
                        )
                    }),
                )
                .route(
                    "/b",
                    get(|| async {
                        (
                            [(header::CONTENT_TYPE, "text/markdown")],
                            "# B\n\nseedtoken_b unique_b",
                        )
                    }),
                );
            let addr = serve(app).await;

            let svc = WebpipeMcp::new().expect("new");
            let r = svc
                .web_seed_search_extract(p(WebSeedSearchExtractArgs {
                    query: "unique_b".to_string(),
                    urls: Some(vec![format!("http://{addr}/a"), format!("http://{addr}/b")]),
                    seed_ids: None,
                    max_urls: Some(2),
                    fetch_backend: Some("local".to_string()),
                    no_network: Some(false),
                    width: Some(80),
                    max_chars: Some(10_000),
                    max_bytes: Some(200_000),
                    top_chunks: Some(3),
                    merged_max_per_seed: None,
                    max_chunk_chars: Some(300),
                    cache_read: Some(false),
                    cache_write: Some(false),
                    cache_ttl_s: None,
                    include_text: Some(false),
                    compact: Some(true),
                }))
                .await
                .expect("seed search extract");
            let v = payload_from_call_tool_result(&r);
            assert_eq!(v["ok"].as_bool(), Some(true));
            assert_eq!(v["kind"].as_str(), Some("web_seed_search_extract"));
            let merged = v["results"]["merged_chunks"]
                .as_array()
                .cloned()
                .unwrap_or_default();
            assert!(!merged.is_empty());
            let any = merged.iter().any(|c| {
                c.get("text")
                    .and_then(|x| x.as_str())
                    .is_some_and(|t| t.contains("unique_b"))
            });
            assert!(any, "expected to find query token in merged chunks");
        }

        #[test]
        fn seed_registry_ids_are_unique_and_nonempty() {
            let mut seen = std::collections::BTreeSet::<&'static str>::new();
            for s in seed_registry() {
                assert!(!s.id.trim().is_empty());
                assert!(!s.url.trim().is_empty());
                assert!(seen.insert(s.id), "duplicate seed id: {}", s.id);
            }
        }

        #[test]
        fn select_seed_urls_defaults_to_registry_order() {
            let (picked, unknown) = select_seed_urls(seed_registry(), None, None, 3);
            assert!(unknown.is_empty());
            assert_eq!(picked.len(), 3);
            assert_eq!(picked[0].0, "public-apis");
            assert_eq!(picked[1].0, "awesome-selfhosted");
            assert_eq!(picked[2].0, "awesome");
        }

        #[test]
        fn select_seed_urls_respects_seed_ids_order_and_reports_unknowns() {
            let (picked, unknown) = select_seed_urls(
                seed_registry(),
                None,
                Some(vec![
                    "awesome-python".to_string(),
                    "nope".to_string(),
                    "public-apis".to_string(),
                    "awesome-python".to_string(), // duplicate should be ignored
                ]),
                10,
            );
            assert_eq!(picked.len(), 2);
            assert_eq!(picked[0].0, "awesome-python");
            assert_eq!(picked[1].0, "public-apis");
            assert_eq!(unknown, vec!["nope".to_string()]);
        }

        #[test]
        fn web_seed_urls_ids_only_includes_ids_array() {
            let svc = WebpipeMcp::new().expect("new");
            // We don't need async: the tool is async, so use a tokio runtime for a minimal call.
            let rt = tokio::runtime::Runtime::new().expect("rt");
            let r = rt
                .block_on(svc.web_seed_urls(p(WebSeedUrlsArgs {
                    max: Some(5),
                    include_ids: Some(true),
                    ids_only: Some(true),
                })))
                .expect("call");
            let v = payload_from_call_tool_result(&r);
            assert_eq!(v["ok"].as_bool(), Some(true));
            assert_eq!(v["kind"].as_str(), Some("web_seed_urls"));
            assert_eq!(v["ids"].as_array().map(|a| a.len()), Some(5));
            // ids_only should omit kind/note in seeds entries.
            let seeds = v["seeds"].as_array().unwrap();
            assert_eq!(seeds.len(), 5);
            assert!(seeds[0].get("id").is_some());
            assert!(seeds[0].get("url").is_some());
            assert!(seeds[0].get("kind").is_none());
            assert!(seeds[0].get("note").is_none());
        }

        #[tokio::test]
        async fn web_seed_search_extract_rejects_all_unknown_seed_ids() {
            let svc = WebpipeMcp::new().expect("new");
            let r = svc
                .web_seed_search_extract(Parameters(Some(WebSeedSearchExtractArgs {
                    query: "anything".to_string(),
                    urls: None,
                    seed_ids: Some(vec!["nope1".to_string(), "nope2".to_string()]),
                    max_urls: Some(5),
                    fetch_backend: Some("local".to_string()),
                    no_network: Some(true),
                    width: Some(80),
                    max_chars: Some(2000),
                    max_bytes: Some(200_000),
                    top_chunks: Some(3),
                    merged_max_per_seed: None,
                    max_chunk_chars: Some(200),
                    cache_read: Some(true),
                    cache_write: Some(false),
                    cache_ttl_s: None,
                    include_text: Some(false),
                    compact: Some(true),
                })))
                .await
                .expect("call");
            let v = payload_from_call_tool_result(&r);
            assert_eq!(v["ok"].as_bool(), Some(false));
            assert_eq!(
                v["error"]["code"].as_str(),
                Some(ErrorCode::InvalidParams.as_str())
            );
            assert_eq!(v["unknown_seed_ids"].as_array().map(|a| a.len()), Some(2));
        }

        #[tokio::test]
        async fn web_search_rejects_empty_query() {
            let _env = EnvGuard::new(&SEARCH_ENV_KEYS);
            let svc = WebpipeMcp::new().expect("new");
            let r = svc
                .web_search(Parameters(Some(WebSearchArgs {
                    query: Some("   ".to_string()),
                    provider: Some("brave".to_string()),
                    auto_mode: None,
                    max_results: Some(5),
                    language: None,
                    country: None,
                    ..Default::default()
                })))
                .await
                .expect("call");
            let v = payload_from_call_tool_result(&r);
            assert_eq!(v["ok"].as_bool(), Some(false));
            assert_eq!(v["provider"].as_str(), Some("brave"));
            assert_eq!(v["request"]["provider"].as_str(), Some("brave"));
            assert_eq!(
                v["error"]["code"].as_str(),
                Some(ErrorCode::InvalidParams.as_str())
            );
        }

        #[tokio::test]
        async fn web_search_rejects_unknown_provider() {
            let _env = EnvGuard::new(&SEARCH_ENV_KEYS);
            let svc = WebpipeMcp::new().expect("new");
            let r = svc
                .web_search(Parameters(Some(WebSearchArgs {
                    query: Some("q".to_string()),
                    provider: Some("nope".to_string()),
                    auto_mode: None,
                    max_results: Some(1),
                    language: None,
                    country: None,
                    ..Default::default()
                })))
                .await
                .expect("call");
            let v = payload_from_call_tool_result(&r);
            assert_eq!(v["ok"].as_bool(), Some(false));
            assert_eq!(v["provider"].as_str(), Some("nope"));
            assert_eq!(v["request"]["provider"].as_str(), Some("nope"));
            assert_eq!(
                v["error"]["code"].as_str(),
                Some(ErrorCode::InvalidParams.as_str())
            );
        }

        #[tokio::test]
        async fn web_fetch_rejects_empty_url() {
            let _env = EnvGuard::new(&["WEBPIPE_CACHE_DIR"]);
            let svc = WebpipeMcp::new().expect("new");
            let r = svc
                .web_fetch(Parameters(Some(WebFetchArgs {
                    url: Some("   ".to_string()),
                    fetch_backend: None,
                    no_network: None,
                    timeout_ms: None,
                    max_bytes: None,
                    max_text_chars: None,
                    headers: None,
                    cache_read: None,
                    cache_write: None,
                    cache_ttl_s: None,
                    include_headers: None,
                    include_text: None,
                })))
                .await
                .expect("call");
            let v = payload_from_call_tool_result(&r);
            assert_eq!(v["ok"].as_bool(), Some(false));
            assert_eq!(
                v["error"]["code"].as_str(),
                Some(ErrorCode::InvalidParams.as_str())
            );
        }

        #[tokio::test]
        async fn web_extract_rejects_empty_url() {
            let _env = EnvGuard::new(&["WEBPIPE_CACHE_DIR"]);
            let svc = WebpipeMcp::new().expect("new");
            let r = svc
                .web_extract(Parameters(Some(WebExtractArgs {
                    url: Some("   ".to_string()),
                    fetch_backend: None,
                    no_network: None,
                    width: None,
                    max_chars: None,
                    query: None,
                    top_chunks: None,
                    max_chunk_chars: None,
                    include_text: None,
                    include_links: None,
                    max_links: None,
                    timeout_ms: None,
                    max_bytes: None,
                    retry_on_truncation: None,
                    truncation_retry_max_bytes: None,
                    cache_read: None,
                    cache_write: None,
                    cache_ttl_s: None,
                    include_structure: None,
                    max_outline_items: None,
                    max_blocks: None,
                    max_block_chars: None,
                    semantic_rerank: None,
                    semantic_auto_fallback: None,
                    semantic_top_k: None,
                })))
                .await
                .expect("call");
            let v = payload_from_call_tool_result(&r);
            assert_eq!(v["ok"].as_bool(), Some(false));
            assert_eq!(
                v["error"]["code"].as_str(),
                Some(ErrorCode::InvalidParams.as_str())
            );
        }

        #[tokio::test]
        async fn web_extract_render_disabled_is_not_configured_and_stable() {
            let env = EnvGuard::new(&["WEBPIPE_CACHE_DIR", "WEBPIPE_RENDER_DISABLE"]);
            env.set("WEBPIPE_RENDER_DISABLE", "1");

            let svc = WebpipeMcp::new().expect("new");
            let r = svc
                .web_extract(Parameters(Some(WebExtractArgs {
                    url: Some("https://example.com/".to_string()),
                    fetch_backend: Some("render".to_string()),
                    no_network: Some(false),
                    width: Some(80),
                    max_chars: Some(2_000),
                    query: None,
                    top_chunks: Some(3),
                    max_chunk_chars: Some(200),
                    include_text: Some(false),
                    include_links: Some(false),
                    max_links: Some(10),
                    timeout_ms: Some(2_000),
                    max_bytes: Some(200_000),
                    retry_on_truncation: None,
                    truncation_retry_max_bytes: None,
                    cache_read: Some(false),
                    cache_write: Some(false),
                    cache_ttl_s: None,
                    include_structure: Some(false),
                    max_outline_items: None,
                    max_blocks: None,
                    max_block_chars: None,
                    semantic_rerank: None,
                    semantic_auto_fallback: None,
                    semantic_top_k: None,
                })))
                .await
                .expect("call");
            let v = payload_from_call_tool_result(&r);
            assert_eq!(v["kind"].as_str(), Some("web_extract"));
            assert_eq!(v["ok"].as_bool(), Some(false));
            assert_eq!(
                v["error"]["code"].as_str(),
                Some(ErrorCode::NotConfigured.as_str())
            );
        }

        #[tokio::test]
        async fn web_search_extract_render_disabled_is_not_configured_and_stable() {
            let env = EnvGuard::new(&["WEBPIPE_CACHE_DIR", "WEBPIPE_RENDER_DISABLE"]);
            env.set("WEBPIPE_RENDER_DISABLE", "1");

            let svc = WebpipeMcp::new().expect("new");
            let r = svc
                .web_search_extract(p(WebSearchExtractArgs {
                    query: Some("x".to_string()),
                    urls: Some(vec!["https://example.com/".to_string()]),
                    url_selection_mode: Some("preserve".to_string()),
                    provider: Some("auto".to_string()),
                    auto_mode: Some("fallback".to_string()),
                    selection_mode: Some("pareto".to_string()),
                    fetch_backend: Some("render".to_string()),
                    no_network: Some(false),
                    firecrawl_fallback_on_empty_extraction: Some(false),
                    firecrawl_fallback_on_low_signal: Some(false),
                    render_fallback_on_empty_extraction: Some(false),
                    render_fallback_on_low_signal: Some(false),
                    max_results: Some(1),
                    max_urls: Some(1),
                    timeout_ms: Some(2_000),
                    max_bytes: Some(200_000),
                    retry_on_truncation: Some(false),
                    truncation_retry_max_bytes: None,
                    width: Some(80),
                    max_chars: Some(2_000),
                    top_chunks: Some(3),
                    max_chunk_chars: Some(200),
                    include_links: Some(false),
                    include_structure: Some(false),
                    max_outline_items: None,
                    max_blocks: None,
                    max_block_chars: None,
                    semantic_rerank: Some(false),
                    semantic_top_k: None,
                    max_links: None,
                    include_text: Some(false),
                    cache_read: Some(false),
                    cache_write: Some(false),
                    cache_ttl_s: None,
                    exploration: Some("balanced".to_string()),
                    agentic: Some(false),
                    agentic_selector: Some("lexical".to_string()),
                    agentic_max_search_rounds: Some(0),
                    agentic_frontier_max: Some(0),
                    planner_max_calls: Some(0),
                    compact: Some(true),
                    ..Default::default()
                }))
                .await
                .expect("call");
            let v = payload_from_call_tool_result(&r);
            assert_eq!(v["kind"].as_str(), Some("web_search_extract"));
            assert_eq!(v["ok"].as_bool(), Some(false));
            assert_eq!(
                v["error"]["code"].as_str(),
                Some(ErrorCode::NotConfigured.as_str())
            );
        }

        #[tokio::test]
        async fn web_extract_render_cdp_mode_rejects_proxy_without_needing_playwright() {
            let env = EnvGuard::new(&[
                "WEBPIPE_CACHE_DIR",
                "WEBPIPE_PRIVACY_MODE",
                "WEBPIPE_ANON_PROXY",
                "WEBPIPE_RENDER_CDP_ENDPOINT",
            ]);
            env.set("WEBPIPE_PRIVACY_MODE", "anonymous");
            env.set("WEBPIPE_ANON_PROXY", "http://127.0.0.1:9");
            env.set("WEBPIPE_RENDER_CDP_ENDPOINT", "http://127.0.0.1:9222");

            let svc = WebpipeMcp::new().expect("new");
            let r = svc
                .web_extract(Parameters(Some(WebExtractArgs {
                    url: Some("https://example.com/".to_string()),
                    fetch_backend: Some("render".to_string()),
                    no_network: Some(false),
                    width: Some(80),
                    max_chars: Some(2_000),
                    query: None,
                    top_chunks: Some(3),
                    max_chunk_chars: Some(200),
                    include_text: Some(false),
                    include_links: Some(false),
                    max_links: Some(10),
                    timeout_ms: Some(2_000),
                    max_bytes: Some(200_000),
                    retry_on_truncation: None,
                    truncation_retry_max_bytes: None,
                    cache_read: Some(false),
                    cache_write: Some(false),
                    cache_ttl_s: None,
                    include_structure: Some(false),
                    max_outline_items: None,
                    max_blocks: None,
                    max_block_chars: None,
                    semantic_rerank: None,
                    semantic_auto_fallback: None,
                    semantic_top_k: None,
                })))
                .await
                .expect("call");
            let v = payload_from_call_tool_result(&r);
            assert_eq!(v["kind"].as_str(), Some("web_extract"));
            assert_eq!(v["ok"].as_bool(), Some(false));
            assert_eq!(
                v["error"]["code"].as_str(),
                Some(ErrorCode::NotSupported.as_str())
            );
        }

        #[tokio::test]
        async fn web_extract_render_rejects_both_cdp_and_user_data_dir_without_needing_playwright()
        {
            let env = EnvGuard::new(&[
                "WEBPIPE_CACHE_DIR",
                "WEBPIPE_PRIVACY_MODE",
                "WEBPIPE_RENDER_CDP_ENDPOINT",
                "WEBPIPE_RENDER_USER_DATA_DIR",
            ]);
            env.set("WEBPIPE_PRIVACY_MODE", "normal");
            env.set("WEBPIPE_RENDER_CDP_ENDPOINT", "http://127.0.0.1:9222");
            env.set("WEBPIPE_RENDER_USER_DATA_DIR", "/tmp/webpipe-test-profile");

            let svc = WebpipeMcp::new().expect("new");
            let r = svc
                .web_extract(Parameters(Some(WebExtractArgs {
                    url: Some("https://example.com/".to_string()),
                    fetch_backend: Some("render".to_string()),
                    no_network: Some(false),
                    width: Some(80),
                    max_chars: Some(2_000),
                    query: None,
                    top_chunks: Some(3),
                    max_chunk_chars: Some(200),
                    include_text: Some(false),
                    include_links: Some(false),
                    max_links: Some(10),
                    timeout_ms: Some(2_000),
                    max_bytes: Some(200_000),
                    retry_on_truncation: None,
                    truncation_retry_max_bytes: None,
                    cache_read: Some(false),
                    cache_write: Some(false),
                    cache_ttl_s: None,
                    include_structure: Some(false),
                    max_outline_items: None,
                    max_blocks: None,
                    max_block_chars: None,
                    semantic_rerank: None,
                    semantic_auto_fallback: None,
                    semantic_top_k: None,
                })))
                .await
                .expect("call");
            let v = payload_from_call_tool_result(&r);
            assert_eq!(v["kind"].as_str(), Some("web_extract"));
            assert_eq!(v["ok"].as_bool(), Some(false));
            assert_eq!(
                v["error"]["code"].as_str(),
                Some(ErrorCode::NotSupported.as_str())
            );
        }
    }
}

fn webpipe_dotenv_enabled() -> bool {
    // `WEBPIPE_DOTENV=0` disables *all* env-file loading (both auto and WEBPIPE_ENV_FILE).
    // Any other value (or unset) keeps it enabled.
    match std::env::var("WEBPIPE_DOTENV") {
        Ok(v) => {
            let v = v.trim().to_ascii_lowercase();
            !(v == "0" || v == "false" || v == "off" || v == "no" || v == "disabled")
        }
        Err(_) => true,
    }
}

fn resolve_webpipe_env_file() -> Option<std::path::PathBuf> {
    // 1) Explicit override.
    if let Ok(p) = std::env::var("WEBPIPE_ENV_FILE") {
        let p = p.trim();
        if !p.is_empty() {
            return Some(std::path::PathBuf::from(p));
        }
    }

    // 2) Auto-search upward for `.env`.
    let mut dir = std::env::current_dir().ok()?;
    loop {
        let cand = dir.join(".env");
        if cand.is_file() {
            return Some(cand);
        }
        if !dir.pop() {
            break;
        }
    }
    None
}

fn load_env_file_no_log(path: &std::path::Path) {
    let Ok(txt) = std::fs::read_to_string(path) else {
        return;
    };

    // Within the env file itself, allow later lines to override earlier lines
    // (so "last occurrence wins"), but never override variables that were set
    // in the process environment before loading the file.
    let mut existed_before_file: std::collections::BTreeMap<String, bool> =
        std::collections::BTreeMap::new();
    for raw in txt.lines() {
        let s = raw.trim();
        if s.is_empty() || s.starts_with('#') {
            continue;
        }
        let s = s.strip_prefix("export ").unwrap_or(s).trim();
        let Some((k, v)) = s.split_once('=') else {
            continue;
        };
        let k = k.trim();
        let mut v = v.trim();
        if k.is_empty() {
            continue;
        }

        // Capture whether this key existed before we started applying the env file.
        let had = existed_before_file
            .entry(k.to_string())
            .or_insert_with(|| std::env::var_os(k).is_some());

        // Don't override explicit process env, but do allow overrides within the file.
        if !*had {
            // Strip one layer of surrounding quotes if present.
            if (v.starts_with('"') && v.ends_with('"') && v.len() >= 2)
                || (v.starts_with('\'') && v.ends_with('\'') && v.len() >= 2)
            {
                v = &v[1..v.len() - 1];
            }
            std::env::set_var(k, v);
        }
    }
}

#[cfg(test)]
#[allow(clippy::items_after_test_module)]
mod dotenv_tests {
    use super::*;
    use std::io::Write;

    struct EnvRestore {
        vars: Vec<(&'static str, Option<String>)>,
    }

    impl EnvRestore {
        fn capture(keys: &[&'static str]) -> Self {
            let vars = keys
                .iter()
                .map(|&k| (k, std::env::var(k).ok()))
                .collect::<Vec<_>>();
            Self { vars }
        }
    }

    impl Drop for EnvRestore {
        fn drop(&mut self) {
            for (k, v) in self.vars.drain(..) {
                match v {
                    Some(val) => std::env::set_var(k, val),
                    None => std::env::remove_var(k),
                }
            }
        }
    }

    #[test]
    fn load_env_file_no_log_parses_export_and_quotes_and_respects_process_env() {
        // This loader is part of the CLI/MCP startup contract:
        // - it must not override explicit process env
        // - it should accept simple `.env` shapes (export, quotes, last-wins within file)
        let _restore =
            EnvRestore::capture(&["WEBPIPE_TEST_FOO", "WEBPIPE_TEST_BAR", "WEBPIPE_TEST_DUP"]);

        std::env::set_var("WEBPIPE_TEST_FOO", "from_process_env");

        let mut f = tempfile::NamedTempFile::new().expect("tmp .env");
        writeln!(
            f,
            r#"
            # comment
            export WEBPIPE_TEST_FOO=from_file_should_not_override
            WEBPIPE_TEST_BAR="quoted value"
            WEBPIPE_TEST_DUP=first
            WEBPIPE_TEST_DUP=second
            "#,
        )
        .expect("write");

        load_env_file_no_log(f.path());

        assert_eq!(
            std::env::var("WEBPIPE_TEST_FOO").as_deref(),
            Ok("from_process_env"),
            "process env must not be overridden by env-file loading"
        );
        assert_eq!(
            std::env::var("WEBPIPE_TEST_BAR").as_deref(),
            Ok("quoted value")
        );
        assert_eq!(
            std::env::var("WEBPIPE_TEST_DUP").as_deref(),
            Ok("second"),
            "within-file last occurrence should win"
        );
    }
}

#[tokio::main]
async fn main() -> Result<()> {
    // Env-file loader.
    //
    // Rationale: Cursor/MCP server environments often aren't interactive shells, so users
    // want a single place to keep keys without exporting them manually.
    //
    // Safety:
    // - by default, auto-loads the nearest `.env` in the current directory or an ancestor
    //   directory (so running from `dev/webpipe/` picks up `dev/.env`).
    // - can be disabled with WEBPIPE_DOTENV=0 (useful for tests/CI)
    // - can be explicitly pinned with WEBPIPE_ENV_FILE=/abs/or/rel/path/to/.env
    // - sets vars only if not already set in the process environment
    // - does not log values
    if webpipe_dotenv_enabled() {
        if let Some(p) = resolve_webpipe_env_file() {
            load_env_file_no_log(&p);
        }
    }

    let cli = Cli::parse();

    match cli.command {
        #[cfg(feature = "stdio")]
        Commands::McpStdio => {
            mcp::serve_stdio()
                .await
                .map_err(|e| anyhow::anyhow!(e.to_string()))?;
        }
        #[cfg(feature = "eval")]
        Commands::EvalSearch(args) => {
            let now = args.now_epoch_s.unwrap_or_else(|| {
                std::time::SystemTime::now()
                    .duration_since(std::time::UNIX_EPOCH)
                    .unwrap_or_default()
                    .as_secs()
            });
            let out = args.out.unwrap_or_else(|| {
                std::path::PathBuf::from(format!(".generated/webpipe-eval-search-{now}.json"))
            });
            let providers = args
                .providers
                .split(',')
                .map(|s| s.trim().to_string())
                .filter(|s| !s.is_empty())
                .collect::<Vec<_>>();
            let queries = eval::load_queries(&args.queries_file, &args.query)?;
            let spec = eval::EvalSearchSpec {
                queries,
                providers,
                max_results: args.max_results,
                language: args.language,
                country: args.country,
                out,
                now_epoch_s: Some(now),
            };
            let p = eval::eval_search(spec).await?;
            println!("{}", p.display());
        }
        #[cfg(feature = "eval")]
        Commands::EvalFetch(args) => {
            let now = args.now_epoch_s.unwrap_or_else(|| {
                std::time::SystemTime::now()
                    .duration_since(std::time::UNIX_EPOCH)
                    .unwrap_or_default()
                    .as_secs()
            });
            let out = args.out.unwrap_or_else(|| {
                std::path::PathBuf::from(format!(".generated/webpipe-eval-fetch-{now}.json"))
            });
            let fetchers = args
                .fetchers
                .split(',')
                .map(|s| s.trim().to_string())
                .filter(|s| !s.is_empty())
                .collect::<Vec<_>>();
            let urls = eval::load_urls(&args.urls_file, &args.url)?;
            let spec = eval::EvalFetchSpec {
                urls,
                fetchers,
                timeout_ms: args.timeout_ms,
                max_bytes: args.max_bytes,
                max_text_chars: args.max_text_chars,
                extract: args.extract,
                extract_width: args.extract_width,
                out,
                now_epoch_s: Some(now),
            };
            let p = eval::eval_fetch(spec).await?;
            println!("{}", p.display());
        }
        #[cfg(all(feature = "eval", feature = "stdio"))]
        Commands::EvalSearchExtract(args) => {
            use rmcp::handler::server::wrapper::Parameters;
            let now = args.now_epoch_s.unwrap_or_else(|| {
                std::time::SystemTime::now()
                    .duration_since(std::time::UNIX_EPOCH)
                    .unwrap_or_default()
                    .as_secs()
            });
            let out = args.out.unwrap_or_else(|| {
                std::path::PathBuf::from(format!(
                    ".generated/webpipe-eval-search-extract-{now}.json"
                ))
            });

            std::fs::create_dir_all(
                out.parent()
                    .unwrap_or_else(|| std::path::Path::new(".generated")),
            )?;

            let mut query_items: Vec<serde_json::Value> = Vec::new();
            for p in &args.queries_json {
                let raw = std::fs::read_to_string(p)?;
                let v: serde_json::Value = serde_json::from_str(&raw)?;
                if v.get("schema_version").and_then(|x| x.as_u64()) != Some(1)
                    || v.get("kind").and_then(|x| x.as_str()) != Some("webpipe_seed_queries")
                {
                    anyhow::bail!("unexpected queries_json kind/schema_version");
                }
                if let Some(qs) = v.get("queries").and_then(|x| x.as_array()) {
                    for q in qs {
                        if let (Some(id), Some(text)) = (
                            q.get("query_id").and_then(|x| x.as_str()),
                            q.get("query").and_then(|x| x.as_str()),
                        ) {
                            query_items.push(serde_json::json!({ "query_id": id, "query": text }));
                        }
                    }
                }
            }
            let queries_plain = eval::load_queries(&args.queries_file, &args.query)?;
            for q in queries_plain {
                query_items.push(serde_json::json!({ "query": q }));
            }
            let query_count = query_items.len();
            let urls = eval::load_urls(&args.urls_file, &args.url)?;
            let url_count = urls.len();

            // Reuse the MCP implementation directly (no transport), so evaluation stays aligned
            // with the actual tool behavior and bounds.
            let svc = mcp::WebpipeMcp::new().map_err(|e| anyhow::anyhow!(e.to_string()))?;

            let mut runs: Vec<serde_json::Value> = Vec::new();

            let selection_modes: Vec<String> =
                if let Some(s) = args.compare_selection_modes.as_ref() {
                    s.split(',')
                        .map(|x| x.trim().to_string())
                        .filter(|x| !x.is_empty())
                        .collect()
                } else {
                    vec![args.selection_mode.clone()]
                };
            if selection_modes.len() > 2 {
                anyhow::bail!(
                    "compare_selection_modes currently supports at most 2 modes (got {}). Hint: use \"score,pareto\".",
                    selection_modes.len()
                );
            }
            for m in &selection_modes {
                if m.as_str() != "score" && m.as_str() != "pareto" {
                    anyhow::bail!("unknown selection_mode in compare_selection_modes: {m} (allowed: score, pareto)");
                }
            }

            if !urls.is_empty() {
                // Dataset-style URL mode: run all provided queries against the same URL seed list.
                // If no queries are provided, run once with an empty query (extraction-only profiling).
                let qs: Vec<serde_json::Value> = if query_items.is_empty() {
                    vec![serde_json::json!({ "query": "" })]
                } else {
                    query_items.clone()
                };

                for qi in qs {
                    let q0 = qi
                        .get("query")
                        .and_then(|x| x.as_str())
                        .unwrap_or("")
                        .to_string();
                    let qid = qi
                        .get("query_id")
                        .and_then(|x| x.as_str())
                        .map(|s| s.to_string());
                    if args.compare_selection_modes.is_some() && q0.trim().is_empty() {
                        anyhow::bail!(
                            "URL mode with compare_selection_modes requires a non-empty query. Pass --query \"...\" or --queries-file <file>."
                        );
                    }

                    let mut per_mode: Vec<serde_json::Value> = Vec::new();
                    for m in &selection_modes {
                        let r = svc
                            .web_search_extract(Parameters(Some(mcp::WebSearchExtractArgs {
                                query: if q0.trim().is_empty() {
                                    None
                                } else {
                                    Some(q0.clone())
                                },
                                urls: Some(urls.clone()),
                                url_selection_mode: Some(args.url_selection_mode.clone()),
                                provider: Some(args.provider.clone()),
                                auto_mode: Some(args.auto_mode.clone()),
                                selection_mode: Some(m.clone()),
                                fetch_backend: Some(args.fetch_backend.clone()),
                                no_network: Some(false),
                                firecrawl_fallback_on_empty_extraction: Some(
                                    args.firecrawl_fallback_on_empty_extraction,
                                ),
                                firecrawl_fallback_on_low_signal: Some(false),
                                render_fallback_on_empty_extraction: Some(false),
                                render_fallback_on_low_signal: Some(false),
                                max_results: args.max_results,
                                max_urls: args.max_urls,
                                timeout_ms: Some(args.timeout_ms),
                                max_bytes: Some(args.max_bytes),
                                retry_on_truncation: Some(args.retry_on_truncation),
                                truncation_retry_max_bytes: args.truncation_retry_max_bytes,
                                width: Some(args.width),
                                max_chars: args.max_chars,
                                top_chunks: args.top_chunks,
                                max_chunk_chars: args.max_chunk_chars,
                                include_links: Some(args.include_links),
                                include_structure: None,
                                max_outline_items: None,
                                max_blocks: None,
                                max_block_chars: None,
                                semantic_rerank: None,
                                semantic_top_k: None,
                                max_links: Some(args.max_links),
                                include_text: Some(args.include_text),
                                cache_read: Some(true),
                                cache_write: Some(true),
                                cache_ttl_s: None,
                                exploration: Some(args.exploration.clone()),
                                agentic: Some(args.agentic),
                                agentic_selector: Some(args.agentic_selector.clone()),
                                agentic_max_search_rounds: args.agentic_max_search_rounds,
                                agentic_frontier_max: args.agentic_frontier_max,
                                planner_max_calls: args.planner_max_calls,
                                compact: None,
                                ..Default::default()
                            })))
                            .await
                            .map_err(|e| anyhow::anyhow!(e.to_string()))?;

                        let v: serde_json::Value = if let Some(sc) = r.structured_content.clone() {
                            sc
                        } else {
                            // Back-compat: older tool results echoed JSON as the first `content` item.
                            // Newer tools may put Markdown first; scan for any parseable JSON text.
                            let mut parsed: Option<serde_json::Value> = None;
                            for c in &r.content {
                                let s = c.as_text().map(|t| t.text.as_str()).unwrap_or("").trim();
                                if s.is_empty() {
                                    continue;
                                }
                                if let Ok(v) = serde_json::from_str::<serde_json::Value>(s) {
                                    parsed = Some(v);
                                    break;
                                }
                            }
                            parsed.ok_or_else(|| anyhow::anyhow!("web_search_extract tool result had no structured_content and no JSON text payload"))?
                        };
                        per_mode.push(serde_json::json!({ "selection_mode": m, "result": v }));
                    }

                    // Stable, tiny comparison: overlap of top_chunks by (url,start,end).
                    let comparison = if per_mode.len() >= 2 {
                        let mut sigs: Vec<std::collections::BTreeSet<(String, usize, usize)>> =
                            Vec::new();
                        for pm in &per_mode {
                            let mut s = std::collections::BTreeSet::new();
                            if let Some(chunks) = pm["result"]["top_chunks"].as_array() {
                                for c in chunks {
                                    let url = c
                                        .get("url")
                                        .and_then(|x| x.as_str())
                                        .unwrap_or("")
                                        .to_string();
                                    let sc =
                                        c.get("start_char").and_then(|x| x.as_u64()).unwrap_or(0)
                                            as usize;
                                    let ec = c.get("end_char").and_then(|x| x.as_u64()).unwrap_or(0)
                                        as usize;
                                    if !url.is_empty() {
                                        s.insert((url, sc, ec));
                                    }
                                }
                            }
                            sigs.push(s);
                        }
                        let a = &sigs[0];
                        let b = &sigs[1];
                        let overlap = a.intersection(b).count();
                        serde_json::json!({
                            "modes": [per_mode[0]["selection_mode"].clone(), per_mode[1]["selection_mode"].clone()],
                            "top_chunks_overlap_count": overlap,
                            "top_chunks_count_a": a.len(),
                            "top_chunks_count_b": b.len()
                        })
                    } else {
                        serde_json::Value::Null
                    };

                    runs.push(serde_json::json!({
                        "mode": "urls",
                        "query_id": qid,
                        "query": q0,
                        "urls_count": urls.len(),
                        "per_selection_mode": per_mode,
                        "comparison": comparison
                    }));
                }
            } else {
                for qi in &query_items {
                    let q = qi
                        .get("query")
                        .and_then(|x| x.as_str())
                        .unwrap_or("")
                        .to_string();
                    let qid = qi
                        .get("query_id")
                        .and_then(|x| x.as_str())
                        .map(|s| s.to_string());
                    let mut per_mode: Vec<serde_json::Value> = Vec::new();
                    for m in &selection_modes {
                        let r = svc
                            .web_search_extract(Parameters(Some(mcp::WebSearchExtractArgs {
                                query: Some(q.clone()),
                                urls: None,
                                provider: Some(args.provider.clone()),
                                auto_mode: Some(args.auto_mode.clone()),
                                selection_mode: Some(m.clone()),
                                fetch_backend: Some(args.fetch_backend.clone()),
                                no_network: Some(false),
                                firecrawl_fallback_on_empty_extraction: Some(
                                    args.firecrawl_fallback_on_empty_extraction,
                                ),
                                firecrawl_fallback_on_low_signal: Some(false),
                                render_fallback_on_empty_extraction: Some(false),
                                render_fallback_on_low_signal: Some(false),
                                max_results: args.max_results,
                                max_urls: args.max_urls,
                                timeout_ms: Some(args.timeout_ms),
                                max_bytes: Some(args.max_bytes),
                                retry_on_truncation: Some(args.retry_on_truncation),
                                truncation_retry_max_bytes: args.truncation_retry_max_bytes,
                                width: Some(args.width),
                                max_chars: args.max_chars,
                                top_chunks: args.top_chunks,
                                max_chunk_chars: args.max_chunk_chars,
                                include_links: Some(args.include_links),
                                include_structure: None,
                                max_outline_items: None,
                                max_blocks: None,
                                max_block_chars: None,
                                semantic_rerank: None,
                                semantic_top_k: None,
                                max_links: Some(args.max_links),
                                include_text: Some(args.include_text),
                                cache_read: Some(true),
                                cache_write: Some(true),
                                cache_ttl_s: None,
                                url_selection_mode: Some(args.url_selection_mode.clone()),
                                exploration: Some(args.exploration.clone()),
                                agentic: Some(args.agentic),
                                agentic_selector: Some(args.agentic_selector.clone()),
                                agentic_max_search_rounds: args.agentic_max_search_rounds,
                                agentic_frontier_max: args.agentic_frontier_max,
                                planner_max_calls: args.planner_max_calls,
                                compact: None,
                                ..Default::default()
                            })))
                            .await
                            .map_err(|e| anyhow::anyhow!(e.to_string()))?;

                        let v: serde_json::Value = if let Some(sc) = r.structured_content.clone() {
                            sc
                        } else {
                            // Back-compat: older tool results echoed JSON as the first `content` item.
                            // Newer tools may put Markdown first; scan for any parseable JSON text.
                            let mut parsed: Option<serde_json::Value> = None;
                            for c in &r.content {
                                let s = c.as_text().map(|t| t.text.as_str()).unwrap_or("").trim();
                                if s.is_empty() {
                                    continue;
                                }
                                if let Ok(v) = serde_json::from_str::<serde_json::Value>(s) {
                                    parsed = Some(v);
                                    break;
                                }
                            }
                            parsed.ok_or_else(|| anyhow::anyhow!("web_search_extract tool result had no structured_content and no JSON text payload"))?
                        };
                        per_mode.push(serde_json::json!({ "selection_mode": m, "result": v }));
                    }

                    let comparison = if per_mode.len() >= 2 {
                        let mut sigs: Vec<std::collections::BTreeSet<(String, usize, usize)>> =
                            Vec::new();
                        for pm in &per_mode {
                            let mut s = std::collections::BTreeSet::new();
                            if let Some(chunks) = pm["result"]["top_chunks"].as_array() {
                                for c in chunks {
                                    let url = c
                                        .get("url")
                                        .and_then(|x| x.as_str())
                                        .unwrap_or("")
                                        .to_string();
                                    let sc =
                                        c.get("start_char").and_then(|x| x.as_u64()).unwrap_or(0)
                                            as usize;
                                    let ec = c.get("end_char").and_then(|x| x.as_u64()).unwrap_or(0)
                                        as usize;
                                    if !url.is_empty() {
                                        s.insert((url, sc, ec));
                                    }
                                }
                            }
                            sigs.push(s);
                        }
                        let a = &sigs[0];
                        let b = &sigs[1];
                        let overlap = a.intersection(b).count();
                        serde_json::json!({
                            "modes": [per_mode[0]["selection_mode"].clone(), per_mode[1]["selection_mode"].clone()],
                            "top_chunks_overlap_count": overlap,
                            "top_chunks_count_a": a.len(),
                            "top_chunks_count_b": b.len()
                        })
                    } else {
                        serde_json::Value::Null
                    };

                    runs.push(serde_json::json!({
                        "mode": "query",
                        "query_id": qid,
                        "query": q,
                        "per_selection_mode": per_mode,
                        "comparison": comparison
                    }));
                }
            }

            let payload = serde_json::json!({
                "schema_version": 1,
                "kind": "eval_search_extract",
                "generated_at_epoch_s": now,
                "inputs": {
                    "provider": args.provider,
                    "auto_mode": args.auto_mode,
                    "selection_mode": args.selection_mode,
                    "compare_selection_modes": args.compare_selection_modes,
                    "fetch_backend": args.fetch_backend,
                    "firecrawl_fallback_on_empty_extraction": args.firecrawl_fallback_on_empty_extraction,
                    "url_selection_mode": args.url_selection_mode,
                    "max_results": args.max_results,
                    "max_urls": args.max_urls,
                    "timeout_ms": args.timeout_ms,
                    "max_bytes": args.max_bytes,
                    "width": args.width,
                    "max_chars": args.max_chars,
                    "top_chunks": args.top_chunks,
                    "max_chunk_chars": args.max_chunk_chars,
                    "include_links": args.include_links,
                    "max_links": args.max_links,
                    "include_text": args.include_text,
                    "query_count": query_count,
                    "url_count": url_count
                },
                "runs": runs
            });

            std::fs::write(&out, serde_json::to_string_pretty(&payload)? + "\n")?;
            println!("{}", out.display());
        }
        #[cfg(all(feature = "eval", feature = "stdio"))]
        Commands::EvalMatrix(args) => {
            use rmcp::handler::server::wrapper::Parameters;

            let now = args.now_epoch_s.unwrap_or_else(|| {
                std::time::SystemTime::now()
                    .duration_since(std::time::UNIX_EPOCH)
                    .unwrap_or_default()
                    .as_secs()
            });
            let out = args.out.unwrap_or_else(|| {
                std::path::PathBuf::from(format!(".generated/webpipe-eval-matrix-{now}.jsonl"))
            });
            std::fs::create_dir_all(
                out.parent()
                    .unwrap_or_else(|| std::path::Path::new(".generated")),
            )?;

            let base_url = args.base_url.trim_end_matches('/').to_string();
            let e2e = eval::load_e2e_queries_v1(&args.queries_json)?;

            let svc = mcp::WebpipeMcp::new().map_err(|e| anyhow::anyhow!(e.to_string()))?;

            let mut f = std::io::BufWriter::new(std::fs::File::create(&out)?);

            let max_results = args.max_results.clamp(1, 20);
            let max_urls = args.max_urls.clamp(1, 10);
            let top_chunks = args.top_chunks.clamp(1, 50);
            let max_chunk_chars = args.max_chunk_chars.clamp(20, 5_000);

            let call_json = |r: rmcp::model::CallToolResult| -> Result<serde_json::Value> {
                if let Some(v) = r.structured_content {
                    return Ok(v);
                }
                // Back-compat: older tool results echoed JSON as the first `content` text item.
                // Newer tools may include Markdown first, so scan for any parseable JSON text.
                for c in &r.content {
                    let s = c.as_text().map(|t| t.text.as_str()).unwrap_or("").trim();
                    if s.is_empty() {
                        continue;
                    }
                    if let Ok(v) = serde_json::from_str::<serde_json::Value>(s) {
                        return Ok(v);
                    }
                }
                anyhow::bail!("tool result had no structured_content and no JSON text payload");
            };

            for q in e2e.queries {
                let urls: Vec<String> = q
                    .url_paths
                    .iter()
                    .map(|p| {
                        let p = p.trim();
                        if p.starts_with("http://") || p.starts_with("https://") {
                            p.to_string()
                        } else {
                            format!("{base_url}/{}", p.trim_start_matches('/'))
                        }
                    })
                    .collect();

                // Case A: search leg (uses configured providers/endpoints via env)
                let t0 = std::time::Instant::now();
                let res_a = svc
                    .web_search_extract(Parameters(Some(mcp::WebSearchExtractArgs {
                        query: Some(q.query.clone()),
                        urls: None,
                        provider: Some(args.provider.clone()),
                        auto_mode: Some(args.auto_mode.clone()),
                        selection_mode: Some(args.selection_mode.clone()),
                        fetch_backend: Some(args.fetch_backend.clone()),
                        no_network: Some(false),
                        max_results: Some(max_results),
                        max_urls: Some(max_urls),
                        timeout_ms: Some(args.timeout_ms),
                        max_bytes: Some(args.max_bytes),
                        top_chunks: Some(top_chunks),
                        max_chunk_chars: Some(max_chunk_chars),
                        include_links: Some(false),
                        include_text: Some(false),
                        agentic: Some(false),
                        url_selection_mode: Some("preserve".to_string()),
                        cache_read: Some(true),
                        cache_write: Some(true),
                        ..Default::default()
                    })))
                    .await
                    .map_err(|e| anyhow::anyhow!(e.to_string()))?;

                let row_a = serde_json::json!({
                    "schema_version": 1,
                    "kind": "webpipe_eval_matrix_row",
                    "generated_at_epoch_s": now,
                    "case": "search",
                    "query_id": q.query_id,
                    "query": q.query,
                    "tags": q.tags,
                    "urls": urls,
                    "elapsed_ms": t0.elapsed().as_millis(),
                    "result": call_json(res_a)?
                });
                use std::io::Write;
                writeln!(f, "{}", serde_json::to_string(&row_a)?)?;

                // Case B: warm cache via urls-mode (bounded)
                let t1 = std::time::Instant::now();
                let res_b = svc
                    .web_search_extract(Parameters(Some(mcp::WebSearchExtractArgs {
                        query: row_a
                            .get("query")
                            .and_then(|v| v.as_str())
                            .map(|s| s.to_string()),
                        urls: Some(
                            row_a
                                .get("urls")
                                .and_then(|v| v.as_array())
                                .into_iter()
                                .flatten()
                                .filter_map(|v| v.as_str().map(|s| s.to_string()))
                                .collect(),
                        ),
                        url_selection_mode: Some("preserve".to_string()),
                        provider: Some("auto".to_string()),
                        auto_mode: Some("fallback".to_string()),
                        selection_mode: Some(args.selection_mode.clone()),
                        fetch_backend: Some(args.fetch_backend.clone()),
                        no_network: Some(false),
                        max_urls: Some(max_urls),
                        timeout_ms: Some(args.timeout_ms),
                        max_bytes: Some(args.max_bytes),
                        top_chunks: Some(top_chunks),
                        max_chunk_chars: Some(max_chunk_chars),
                        include_links: Some(false),
                        include_text: Some(false),
                        agentic: Some(false),
                        cache_read: Some(true),
                        cache_write: Some(true),
                        ..Default::default()
                    })))
                    .await
                    .map_err(|e| anyhow::anyhow!(e.to_string()))?;
                let row_b = serde_json::json!({
                    "schema_version": 1,
                    "kind": "webpipe_eval_matrix_row",
                    "generated_at_epoch_s": now,
                    "case": "warm_urls",
                    "query_id": row_a["query_id"].clone(),
                    "query": row_a["query"].clone(),
                    "tags": row_a["tags"].clone(),
                    "urls": row_a["urls"].clone(),
                    "elapsed_ms": t1.elapsed().as_millis(),
                    "result": call_json(res_b)?
                });
                writeln!(f, "{}", serde_json::to_string(&row_b)?)?;

                // Case C: offline replay (urls-mode + no_network=true)
                let t2 = std::time::Instant::now();
                let res_c = svc
                    .web_search_extract(Parameters(Some(mcp::WebSearchExtractArgs {
                        query: row_a
                            .get("query")
                            .and_then(|v| v.as_str())
                            .map(|s| s.to_string()),
                        urls: Some(
                            row_a
                                .get("urls")
                                .and_then(|v| v.as_array())
                                .into_iter()
                                .flatten()
                                .filter_map(|v| v.as_str().map(|s| s.to_string()))
                                .collect(),
                        ),
                        url_selection_mode: Some("preserve".to_string()),
                        provider: Some("auto".to_string()),
                        auto_mode: Some("fallback".to_string()),
                        selection_mode: Some(args.selection_mode.clone()),
                        fetch_backend: Some(args.fetch_backend.clone()),
                        no_network: Some(true),
                        max_urls: Some(max_urls),
                        timeout_ms: Some(args.timeout_ms),
                        max_bytes: Some(args.max_bytes),
                        top_chunks: Some(top_chunks),
                        max_chunk_chars: Some(max_chunk_chars),
                        include_links: Some(false),
                        include_text: Some(false),
                        agentic: Some(false),
                        cache_read: Some(true),
                        cache_write: Some(false),
                        ..Default::default()
                    })))
                    .await
                    .map_err(|e| anyhow::anyhow!(e.to_string()))?;
                let row_c = serde_json::json!({
                    "schema_version": 1,
                    "kind": "webpipe_eval_matrix_row",
                    "generated_at_epoch_s": now,
                    "case": "offline_urls",
                    "query_id": row_a["query_id"].clone(),
                    "query": row_a["query"].clone(),
                    "tags": row_a["tags"].clone(),
                    "urls": row_a["urls"].clone(),
                    "elapsed_ms": t2.elapsed().as_millis(),
                    "result": call_json(res_c)?
                });
                writeln!(f, "{}", serde_json::to_string(&row_c)?)?;
            }

            println!("{}", out.display());
        }
        #[cfg(feature = "eval")]
        Commands::EvalMatrixScore(args) => {
            let now = args.now_epoch_s.unwrap_or_else(|| {
                std::time::SystemTime::now()
                    .duration_since(std::time::UNIX_EPOCH)
                    .unwrap_or_default()
                    .as_secs()
            });
            let out = args.out.unwrap_or_else(|| {
                std::path::PathBuf::from(format!(".generated/webpipe-eval-matrix-score-{now}.json"))
            });
            std::fs::create_dir_all(
                out.parent()
                    .unwrap_or_else(|| std::path::Path::new(".generated")),
            )?;

            let qrels = eval::load_e2e_qrels_v1(&args.qrels)?;

            // Read + parse JSONL rows.
            let raw = std::fs::read_to_string(&args.matrix_artifact)?;
            let mut rows: Vec<serde_json::Value> = Vec::new();
            for (i, line) in raw.lines().enumerate() {
                let s = line.trim();
                if s.is_empty() {
                    continue;
                }
                let v: serde_json::Value = serde_json::from_str(s).map_err(|e| {
                    anyhow::anyhow!("matrix_artifact line {}: invalid json: {}", i + 1, e)
                })?;
                rows.push(v);
            }

            // Index: query_id -> case -> row
            let mut by_q: std::collections::BTreeMap<
                String,
                std::collections::BTreeMap<String, serde_json::Value>,
            > = std::collections::BTreeMap::new();
            for r in rows {
                let qid = r
                    .get("query_id")
                    .and_then(|v| v.as_str())
                    .unwrap_or("")
                    .to_string();
                let case = r
                    .get("case")
                    .and_then(|v| v.as_str())
                    .unwrap_or("")
                    .to_string();
                if qid.is_empty() || case.is_empty() {
                    continue;
                }
                by_q.entry(qid).or_default().insert(case, r);
            }

            fn extract_urls_from_result(
                res: &serde_json::Value,
            ) -> std::collections::BTreeSet<String> {
                let mut out = std::collections::BTreeSet::new();
                if let Some(top) = res.get("top_chunks").and_then(|v| v.as_array()) {
                    for c in top {
                        if let Some(u) = c.get("url").and_then(|v| v.as_str()) {
                            out.insert(u.to_string());
                        }
                    }
                }
                if let Some(per_url) = res.get("results").and_then(|v| v.as_array()) {
                    for r in per_url {
                        if let Some(u) = r.get("url").and_then(|v| v.as_str()) {
                            out.insert(u.to_string());
                        }
                        if let Some(u) = r.get("final_url").and_then(|v| v.as_str()) {
                            out.insert(u.to_string());
                        }
                    }
                }
                out
            }

            let expected_cases = ["search", "warm_urls", "offline_urls"];
            let mut per_query: Vec<serde_json::Value> = Vec::new();
            let mut totals = serde_json::json!({
                "queries": qrels.qrels.len(),
                "cases": { "search": {"ok": 0, "hit": 0}, "warm_urls": {"ok": 0, "hit": 0}, "offline_urls": {"ok": 0, "hit": 0} }
            });

            for q in &qrels.qrels {
                let mut per_case = Vec::new();
                for case in expected_cases {
                    let row = by_q.get(&q.query_id).and_then(|m| m.get(case)).cloned();

                    if let Some(row) = row {
                        let res = row
                            .get("result")
                            .cloned()
                            .unwrap_or(serde_json::Value::Null);
                        let ok = res.get("ok").and_then(|v| v.as_bool()).unwrap_or(false);
                        let urls = extract_urls_from_result(&res);
                        let hit = ok
                            && q.expected_url_substrings.iter().any(|needle| {
                                let needle = needle.as_str();
                                urls.iter().any(|u| u.contains(needle))
                            });

                        if ok {
                            totals["cases"][case]["ok"] = serde_json::json!(
                                totals["cases"][case]["ok"].as_u64().unwrap_or(0) + 1
                            );
                        }
                        if hit {
                            totals["cases"][case]["hit"] = serde_json::json!(
                                totals["cases"][case]["hit"].as_u64().unwrap_or(0) + 1
                            );
                        }

                        per_case.push(serde_json::json!({
                            "case": case,
                            "row_present": true,
                            "ok": ok,
                            "hit": hit,
                            "expected_url_substrings": q.expected_url_substrings,
                            "observed_url_count": urls.len(),
                        }));
                    } else {
                        per_case.push(serde_json::json!({
                            "case": case,
                            "row_present": false,
                            "ok": false,
                            "hit": false,
                            "expected_url_substrings": q.expected_url_substrings,
                            "observed_url_count": 0,
                        }));
                    }
                }
                per_query.push(serde_json::json!({
                    "query_id": q.query_id,
                    "cases": per_case
                }));
            }

            let payload = serde_json::json!({
                "schema_version": 1,
                "kind": "webpipe_eval_matrix_score",
                "generated_at_epoch_s": now,
                "inputs": {
                    "matrix_artifact": args.matrix_artifact,
                    "qrels": args.qrels
                },
                "totals": totals,
                "per_query": per_query
            });

            std::fs::write(&out, serde_json::to_string_pretty(&payload)? + "\n")?;
            println!("{}", out.display());
        }
        #[cfg(feature = "eval")]
        Commands::EvalMatrixExport(args) => {
            let now = args.now_epoch_s.unwrap_or_else(|| {
                std::time::SystemTime::now()
                    .duration_since(std::time::UNIX_EPOCH)
                    .unwrap_or_default()
                    .as_secs()
            });
            let out = args.out.unwrap_or_else(|| {
                std::path::PathBuf::from(format!(
                    ".generated/webpipe-eval-matrix-export-{now}.jsonl"
                ))
            });
            std::fs::create_dir_all(
                out.parent()
                    .unwrap_or_else(|| std::path::Path::new(".generated")),
            )?;

            let qrels = if let Some(p) = args.qrels.as_ref() {
                Some(eval::load_e2e_qrels_v1(p)?)
            } else {
                None
            };
            let mut qrels_by_qid: std::collections::BTreeMap<String, Vec<String>> =
                std::collections::BTreeMap::new();
            if let Some(ref qrels) = qrels {
                for q in &qrels.qrels {
                    qrels_by_qid.insert(q.query_id.clone(), q.expected_url_substrings.clone());
                }
            }

            fn truncate_chars(s: &str, max_chars: usize) -> (String, bool) {
                if max_chars == 0 {
                    return ("".to_string(), !s.is_empty());
                }
                let mut out = String::new();
                for (n, ch) in s.chars().enumerate() {
                    if n >= max_chars {
                        return (out, true);
                    }
                    out.push(ch);
                }
                (out, false)
            }

            fn extract_urls(res: &serde_json::Value) -> std::collections::BTreeSet<String> {
                let mut out = std::collections::BTreeSet::new();
                if let Some(top) = res.get("top_chunks").and_then(|v| v.as_array()) {
                    for c in top {
                        if let Some(u) = c.get("url").and_then(|v| v.as_str()) {
                            out.insert(u.to_string());
                        }
                    }
                }
                if let Some(per_url) = res.get("results").and_then(|v| v.as_array()) {
                    for r in per_url {
                        if let Some(u) = r.get("url").and_then(|v| v.as_str()) {
                            out.insert(u.to_string());
                        }
                        if let Some(u) = r.get("final_url").and_then(|v| v.as_str()) {
                            out.insert(u.to_string());
                        }
                    }
                }
                out
            }

            // Read + parse JSONL rows.
            let raw = std::fs::read_to_string(&args.matrix_artifact)?;
            let mut out_rows: Vec<serde_json::Value> = Vec::new();
            for (i, line) in raw.lines().enumerate() {
                let s = line.trim();
                if s.is_empty() {
                    continue;
                }
                let row: serde_json::Value = serde_json::from_str(s).map_err(|e| {
                    anyhow::anyhow!("matrix_artifact line {}: invalid json: {}", i + 1, e)
                })?;

                let query_id = row
                    .get("query_id")
                    .and_then(|v| v.as_str())
                    .unwrap_or("")
                    .to_string();
                let case = row
                    .get("case")
                    .and_then(|v| v.as_str())
                    .unwrap_or("")
                    .to_string();
                let query = row
                    .get("query")
                    .and_then(|v| v.as_str())
                    .unwrap_or("")
                    .to_string();
                if query_id.is_empty() || case.is_empty() {
                    continue;
                }

                let tags: Vec<String> = row
                    .get("tags")
                    .and_then(|v| v.as_array())
                    .map(|a| {
                        a.iter()
                            .filter_map(|v| v.as_str().map(|s| s.to_string()))
                            .collect()
                    })
                    .unwrap_or_default();

                let res = row
                    .get("result")
                    .cloned()
                    .unwrap_or(serde_json::Value::Null);
                let ok = res.get("ok").and_then(|v| v.as_bool()).unwrap_or(false);
                let backend_provider = res
                    .get("backend_provider")
                    .and_then(|v| v.as_str())
                    .map(|s| s.to_string());
                let urls = extract_urls(&res);

                let expected = qrels_by_qid.get(&query_id).cloned().unwrap_or_default();
                let matched = if ok {
                    expected
                        .iter()
                        .find(|needle| urls.iter().any(|u| u.contains(needle.as_str())))
                        .cloned()
                } else {
                    None
                };
                let hit_expected_url = matched.is_some();

                // Build a compact judge text: concatenate top_chunks.text (already bounded by tool),
                // then truncate here as a safety net.
                let mut joined = String::new();
                if let Some(top) = res.get("top_chunks").and_then(|v| v.as_array()) {
                    for (idx, c) in top.iter().enumerate() {
                        if let Some(t) = c.get("text").and_then(|v| v.as_str()) {
                            if idx > 0 {
                                joined.push('\n');
                                joined.push('\n');
                            }
                            joined.push_str(t);
                        }
                    }
                }
                let (judge_text, judge_text_truncated) =
                    truncate_chars(&joined, args.max_text_chars);

                out_rows.push(serde_json::json!({
                    "schema_version": 1,
                    "kind": "webpipe_eval_matrix_example",
                    "generated_at_epoch_s": now,
                    "query_id": query_id,
                    "case": case,
                    "query": query,
                    "tags": tags,
                    "expected_url_substrings": expected,
                    "hit_expected_url": hit_expected_url,
                    "matched_substring": matched,
                    "ok": ok,
                    "backend_provider": backend_provider,
                    "observed_url_count": urls.len(),
                    "judge_text": judge_text,
                    "judge_text_truncated": judge_text_truncated
                }));
            }

            let mut f = std::io::BufWriter::new(std::fs::File::create(&out)?);
            use std::io::Write;
            for r in out_rows {
                writeln!(f, "{}", serde_json::to_string(&r)?)?;
            }

            println!("{}", out.display());
        }
        #[cfg(feature = "eval")]
        Commands::EvalMatrixJudge(args) => {
            let now = args.now_epoch_s.unwrap_or_else(|| {
                std::time::SystemTime::now()
                    .duration_since(std::time::UNIX_EPOCH)
                    .unwrap_or_default()
                    .as_secs()
            });
            let out = args.out.unwrap_or_else(|| {
                std::path::PathBuf::from(format!(".generated/webpipe-eval-matrix-judge-{now}.json"))
            });
            std::fs::create_dir_all(
                out.parent()
                    .unwrap_or_else(|| std::path::Path::new(".generated")),
            )?;

            let raw = std::fs::read_to_string(&args.examples_artifact)?;

            fn tokenize_ascii(s: &str) -> Vec<String> {
                // Deterministic, cheap tokenization for overlap checks.
                let mut out = Vec::new();
                let mut cur = String::new();
                for ch in s.chars() {
                    let c = ch.to_ascii_lowercase();
                    if c.is_ascii_alphanumeric() {
                        cur.push(c);
                    } else {
                        if cur.len() >= 3 {
                            out.push(cur.clone());
                        }
                        cur.clear();
                    }
                }
                if cur.len() >= 3 {
                    out.push(cur);
                }
                out
            }

            fn is_low_signal_text(t: &str) -> bool {
                let s = t.to_ascii_lowercase();
                // Cheap heuristics: known failure modes for JS apps/auth walls.
                s.contains("enable javascript")
                    || s.contains("javascript required")
                    || s.contains("access denied")
                    || s.contains("sign in")
                    || s.contains("captcha")
                    || s.contains("cloudflare")
                    || s.contains("loading...")
            }

            let mut per_example = Vec::new();
            let mut totals = serde_json::json!({
                "examples": 0u64,
                "by_case": {},
                "metrics": {
                    "ok": 0u64,
                    "hit_expected_url": 0u64,
                    "nonempty_text": 0u64,
                    "query_overlap": 0u64,
                    "low_signal": 0u64
                }
            });

            for (i, line) in raw.lines().enumerate() {
                let s = line.trim();
                if s.is_empty() {
                    continue;
                }
                let ex: serde_json::Value = serde_json::from_str(s).map_err(|e| {
                    anyhow::anyhow!("examples_artifact line {}: invalid json: {}", i + 1, e)
                })?;

                let case = ex.get("case").and_then(|v| v.as_str()).unwrap_or("unknown");
                let ok = ex.get("ok").and_then(|v| v.as_bool()).unwrap_or(false);
                let hit = ex
                    .get("hit_expected_url")
                    .and_then(|v| v.as_bool())
                    .unwrap_or(false);
                let query = ex.get("query").and_then(|v| v.as_str()).unwrap_or("");
                let text = ex.get("judge_text").and_then(|v| v.as_str()).unwrap_or("");

                let nonempty_text = !text.trim().is_empty();
                let low_signal = nonempty_text && is_low_signal_text(text);

                let q_tokens = tokenize_ascii(query);
                let t_tokens = tokenize_ascii(text);
                let mut overlap = 0usize;
                if !q_tokens.is_empty() && !t_tokens.is_empty() {
                    let tset: std::collections::BTreeSet<&str> =
                        t_tokens.iter().map(|s| s.as_str()).collect();
                    for qt in &q_tokens {
                        if tset.contains(qt.as_str()) {
                            overlap += 1;
                        }
                    }
                }
                let query_overlap = overlap >= 1;

                // Baseline pass/fail: must be ok + not low-signal + (hit OR overlap).
                let pass = ok && nonempty_text && !low_signal && (hit || query_overlap);

                totals["examples"] =
                    serde_json::json!(totals["examples"].as_u64().unwrap_or(0) + 1);
                if ok {
                    totals["metrics"]["ok"] =
                        serde_json::json!(totals["metrics"]["ok"].as_u64().unwrap_or(0) + 1);
                }
                if hit {
                    totals["metrics"]["hit_expected_url"] = serde_json::json!(
                        totals["metrics"]["hit_expected_url"].as_u64().unwrap_or(0) + 1
                    );
                }
                if nonempty_text {
                    totals["metrics"]["nonempty_text"] = serde_json::json!(
                        totals["metrics"]["nonempty_text"].as_u64().unwrap_or(0) + 1
                    );
                }
                if query_overlap {
                    totals["metrics"]["query_overlap"] = serde_json::json!(
                        totals["metrics"]["query_overlap"].as_u64().unwrap_or(0) + 1
                    );
                }
                if low_signal {
                    totals["metrics"]["low_signal"] = serde_json::json!(
                        totals["metrics"]["low_signal"].as_u64().unwrap_or(0) + 1
                    );
                }

                // Per-case counters
                if totals["by_case"][case].is_null() {
                    totals["by_case"][case] = serde_json::json!({
                        "examples": 0u64,
                        "pass": 0u64
                    });
                }
                totals["by_case"][case]["examples"] = serde_json::json!(
                    totals["by_case"][case]["examples"].as_u64().unwrap_or(0) + 1
                );
                if pass {
                    totals["by_case"][case]["pass"] = serde_json::json!(
                        totals["by_case"][case]["pass"].as_u64().unwrap_or(0) + 1
                    );
                }

                per_example.push(serde_json::json!({
                    "query_id": ex.get("query_id").cloned().unwrap_or(serde_json::Value::Null),
                    "case": case,
                    "ok": ok,
                    "hit_expected_url": hit,
                    "nonempty_text": nonempty_text,
                    "low_signal": low_signal,
                    "query_overlap": query_overlap,
                    "pass": pass
                }));
            }

            let payload = serde_json::json!({
                "schema_version": 1,
                "kind": "webpipe_eval_matrix_judge",
                "generated_at_epoch_s": now,
                "inputs": {
                    "examples_artifact": args.examples_artifact
                },
                "totals": totals,
                "per_example": per_example
            });

            std::fs::write(&out, serde_json::to_string_pretty(&payload)? + "\n")?;
            println!("{}", out.display());
        }
        #[cfg(feature = "eval")]
        Commands::EvalMatrixLlmJudge(args) => {
            let now = args.now_epoch_s.unwrap_or_else(|| {
                std::time::SystemTime::now()
                    .duration_since(std::time::UNIX_EPOCH)
                    .unwrap_or_default()
                    .as_secs()
            });
            let out = args.out.unwrap_or_else(|| {
                std::path::PathBuf::from(format!(
                    ".generated/webpipe-eval-matrix-llm-judge-{now}.json"
                ))
            });
            std::fs::create_dir_all(
                out.parent()
                    .unwrap_or_else(|| std::path::Path::new(".generated")),
            )?;

            let transcript_enabled = args.transcript;
            let transcript_max_chars = args.transcript_max_chars.clamp(200, 200_000);
            let transcript_path = args.transcript_jsonl.clone().unwrap_or_else(|| {
                std::path::PathBuf::from(format!("{}.transcript.jsonl", out.display()))
            });
            let mut transcript_seq: u64 = 0;

            fn append_jsonl(
                path: &std::path::Path,
                line: &serde_json::Value,
            ) -> anyhow::Result<()> {
                if let Some(p) = path.parent() {
                    std::fs::create_dir_all(p)?;
                }
                use std::io::Write;
                let mut f = std::fs::OpenOptions::new()
                    .create(true)
                    .append(true)
                    .open(path)?;
                writeln!(f, "{}", serde_json::to_string(line)?)?;
                Ok(())
            }

            let raw = std::fs::read_to_string(&args.examples_artifact)?;

            fn take_chars(s: &str, max_chars: usize) -> (String, bool) {
                if max_chars == 0 {
                    return (String::new(), !s.is_empty());
                }
                let mut out = String::new();
                for (n, ch) in s.chars().enumerate() {
                    if n >= max_chars {
                        return (out, true);
                    }
                    out.push(ch);
                }
                (out, false)
            }

            fn extract_json_object(s: &str) -> Option<serde_json::Value> {
                // Prefer parsing the full response first (common case).
                let trimmed = s.trim();
                if let Ok(v) = serde_json::from_str::<serde_json::Value>(trimmed) {
                    return Some(v);
                }

                // Fallback: pull out the outermost {...} region.
                //
                // IMPORTANT: offsets/spans for user text in this workspace are char-based, and
                // `&str` slicing by byte offsets can panic on Unicode. So we operate in char
                // offsets and materialize a `String` slice via `.chars()`.
                let mut start: Option<usize> = None;
                let mut end: Option<usize> = None;
                for (i, ch) in trimmed.chars().enumerate() {
                    if start.is_none() && ch == '{' {
                        start = Some(i);
                    }
                    if ch == '}' {
                        end = Some(i);
                    }
                }
                let (start, end) = match (start, end) {
                    (Some(a), Some(b)) if b >= a => (a, b),
                    _ => return None,
                };

                let slice: String = trimmed
                    .chars()
                    .skip(start)
                    .take(end.saturating_sub(start) + 1)
                    .collect();
                serde_json::from_str(&slice).ok()
            }

            fn tx_trunc(s: &str, max_chars: usize) -> serde_json::Value {
                let (t, truncated) = take_chars(s, max_chars);
                serde_json::json!({
                    "text": t,
                    "chars": s.chars().count(),
                    "truncated": truncated
                })
            }

            let llm_backend = args.llm_backend.to_ascii_lowercase();
            let temperature = args.temperature.or(Some(0.0));
            let top_p = args.top_p.or(Some(1.0));
            let max_examples = args.max_examples.clamp(1, 10_000);
            let timeout_ms = args.timeout_ms.clamp(500, 300_000);
            let max_prompt_chars = args.max_prompt_chars.clamp(200, 50_000);
            let json_mode = args.json_mode;
            let retry_on_parse_fail = args.retry_on_parse_fail;

            let http = reqwest::Client::new();
            let openai_like = match llm_backend.as_str() {
                "openai" => {
                    let api_key = mcp::WebpipeMcp::openai_api_key_from_env().ok_or_else(|| {
                        anyhow::anyhow!("missing OPENAI_API_KEY (or WEBPIPE_OPENAI_API_KEY)")
                    })?;
                    let model = args
                        .llm_model
                        .clone()
                        .unwrap_or_else(mcp::WebpipeMcp::openai_model_from_env);
                    Some(webpipe_local::openai_compat::OpenAiCompatClient::new(
                        http.clone(),
                        "https://api.openai.com".to_string(),
                        Some(api_key),
                        model,
                    )?)
                }
                "openai_compat" => {
                    Some(webpipe_local::openai_compat::OpenAiCompatClient::from_env(
                        http.clone(),
                        args.llm_model.clone(),
                    )?)
                }
                "openrouter" => {
                    let api_key =
                        mcp::WebpipeMcp::openrouter_api_key_from_env().ok_or_else(|| {
                            anyhow::anyhow!(
                                "missing OPENROUTER_API_KEY (or WEBPIPE_OPENROUTER_API_KEY)"
                            )
                        })?;
                    let model = args
                        .llm_model
                        .clone()
                        .unwrap_or_else(mcp::WebpipeMcp::openrouter_model_from_env);
                    Some(webpipe_local::openai_compat::OpenAiCompatClient::new(
                        http.clone(),
                        // OpenAiCompatClient appends `/v1/chat/completions`.
                        "https://openrouter.ai/api".to_string(),
                        Some(api_key),
                        model,
                    )?)
                }
                "groq" => {
                    let api_key = mcp::WebpipeMcp::groq_api_key_from_env().ok_or_else(|| {
                        anyhow::anyhow!("missing GROQ_API_KEY (or WEBPIPE_GROQ_API_KEY)")
                    })?;
                    let model = args
                        .llm_model
                        .clone()
                        .unwrap_or_else(mcp::WebpipeMcp::groq_model_from_env);
                    Some(webpipe_local::openai_compat::OpenAiCompatClient::new(
                        http.clone(),
                        // OpenAiCompatClient appends `/v1/chat/completions`.
                        "https://api.groq.com/openai".to_string(),
                        Some(api_key),
                        model,
                    )?)
                }
                _ => None,
            };
            let ollama = if llm_backend == "ollama" {
                Some(webpipe_local::ollama::OllamaClient::from_env(http.clone())?)
            } else {
                None
            };
            let llm_model_effective: Option<String> =
                openai_like.as_ref().map(|c| c.model().to_string());

            #[derive(serde::Deserialize)]
            struct OpenRouterModels {
                #[serde(default)]
                data: Vec<OpenRouterModel>,
            }

            #[derive(serde::Deserialize)]
            struct OpenRouterModel {
                id: String,
                #[serde(default)]
                supported_parameters: Vec<String>,
            }

            async fn openrouter_supported_params(
                http: &reqwest::Client,
                model_id: &str,
            ) -> Option<std::collections::BTreeSet<String>> {
                if model_id.trim().is_empty() {
                    return None;
                }
                let resp = http
                    .get("https://openrouter.ai/api/v1/models")
                    .timeout(std::time::Duration::from_millis(8_000))
                    .send()
                    .await
                    .ok()?;
                if !resp.status().is_success() {
                    return None;
                }
                let parsed: OpenRouterModels = resp.json().await.ok()?;
                let m = parsed.data.into_iter().find(|m| m.id == model_id)?;
                Some(m.supported_parameters.into_iter().collect())
            }

            let openrouter_caps: Option<std::collections::BTreeSet<String>> =
                if llm_backend == "openrouter" {
                    if let Some(mid) = llm_model_effective.as_deref() {
                        openrouter_supported_params(&http, mid).await
                    } else {
                        None
                    }
                } else {
                    None
                };

            let mut chat_options = webpipe_local::openai_compat::ChatOptions::default();
            if llm_backend == "openrouter" {
                if let Ok(v) = std::env::var("WEBPIPE_OPENROUTER_INCLUDE_REASONING") {
                    let v = v.trim().to_ascii_lowercase();
                    if (v == "true" || v == "1")
                        && openrouter_caps
                            .as_ref()
                            .map(|s| s.contains("include_reasoning"))
                            .unwrap_or(true)
                    {
                        chat_options.include_reasoning = Some(true);
                    }
                    if (v == "false" || v == "0")
                        && openrouter_caps
                            .as_ref()
                            .map(|s| s.contains("include_reasoning"))
                            .unwrap_or(true)
                    {
                        chat_options.include_reasoning = Some(false);
                    }
                }
                if let Ok(eff) = std::env::var("WEBPIPE_OPENROUTER_REASONING_EFFORT") {
                    let eff = eff.trim().to_string();
                    if !eff.is_empty()
                        && openrouter_caps
                            .as_ref()
                            .map(|s| s.contains("reasoning"))
                            .unwrap_or(true)
                    {
                        chat_options.reasoning_effort = Some(eff);
                    }
                }
            }

            let system: String = if let Some(p) = args.system_prompt_file.as_ref() {
                std::fs::read_to_string(p)?
            } else {
                match args.prompt_preset.to_ascii_lowercase().as_str() {
                    "v1" => r#"You are a strict judge of retrieval/extraction quality.

You will receive:
- a user query
- extracted evidence text (possibly noisy or incomplete)

Return a single JSON object ONLY, with this schema:
{
  "relevant": boolean,
  "answerable": boolean,
  "confidence": number,              // 0..1
  "issues": string[],                // subset of: ["empty","low_signal","off_topic","too_short","boilerplate","gunk","missing_key_facts"]
  "notes": string,                   // <= 200 chars
  "quotes": string[]                 // up to 2 short quotes from the evidence (<= 200 chars each)
}

Be conservative: mark answerable=false if the evidence is not sufficient."#
                        .to_string(),
                    _ => r#"You are a strict judge of retrieval/extraction quality.

Goal: detect tail failures (JS challenges, app-shell gunk, boilerplate, truncation, off-topic evidence).

Input:
- Query: a user question
- Evidence: extracted text (may be noisy/incomplete)

Output: return ONE JSON object ONLY. No markdown, no prose, no code fences.
Schema:
{
  "relevant": boolean,
  "answerable": boolean,
  "confidence": number,              // 0..1
  "issues": string[],                // subset of: ["empty","low_signal","off_topic","too_short","boilerplate","gunk","missing_key_facts","truncated","js_challenge"]
  "notes": string,                   // <= 200 chars
  "quotes": string[]                 // up to 2 short direct quotes (<= 200 chars each)
}

Conservatism:
- If evidence is mostly navigation/boilerplate, set answerable=false and issues include "boilerplate".
- If evidence looks like a JS/CAPTCHA/auth wall, set relevant=false, answerable=false, issues include "js_challenge".
- If evidence is clearly minified/serialized bundle junk, issues include "gunk".
- If query terms are absent and the text is about something else, issues include "off_topic"."#
                        .to_string(),
                }
            };

            let mut per_example: Vec<serde_json::Value> = Vec::new();
            let mut totals = serde_json::json!({
                "examples": 0u64,
                "ok": 0u64,
                "relevant": 0u64,
                "answerable": 0u64,
                "parse_failed": 0u64
            });

            for (i, line) in raw.lines().enumerate() {
                if per_example.len() >= max_examples {
                    break;
                }
                let s = line.trim();
                if s.is_empty() {
                    continue;
                }
                let ex: serde_json::Value = serde_json::from_str(s).map_err(|e| {
                    anyhow::anyhow!("examples_artifact line {}: invalid json: {}", i + 1, e)
                })?;

                let query_id = ex
                    .get("query_id")
                    .cloned()
                    .unwrap_or(serde_json::Value::Null);
                let case = ex.get("case").and_then(|v| v.as_str()).unwrap_or("unknown");
                let ok = ex.get("ok").and_then(|v| v.as_bool()).unwrap_or(false);
                let query = ex.get("query").and_then(|v| v.as_str()).unwrap_or("");
                let text0 = ex.get("judge_text").and_then(|v| v.as_str()).unwrap_or("");
                let (text, prompt_truncated) = take_chars(text0, max_prompt_chars);

                let user = format!("Query:\n{}\n\nEvidence:\n{}\n", query.trim(), text.trim());

                let t0 = std::time::Instant::now();
                let call_id = format!(
                    "eval_matrix_llm_judge:{}:{}:{}",
                    now,
                    per_example.len(),
                    ex.get("query_id").and_then(|x| x.as_str()).unwrap_or("")
                );
                let mut resp_text = match llm_backend.as_str() {
                    "ollama" => {
                        let client = ollama.as_ref().expect("ollama client");
                        client
                            .chat(&system, &user, timeout_ms)
                            .await
                            .map_err(|e| anyhow::anyhow!(e.to_string()))?
                    }
                    "openai" | "openai_compat" | "openrouter" | "groq" => {
                        let client = openai_like.as_ref().expect("openai-like client");
                        let wants_json = json_mode;
                        let json_allowed = llm_backend.as_str() != "openrouter"
                            || openrouter_caps
                                .as_ref()
                                .map(|s| s.contains("response_format"))
                                .unwrap_or(true);
                        if wants_json && json_allowed {
                            match client
                                .chat_json_with_options(
                                    &system,
                                    &user,
                                    timeout_ms,
                                    args.max_tokens.or(Some(250)),
                                    temperature,
                                    top_p,
                                    chat_options.clone(),
                                )
                                .await
                            {
                                Ok(s) => s,
                                Err(_) if llm_backend.as_str() == "openrouter" => client
                                    .chat_with_options(
                                        &system,
                                        &user,
                                        timeout_ms,
                                        args.max_tokens.or(Some(250)),
                                        temperature,
                                        top_p,
                                        chat_options.clone(),
                                    )
                                    .await
                                    .map_err(|e| anyhow::anyhow!(e.to_string()))?,
                                Err(e) => return Err(anyhow::anyhow!(e.to_string())),
                            }
                        } else {
                            client
                                .chat_with_options(
                                    &system,
                                    &user,
                                    timeout_ms,
                                    args.max_tokens.or(Some(250)),
                                    temperature,
                                    top_p,
                                    chat_options.clone(),
                                )
                                .await
                                .map_err(|e| anyhow::anyhow!(e.to_string()))?
                        }
                    }
                    other => anyhow::bail!(
                        "unknown llm_backend: {other} (allowed: openai, openai_compat, openrouter, groq, ollama)"
                    ),
                };
                let elapsed_ms = t0.elapsed().as_millis();

                let mut parsed = extract_json_object(&resp_text);
                if parsed.is_none() && retry_on_parse_fail && openai_like.is_some() {
                    let (prev, _) = take_chars(&resp_text, 800);
                    let retry_user = format!(
                        "Your previous output was not parseable JSON.\nReturn ONLY one JSON object matching the schema. No extra text.\n\nQuery:\n{}\n\nEvidence:\n{}\n\nPrevious output:\n{}\n",
                        query.trim(),
                        text.trim(),
                        prev.trim()
                    );
                    let client = openai_like.as_ref().expect("openai-like client");
                    let wants_json = json_mode;
                    let json_allowed = llm_backend.as_str() != "openrouter"
                        || openrouter_caps
                            .as_ref()
                            .map(|s| s.contains("response_format"))
                            .unwrap_or(true);
                    let retry_text = if wants_json && json_allowed {
                        match client
                            .chat_json_with_options(
                                &system,
                                &retry_user,
                                timeout_ms,
                                args.max_tokens.or(Some(250)),
                                temperature,
                                top_p,
                                chat_options.clone(),
                            )
                            .await
                        {
                            Ok(s) => s,
                            Err(_) if llm_backend.as_str() == "openrouter" => client
                                .chat_with_options(
                                    &system,
                                    &retry_user,
                                    timeout_ms,
                                    args.max_tokens.or(Some(250)),
                                    temperature,
                                    top_p,
                                    chat_options.clone(),
                                )
                                .await
                                .unwrap_or_default(),
                            Err(_) => String::new(),
                        }
                    } else {
                        client
                            .chat_with_options(
                                &system,
                                &retry_user,
                                timeout_ms,
                                args.max_tokens.or(Some(250)),
                                temperature,
                                top_p,
                                chat_options.clone(),
                            )
                            .await
                            .unwrap_or_default()
                    };
                    resp_text = retry_text;
                    parsed = extract_json_object(&resp_text);
                }

                if transcript_enabled {
                    transcript_seq = transcript_seq.wrapping_add(1);
                    let line = serde_json::json!({
                        "schema_version": 1,
                        "kind": "webpipe_eval_transcript_event",
                        "generated_at_epoch_s": now,
                        "seq": transcript_seq,
                        "run_kind": "eval_matrix_llm_judge",
                        "call_id": call_id,
                        "llm": {
                            "backend": llm_backend,
                            "model_effective": llm_model_effective,
                            "json_mode": json_mode,
                            "timeout_ms": timeout_ms
                        },
                        "prompt": {
                            "system": tx_trunc(&system, transcript_max_chars),
                            "user": tx_trunc(&user, transcript_max_chars),
                            "prompt_truncated": prompt_truncated
                        },
                        "response": {
                            "raw": tx_trunc(&resp_text, transcript_max_chars),
                            "parsed": parsed
                        },
                        "timing_ms": elapsed_ms
                    });
                    let _ = append_jsonl(transcript_path.as_path(), &line);
                }
                let (relevant, answerable, confidence) = if let Some(v) = parsed.as_ref() {
                    (
                        v.get("relevant").and_then(|x| x.as_bool()).unwrap_or(false),
                        v.get("answerable")
                            .and_then(|x| x.as_bool())
                            .unwrap_or(false),
                        v.get("confidence").and_then(|x| x.as_f64()).unwrap_or(0.0),
                    )
                } else {
                    (false, false, 0.0)
                };

                totals["examples"] =
                    serde_json::json!(totals["examples"].as_u64().unwrap_or(0) + 1);
                if ok {
                    totals["ok"] = serde_json::json!(totals["ok"].as_u64().unwrap_or(0) + 1);
                }
                if relevant {
                    totals["relevant"] =
                        serde_json::json!(totals["relevant"].as_u64().unwrap_or(0) + 1);
                }
                if answerable {
                    totals["answerable"] =
                        serde_json::json!(totals["answerable"].as_u64().unwrap_or(0) + 1);
                }
                if parsed.is_none() {
                    totals["parse_failed"] =
                        serde_json::json!(totals["parse_failed"].as_u64().unwrap_or(0) + 1);
                }

                per_example.push(serde_json::json!({
                    "query_id": query_id,
                    "case": case,
                    "ok": ok,
                    "prompt_truncated": prompt_truncated,
                    "elapsed_ms": t0.elapsed().as_millis(),
                    "llm_backend": llm_backend,
                    "llm_model": args.llm_model,
                    "llm_model_effective": llm_model_effective,
                    "judge": parsed,
                    "judge_raw": if parsed.is_none() { resp_text } else { String::new() },
                    "derived": {
                        "relevant": relevant,
                        "answerable": answerable,
                        "confidence": confidence
                    }
                }));
            }

            let payload = serde_json::json!({
                "schema_version": 1,
                "kind": "webpipe_eval_matrix_llm_judge",
                "generated_at_epoch_s": now,
                "inputs": {
                    "examples_artifact": args.examples_artifact,
                    "llm_backend": llm_backend,
                    "llm_model": args.llm_model,
                    "llm_model_effective": llm_model_effective,
                    "prompt_preset": args.prompt_preset,
                    "system_prompt_file": args.system_prompt_file,
                    "json_mode": json_mode,
                    "retry_on_parse_fail": retry_on_parse_fail,
                    "timeout_ms": timeout_ms,
                    "max_examples": max_examples,
                    "max_prompt_chars": max_prompt_chars,
                    "max_tokens": args.max_tokens,
                    "temperature": temperature,
                    "top_p": top_p,
                    "transcript": transcript_enabled,
                    "transcript_jsonl": if transcript_enabled { Some(transcript_path.clone()) } else { None },
                    "transcript_max_chars": transcript_max_chars
                },
                "totals": totals,
                "per_example": per_example
            });

            std::fs::write(&out, serde_json::to_string_pretty(&payload)? + "\n")?;
            println!("{}", out.display());
        }
        #[cfg(feature = "eval")]
        Commands::EvalMatrixRun(args) => {
            let now = args.now_epoch_s.unwrap_or_else(|| {
                std::time::SystemTime::now()
                    .duration_since(std::time::UNIX_EPOCH)
                    .unwrap_or_default()
                    .as_secs()
            });
            let now_s = now.to_string();
            let out_dir = args
                .out_dir
                .unwrap_or_else(|| std::path::PathBuf::from(".generated"));
            std::fs::create_dir_all(&out_dir)?;

            let matrix_out = out_dir.join(format!("webpipe-eval-matrix-run-{now}.jsonl"));
            let score_out = out_dir.join(format!("webpipe-eval-matrix-score-run-{now}.json"));
            let export_out = out_dir.join(format!("webpipe-eval-matrix-export-run-{now}.jsonl"));
            let judge_out = out_dir.join(format!("webpipe-eval-matrix-judge-run-{now}.json"));
            let manifest_out = out_dir.join(format!("webpipe-eval-matrix-manifest-run-{now}.json"));

            let exe = std::env::current_exe()?;
            let queries_json = args.queries_json.to_string_lossy().to_string();
            let qrels = args.qrels.to_string_lossy().to_string();
            let matrix_out_s = matrix_out.to_string_lossy().to_string();
            let score_out_s = score_out.to_string_lossy().to_string();
            let export_out_s = export_out.to_string_lossy().to_string();
            let judge_out_s = judge_out.to_string_lossy().to_string();
            let max_text_chars_s = args.max_text_chars.to_string();

            fn run(mut cmd: std::process::Command) -> Result<()> {
                let out = cmd.output()?;
                if !out.status.success() {
                    let code = out.status.code().unwrap_or(-1);
                    anyhow::bail!(
                        "command failed (exit={code}):\nstdout:\n{}\nstderr:\n{}",
                        String::from_utf8_lossy(&out.stdout),
                        String::from_utf8_lossy(&out.stderr)
                    );
                }
                Ok(())
            }

            // 1) eval-matrix
            let mut c = std::process::Command::new(&exe);
            c.args([
                "eval-matrix",
                "--queries-json",
                &queries_json,
                "--base-url",
                &args.base_url,
                "--provider",
                &args.provider,
                "--auto-mode",
                &args.auto_mode,
                "--selection-mode",
                &args.selection_mode,
                "--fetch-backend",
                &args.fetch_backend,
                "--out",
                &matrix_out_s,
                "--now-epoch-s",
                &now_s,
            ]);
            run(c)?;

            // 2) eval-matrix-score
            let mut c = std::process::Command::new(&exe);
            c.args([
                "eval-matrix-score",
                "--matrix-artifact",
                &matrix_out_s,
                "--qrels",
                &qrels,
                "--out",
                &score_out_s,
                "--now-epoch-s",
                &now_s,
            ]);
            run(c)?;

            // 3) eval-matrix-export
            let mut c = std::process::Command::new(&exe);
            c.args([
                "eval-matrix-export",
                "--matrix-artifact",
                &matrix_out_s,
                "--qrels",
                &qrels,
                "--max-text-chars",
                &max_text_chars_s,
                "--out",
                &export_out_s,
                "--now-epoch-s",
                &now_s,
            ]);
            run(c)?;

            // 4) eval-matrix-judge
            let mut c = std::process::Command::new(&exe);
            c.args([
                "eval-matrix-judge",
                "--examples-artifact",
                &export_out_s,
                "--out",
                &judge_out_s,
                "--now-epoch-s",
                &now_s,
            ]);
            run(c)?;

            let git_sha = args.git_sha.clone().or_else(best_effort_git_sha);
            let manifest = serde_json::json!({
                "schema_version": 1,
                "kind": "webpipe_eval_matrix_run_manifest",
                "generated_at_epoch_s": now,
                "inputs": {
                    "queries_json": args.queries_json,
                    "qrels": args.qrels,
                    "base_url": args.base_url,
                    "provider": args.provider,
                    "auto_mode": args.auto_mode,
                    "selection_mode": args.selection_mode,
                    "fetch_backend": args.fetch_backend,
                    "max_text_chars": args.max_text_chars
                },
                "git": {
                    "sha": git_sha
                },
                "artifacts": {
                    "matrix": matrix_out,
                    "score": score_out,
                    "export": export_out,
                    "judge": judge_out
                }
            });
            std::fs::write(
                &manifest_out,
                serde_json::to_string_pretty(&manifest)? + "\n",
            )?;
            match args.output.to_ascii_lowercase().as_str() {
                "json" => println!("{}", serde_json::to_string(&manifest)?),
                _ => println!("{}", judge_out.display()),
            }
        }
        #[cfg(all(feature = "eval", feature = "stdio"))]
        Commands::EvalJudgeSwarm(args) => {
            use rmcp::handler::server::wrapper::Parameters;

            let now = args.now_epoch_s.unwrap_or_else(|| {
                std::time::SystemTime::now()
                    .duration_since(std::time::UNIX_EPOCH)
                    .unwrap_or_default()
                    .as_secs()
            });
            let out = args.out.unwrap_or_else(|| {
                std::path::PathBuf::from(format!(".generated/webpipe-eval-judge-swarm-{now}.json"))
            });
            std::fs::create_dir_all(
                out.parent()
                    .unwrap_or_else(|| std::path::Path::new(".generated")),
            )?;

            let transcript_enabled = args.transcript;
            let transcript_max_chars = args.transcript_max_chars.clamp(200, 200_000);
            let transcript_path = args.transcript_jsonl.clone().unwrap_or_else(|| {
                std::path::PathBuf::from(format!("{}.transcript.jsonl", out.display()))
            });
            let mut transcript_seq: u64 = 0;

            fn parse_csv(s: &str) -> Vec<String> {
                s.split(',')
                    .map(|x| x.trim().to_string())
                    .filter(|x| !x.is_empty())
                    .collect()
            }

            fn take_chars(s: &str, max_chars: usize) -> (String, bool) {
                if max_chars == 0 {
                    return (String::new(), !s.is_empty());
                }
                let mut out = String::new();
                for (n, ch) in s.chars().enumerate() {
                    if n >= max_chars {
                        return (out, true);
                    }
                    out.push(ch);
                }
                (out, false)
            }

            fn append_jsonl(
                path: &std::path::Path,
                line: &serde_json::Value,
            ) -> anyhow::Result<()> {
                if let Some(p) = path.parent() {
                    std::fs::create_dir_all(p)?;
                }
                use std::io::Write;
                let mut f = std::fs::OpenOptions::new()
                    .create(true)
                    .append(true)
                    .open(path)?;
                writeln!(f, "{}", serde_json::to_string(line)?)?;
                Ok(())
            }

            fn tx_trunc(s: &str, max_chars: usize) -> serde_json::Value {
                let (t, truncated) = take_chars(s, max_chars);
                serde_json::json!({
                    "text": t,
                    "chars": s.chars().count(),
                    "truncated": truncated
                })
            }

            fn json_trunc(v: &serde_json::Value, max_chars: usize) -> serde_json::Value {
                let s = serde_json::to_string(v).unwrap_or_default();
                tx_trunc(&s, max_chars)
            }

            fn extract_json_object(s: &str) -> Option<serde_json::Value> {
                // Prefer parsing the full response first (common case).
                let trimmed = s.trim();
                if let Ok(v) = serde_json::from_str::<serde_json::Value>(trimmed) {
                    return Some(v);
                }

                // Fallback: pull out the outermost {...} region, Unicode-safe (char offsets).
                let mut start: Option<usize> = None;
                let mut end: Option<usize> = None;
                for (i, ch) in trimmed.chars().enumerate() {
                    if start.is_none() && ch == '{' {
                        start = Some(i);
                    }
                    if ch == '}' {
                        end = Some(i);
                    }
                }
                let (start, end) = match (start, end) {
                    (Some(a), Some(b)) if b >= a => (a, b),
                    _ => return None,
                };

                let slice: String = trimmed
                    .chars()
                    .skip(start)
                    .take(end.saturating_sub(start) + 1)
                    .collect();
                serde_json::from_str(&slice).ok()
            }

            // NOTE: We intentionally avoid any cross-judge “global notes” channel.
            // It is too easy to accidentally leak labels/ground-truth or cause side-note bleed.

            fn hash64(seed: u64, s: &str) -> u64 {
                // Deterministic FNV-1a-ish (no extra deps).
                let mut h: u64 = 0xcbf29ce484222325 ^ seed;
                for b in s.as_bytes() {
                    h ^= *b as u64;
                    h = h.wrapping_mul(0x100000001b3);
                }
                h
            }

            #[derive(Clone, Debug, Default)]
            struct CurriculumStats {
                worked: bool,
                best_score: f64,
            }

            fn load_curriculum(
                path: &std::path::Path,
            ) -> anyhow::Result<std::collections::BTreeMap<String, CurriculumStats>> {
                let raw = std::fs::read_to_string(path)?;
                let v: serde_json::Value = serde_json::from_str(&raw)?;
                let ok = v.get("schema_version").and_then(|x| x.as_u64()) == Some(1)
                    && v.get("kind").and_then(|x| x.as_str()) == Some("webpipe_eval_judge_swarm");
                if !ok {
                    anyhow::bail!("curriculum_from: unexpected kind/schema_version");
                }
                let mut out: std::collections::BTreeMap<String, CurriculumStats> =
                    std::collections::BTreeMap::new();
                let Some(reports) = v.get("judge_reports").and_then(|x| x.as_array()) else {
                    return Ok(out);
                };
                for r in reports {
                    let Some(per_query) = r.get("per_query").and_then(|x| x.as_array()) else {
                        continue;
                    };
                    for q in per_query {
                        let qid = q
                            .get("query_id")
                            .and_then(|x| x.as_str())
                            .unwrap_or("")
                            .trim();
                        if qid.is_empty() {
                            continue;
                        }
                        let _hit_any = q
                            .get("hit_expected_url_any_trial")
                            .and_then(|x| x.as_bool())
                            .unwrap_or(false);
                        let solved = q
                            .get("task_solve")
                            .and_then(|x| x.get("evidence_sufficient"))
                            .and_then(|x| x.as_bool())
                            .unwrap_or(false);
                        // Curriculum "worked" should reflect *answerability*, not just URL hits.
                        // URL hits can be misleading when extraction returns empty/boilerplate.
                        let mut any_answerable = false;
                        if let Some(trials) = q.get("trials").and_then(|x| x.as_array()) {
                            for t in trials {
                                let a = t
                                    .get("judge")
                                    .and_then(|j| j.get("answerable"))
                                    .and_then(|x| x.as_bool())
                                    .unwrap_or(false);
                                if a {
                                    any_answerable = true;
                                    break;
                                }
                            }
                        }
                        let worked = solved || any_answerable;

                        let mut best_score = 0.0;
                        if let Some(trials) = q.get("trials").and_then(|x| x.as_array()) {
                            for t in trials {
                                let s = t
                                    .get("judge")
                                    .and_then(|j| j.get("overall_score"))
                                    .and_then(|x| x.as_f64())
                                    .unwrap_or(0.0);
                                if s > best_score {
                                    best_score = s;
                                }
                            }
                        }

                        out.entry(qid.to_string())
                            .and_modify(|st| {
                                st.worked = st.worked || worked;
                                if best_score > st.best_score {
                                    st.best_score = best_score;
                                }
                            })
                            .or_insert(CurriculumStats { worked, best_score });
                    }
                }
                Ok(out)
            }

            #[derive(Clone, Debug)]
            struct TrialCfg {
                id: &'static str,
                agentic: bool,
                agentic_selector: &'static str,
                url_selection_mode: &'static str,
                fetch_backend: String,
            }

            #[derive(Clone, Debug)]
            struct JudgePreset {
                id: String,
                domain_tags: Vec<String>,
                system: String,
            }

            fn builtin_judge_presets() -> std::collections::BTreeMap<String, JudgePreset> {
                let mut out = std::collections::BTreeMap::new();
                out.insert(
                    "docs".to_string(),
                    JudgePreset {
                        id: "docs".to_string(),
                        domain_tags: vec!["docs".to_string()],
                        system: r#"You are a strict, adversarial judge for developer documentation retrieval and extraction.

You will see multiple trials (different tool strategies) for the same query.
You must score signal quality and tail-risk failure modes (boilerplate, JS shells, off-topic pages, truncation).

Return ONE JSON object ONLY (no markdown, no prose) with schema:
{
  "overall_score": number,            // 0..100
  "relevant": boolean,
  "answerable": boolean,
  "confidence": number,              // 0..1
  "issues": string[],                // use strings like: ["ux_confusing","writing_poor","math_broken","js_challenge","boilerplate","truncated","missing_key_facts"]
  "musings": [{"dimension": string, "note": string}] // optional; up to 12 short items (dimension<=32, note<=200)
}

Musings guidance (if you include them):
- Prefer 1–3 items, especially when answerable=false or overall_score < 85.
- Choose any dimensions you want; keep them short and stable (e.g. "ux", "coverage", "credibility", "freshness", "math", "latency", "retrieval").
}"#
                        .to_string(),
                    },
                );
                out.insert(
                    "academic".to_string(),
                    JudgePreset {
                        id: "academic".to_string(),
                        domain_tags: vec![
                            "academic".to_string(),
                            "paper".to_string(),
                            "arxiv".to_string(),
                        ],
                        system: r#"You are a strict, adversarial judge for academic/paper retrieval and extraction.

Favor trials that surface concrete claims, definitions, and key results from the evidence text.
Penalize boilerplate, navigation-only pages, and missing key facts.

Return ONE JSON object ONLY (no markdown, no prose) with schema:
{
  "overall_score": number,            // 0..100
  "relevant": boolean,
  "answerable": boolean,
  "confidence": number,              // 0..1
  "issues": string[],                // use strings like: ["missing_key_facts","writing_poor","math_broken","off_topic","truncated"]
  "musings": [{"dimension": string, "note": string}] // optional; up to 12 short items
}

Musings guidance (if you include them): prefer 1–3 items, especially on failures.
}"#
                        .to_string(),
                    },
                );
                out.insert(
                    "news".to_string(),
                    JudgePreset {
                        id: "news".to_string(),
                        domain_tags: vec!["news".to_string()],
                        system: r#"You are a strict, adversarial judge for news/article retrieval and extraction.

Penalize paywalls/auth walls, JS challenges, and low-signal app shells.
Favor trials with concrete facts and minimal boilerplate.

Return ONE JSON object ONLY (no markdown, no prose) with schema:
{
  "overall_score": number,            // 0..100
  "relevant": boolean,
  "answerable": boolean,
  "confidence": number,              // 0..1
  "issues": string[],                // use strings like: ["paywall","js_challenge","boilerplate","low_signal","off_topic"]
  "musings": [{"dimension": string, "note": string}] // optional; up to 12 short items
}

Musings guidance (if you include them): prefer 1–3 items, especially on failures.
}"#
                        .to_string(),
                    },
                );
                // Lens-specific “poor man’s adversarial judges”.
                out.insert(
                    "ux".to_string(),
                    JudgePreset {
                        id: "ux".to_string(),
                        domain_tags: vec!["docs".to_string(), "news".to_string(), "academic".to_string()],
                        system: r#"You are an adversarial UX/layout judge for retrieval/extraction outputs.

You are judging evidence text produced by a web retrieval + extraction pipeline (not a hand-written document).
Your job is to catch UX-shaped failures that make the page unusable even if it contains some relevant words.

Expert rubric (what “good” looks like):
- The extract surfaces the *right section* quickly (not nav chrome).
- The extract preserves meaningful structure: headings, short paragraphs, lists.
- The extract is scannable: low boilerplate ratio, minimal repetition.
- It avoids “false affordances”: pages that look like content but are actually app shells / cookie walls / JS challenges.

Hard negatives (almost always answerable=false):
- JS challenge / captcha / auth wall / “enable JavaScript”
- paywall / subscribe wall where the body is missing
- empty or near-empty output
- minified bundle junk or serialized noise

Output ONE JSON object only (no markdown, no prose). Required keys:
{
  "overall_score": number,            // 0..100
  "relevant": boolean,
  "answerable": boolean,
  "confidence": number,              // 0..1
  "issues": string[],                // use short codes
  "musings": [{"dimension": string, "note": string}] // optional; up to 12 short items
}

Musings guidance (if you include them): prefer 1–3 items, especially on failures.

Optional (structured notes; keep short): you MAY include:
{
  "good": string[],   // up to 3 bullets
  "bad": string[],    // up to 3 bullets
  "fixes": string[]   // up to 3 concrete fixes for the tool/pipeline
}

Issue codes (pick 1–6):
- "empty", "low_signal", "boilerplate", "nav_only", "js_challenge", "paywall", "gunk", "truncated",
- "wrong_section", "duplication", "toc_spam", "poor_structure", "hard_to_scan"

Scoring guidance:
- 90–100: clean, structured, right section, minimal noise.
- 60–89: mostly right but some chrome/noise/truncation.
- 0–59: wrong section, mostly boilerplate, or hard-negative."#
                            .to_string(),
                    },
                );
                out.insert(
                    "writing".to_string(),
                    JudgePreset {
                        id: "writing".to_string(),
                        domain_tags: vec!["docs".to_string(), "news".to_string(), "academic".to_string()],
                        system: r#"You are an adversarial writing-quality judge for retrieved/extracted text.

You are judging whether the extracted evidence is *readable and sufficient* to answer the user query.
Assume extraction artifacts exist (broken wrapping, missing punctuation, duplicated nav). You must detect them.

Expert rubric (math/tech writing aware):
- Definitions appear before use (terms introduced, symbols explained in nearby prose).
- The “answer-bearing” paragraph is present (not just a TOC).
- Lists read cleanly (no orphan punctuation, no mangled numbering).
- References to code/config are intact (flags, names, units).
- For technical docs: includes at least one concrete step / API / command.

Readability targets (concrete, use as heuristics):
- Line length: prefer ~65–75 characters for prose; if the evidence is presented as a “block”, treat >80 chars as a readability risk.
  (WCAG 2.1 SC 1.4.8 suggests a width cap of 80 characters/glyphs for blocks of text.)
- Leading: for dense prose, ~1.5 line spacing improves scan and reduces line-skipping (also cited in WCAG 2.1 SC 1.4.8 guidance).

Penalize:
- incoherent fragments / sentence boundary loss
- missing key steps/constraints/definitions
- “TOC spam” (lots of headings, no substance)
- boilerplate domination (cookie banners, nav)
- truncation that drops the “how-to”

Output ONE JSON object only. Required keys:
{
  "overall_score": number,            // 0..100
  "relevant": boolean,
  "answerable": boolean,
  "confidence": number,              // 0..1
  "issues": string[],
  "musings": [{"dimension": string, "note": string}] // optional; up to 12 short items
}

Optional structured notes (short): you MAY include:
{
  "good": string[],
  "bad": string[],
  "fixes": string[]
}

Issue codes (pick 1–6):
- "writing_poor", "too_short", "missing_key_facts", "off_topic", "boilerplate", "toc_spam",
- "duplication", "broken_punctuation", "broken_lists", "truncated", "gunk"

Scoring guidance:
- 90–100: coherent, answer-bearing, includes concrete steps.
- 60–89: mostly coherent but missing one key piece or has moderate noise.
- 0–59: fragmented, dominated by boilerplate/TOC, or missing the answer-bearing portion."#
                            .to_string(),
                    },
                );
                out.insert(
                    "rendered_page".to_string(),
                    JudgePreset {
                        id: "rendered_page".to_string(),
                        domain_tags: vec!["docs".to_string(), "news".to_string(), "academic".to_string()],
                        system: r#"You are an adversarial “rendered technical page” judge.

You are judging extracted evidence text (HTML→text), but your rubric is: “would the original rendered page be usable and trustworthy?”
Treat missing structure as a rendering/usability failure even if some keywords are present.

Expert rubric:
- The table-of-contents (if present) is not the only content; the answer-bearing section is included.
- Heading hierarchy is preserved (you can tell what’s a section vs subsection).
- Lists are readable (no smashed numbering / orphan punctuation).
- Code blocks are intact (commands/flags not truncated or wrapped into nonsense).
- Math/notation (if relevant) survives (symbols/relations; no delimiter soup).

Hard negatives (answerable=false):
- nav-only / app shell / login wall / “enable JavaScript”
- content is mostly boilerplate or duplicated chrome
- the extract is so structure-poor you can’t locate the answer-bearing section

Output ONE JSON object only. Required keys:
{
  "overall_score": number,            // 0..100
  "relevant": boolean,
  "answerable": boolean,
  "confidence": number,              // 0..1
  "issues": string[],
  "musings": [{"dimension": string, "note": string}] // optional; up to 12 short items
}

Optional structured notes (short): you MAY include:
{
  "good": string[],
  "bad": string[],
  "fixes": string[]
}

Issue codes (pick 1–8):
- "nav_only", "boilerplate", "toc_spam", "poor_hierarchy", "broken_lists", "code_broken", "math_broken", "truncated", "low_signal"

Scoring guidance:
- 90–100: strongly structured and answer-bearing.
- 60–89: mostly usable but one structural weakness.
- 0–59: structure is missing or dominated by chrome."#
                            .to_string(),
                    },
                );
                out.insert(
                    "math".to_string(),
                    JudgePreset {
                        id: "math".to_string(),
                        domain_tags: vec!["docs".to_string(), "academic".to_string()],
                        system: r#"You are an adversarial math/notation fidelity judge.

You are judging evidence text that may have lost structure during HTML→text extraction.
You must detect when *math-bearing meaning* was lost.

Expert rubric (math exposition aware):
- Key expressions survive (symbols, subscripts, Greek, set membership like “∈”, fractions, sums).
- The statement is interpretable: variables referenced in prose, units/assumptions present.
- Display math is not reduced to garbage; inline math is not flattened into ambiguous ASCII.

KaTeX/LaTeX failure modes to watch for (technical, common in extraction):
- Broken delimiters or delimiter confusion: e.g. `$...$` vs `$$...$$` vs `\(...\)`/`\[...\]`.
  KaTeX auto-render treats delimiters in-order; if `$` is handled before `$$`, `$$` can be misparsed as empty math.
- Render-vs-text mismatch: the page may render math correctly, but extracted text can drop backslashes/braces, losing meaning.

Hard negatives:
- The query clearly needs a formula/definition, but the extract has none.
- The extract contains broken LaTeX delimiters / mangled backslashes / missing braces that destroy meaning.
- The extract is mostly unreadable glyph soup.

Output ONE JSON object only. Required keys:
{
  "overall_score": number,            // 0..100
  "relevant": boolean,
  "answerable": boolean,
  "confidence": number,              // 0..1
  "issues": string[],
  "musings": [{"dimension": string, "note": string}] // optional; up to 12 short items
}

Optional structured notes (short): you MAY include:
{
  "good": string[],
  "bad": string[],
  "fixes": string[]
}

Issue codes (pick 1–6):
- "math_missing", "math_broken", "symbol_loss", "delimiter_broken", "equation_missing",
- "missing_definitions", "too_short", "truncated", "off_topic"

Scoring guidance:
- 90–100: equations/notation intact and sufficient.
- 60–89: mostly intact but one key definition/equation missing.
- 0–59: math meaning lost (missing/broken/garbled)."#
                            .to_string(),
                    },
                );
                out.insert(
                    "redteam".to_string(),
                    JudgePreset {
                        id: "redteam".to_string(),
                        domain_tags: vec![
                            "docs".to_string(),
                            "news".to_string(),
                            "academic".to_string(),
                        ],
                        system: r#"You are a red-team adversarial judge.

You are skeptical by default. Your job is to surface plausible failure modes and false confidence.
If you can find any plausible failure mode, mark answerable=false and record it.

Red-team checklist:
- Wrong page type: nav-only / app shell / marketing / forum / SEO spam.
- Mismatch: query asks “how/why/definition”, evidence has only a snippet.
- Staleness risk: versioned docs mismatch (e.g. old API).
- Truncation risk: evidence cuts off at the “critical step”.
- Citation risk: claims appear without any supporting quote-like text.
- Confabulation risk: answerable=true would require outside knowledge.

Output ONE JSON object only. Required keys:
{
  "overall_score": number,            // 0..100
  "relevant": boolean,
  "answerable": boolean,
  "confidence": number,              // 0..1
  "issues": string[],                // include at least one issue unless truly clean
  "musings": [{"dimension": string, "note": string}] // optional; up to 12 short items
}

Optional structured notes (short): you MAY include:
{
  "bad": string[],    // up to 3 failure modes
  "fixes": string[]   // up to 3 next experiments to reduce risk
}

Issue codes (pick 1–8):
- "needs_citation", "insufficient_evidence", "wrong_page_type", "stale_version_risk",
- "truncated", "off_topic", "boilerplate", "low_signal", "js_challenge", "paywall"

Scoring guidance:
- 90–100: clean evidence, direct support, low tail-risk.
- 60–89: plausible but needs one more supporting detail.
- 0–59: high risk of being wrong or unsupported."#
                            .to_string(),
                    },
                );
                out
            }

            let llm_backend = args.llm_backend.to_ascii_lowercase();
            let temperature = args.temperature.or(Some(0.0));
            let top_p = args.top_p.or(Some(1.0));
            let json_mode = args.json_mode;
            let retry_on_parse_fail = args.retry_on_parse_fail;
            let llm_timeout_ms = args.llm_timeout_ms.clamp(500, 300_000);

            let http = reqwest::Client::new();
            let openai_like = match llm_backend.as_str() {
                "openai" => {
                    let api_key = mcp::WebpipeMcp::openai_api_key_from_env().ok_or_else(|| {
                        anyhow::anyhow!("missing OPENAI_API_KEY (or WEBPIPE_OPENAI_API_KEY)")
                    })?;
                    let model = args
                        .llm_model
                        .clone()
                        .unwrap_or_else(mcp::WebpipeMcp::openai_model_from_env);
                    Some(webpipe_local::openai_compat::OpenAiCompatClient::new(
                        http.clone(),
                        "https://api.openai.com".to_string(),
                        Some(api_key),
                        model,
                    )?)
                }
                "openai_compat" => {
                    Some(webpipe_local::openai_compat::OpenAiCompatClient::from_env(
                        http.clone(),
                        args.llm_model.clone(),
                    )?)
                }
                "openrouter" => {
                    let api_key =
                        mcp::WebpipeMcp::openrouter_api_key_from_env().ok_or_else(|| {
                            anyhow::anyhow!(
                                "missing OPENROUTER_API_KEY (or WEBPIPE_OPENROUTER_API_KEY)"
                            )
                        })?;
                    let model = args
                        .llm_model
                        .clone()
                        .unwrap_or_else(mcp::WebpipeMcp::openrouter_model_from_env);
                    Some(webpipe_local::openai_compat::OpenAiCompatClient::new(
                        http.clone(),
                        // OpenAiCompatClient appends `/v1/chat/completions`.
                        "https://openrouter.ai/api".to_string(),
                        Some(api_key),
                        model,
                    )?)
                }
                "groq" => {
                    let api_key = mcp::WebpipeMcp::groq_api_key_from_env().ok_or_else(|| {
                        anyhow::anyhow!("missing GROQ_API_KEY (or WEBPIPE_GROQ_API_KEY)")
                    })?;
                    let model = args
                        .llm_model
                        .clone()
                        .unwrap_or_else(mcp::WebpipeMcp::groq_model_from_env);
                    Some(webpipe_local::openai_compat::OpenAiCompatClient::new(
                        http.clone(),
                        // OpenAiCompatClient appends `/v1/chat/completions`.
                        "https://api.groq.com/openai".to_string(),
                        Some(api_key),
                        model,
                    )?)
                }
                _ => None,
            };
            let ollama = if llm_backend == "ollama" {
                Some(webpipe_local::ollama::OllamaClient::from_env(http.clone())?)
            } else {
                None
            };
            let llm_model_effective: Option<String> =
                openai_like.as_ref().map(|c| c.model().to_string());
            if openai_like.is_none() && ollama.is_none() {
                anyhow::bail!(
                    "unknown llm_backend: {} (allowed: openai, openai_compat, openrouter, groq, ollama)",
                    llm_backend
                );
            }

            #[derive(serde::Deserialize)]
            struct OpenRouterModels {
                #[serde(default)]
                data: Vec<OpenRouterModel>,
            }

            #[derive(serde::Deserialize)]
            struct OpenRouterModel {
                id: String,
                #[serde(default)]
                supported_parameters: Vec<String>,
            }

            async fn openrouter_supported_params(
                http: &reqwest::Client,
                model_id: &str,
            ) -> Option<std::collections::BTreeSet<String>> {
                if model_id.trim().is_empty() {
                    return None;
                }
                let resp = http
                    .get("https://openrouter.ai/api/v1/models")
                    .timeout(std::time::Duration::from_millis(8_000))
                    .send()
                    .await
                    .ok()?;
                if !resp.status().is_success() {
                    return None;
                }
                let parsed: OpenRouterModels = resp.json().await.ok()?;
                let m = parsed.data.into_iter().find(|m| m.id == model_id)?;
                Some(m.supported_parameters.into_iter().collect())
            }

            let openrouter_caps: Option<std::collections::BTreeSet<String>> =
                if llm_backend == "openrouter" {
                    if let Some(mid) = llm_model_effective.as_deref() {
                        openrouter_supported_params(&http, mid).await
                    } else {
                        None
                    }
                } else {
                    None
                };

            let mut chat_options = webpipe_local::openai_compat::ChatOptions::default();
            if llm_backend == "openrouter" {
                if let Ok(v) = std::env::var("WEBPIPE_OPENROUTER_INCLUDE_REASONING") {
                    let v = v.trim().to_ascii_lowercase();
                    if (v == "true" || v == "1")
                        && openrouter_caps
                            .as_ref()
                            .map(|s| s.contains("include_reasoning"))
                            .unwrap_or(true)
                    {
                        chat_options.include_reasoning = Some(true);
                    }
                    if (v == "false" || v == "0")
                        && openrouter_caps
                            .as_ref()
                            .map(|s| s.contains("include_reasoning"))
                            .unwrap_or(true)
                    {
                        chat_options.include_reasoning = Some(false);
                    }
                }
                if let Ok(eff) = std::env::var("WEBPIPE_OPENROUTER_REASONING_EFFORT") {
                    let eff = eff.trim().to_string();
                    if !eff.is_empty()
                        && openrouter_caps
                            .as_ref()
                            .map(|s| s.contains("reasoning"))
                            .unwrap_or(true)
                    {
                        chat_options.reasoning_effort = Some(eff);
                    }
                }
            }

            #[allow(clippy::too_many_arguments)]
            async fn llm_call(
                llm_backend: &str,
                openai_like: Option<&webpipe_local::openai_compat::OpenAiCompatClient>,
                ollama: Option<&webpipe_local::ollama::OllamaClient>,
                openrouter_caps: Option<&std::collections::BTreeSet<String>>,
                chat_options: &webpipe_local::openai_compat::ChatOptions,
                messages: Vec<webpipe_local::openai_compat::ChatMessage>,
                timeout_ms: u64,
                temperature: Option<f64>,
                top_p: Option<f64>,
                json_mode: bool,
            ) -> anyhow::Result<String> {
                match llm_backend {
                    "ollama" => {
                        // Ollama adapter is system+user only; use best-effort extraction.
                        let system = messages
                            .iter()
                            .rev()
                            .find(|m| m.role == "system")
                            .map(|m| m.content.as_str())
                            .unwrap_or("");
                        let user = messages
                            .iter()
                            .rev()
                            .find(|m| m.role == "user")
                            .map(|m| m.content.as_str())
                            .unwrap_or("");
                        Ok(ollama
                            .expect("ollama client")
                            .chat(system, user, timeout_ms)
                            .await
                            .map_err(|e| anyhow::anyhow!(e.to_string()))?)
                    }
                    "openai" | "openai_compat" | "openrouter" | "groq" => {
                        let c = openai_like.expect("openai-like client");
                        let wants_json = json_mode;
                        let json_allowed = llm_backend != "openrouter"
                            || openrouter_caps
                                .map(|s| s.contains("response_format"))
                                .unwrap_or(true);
                        let json_mode_effective = wants_json && json_allowed;
                        Ok(c.chat_messages_with_options(
                            messages,
                            timeout_ms,
                            Some(350),
                            temperature,
                            top_p,
                            json_mode_effective,
                            chat_options.clone(),
                        )
                        .await
                        .map_err(|e| anyhow::anyhow!(e.to_string()))?)
                    }
                    _ => anyhow::bail!("unknown llm_backend"),
                }
            }

            // Issues are a controlled vocabulary; reject free-form drift.
            const ISSUE_VOCAB: &[&str] = &[
                "missing_key_facts",
                "too_short",
                "boilerplate",
                "low_signal",
                "off_topic",
                "truncated",
                "did_not_hit_expected_url",
            ];

            fn validate_scorecard(v: &serde_json::Value) -> bool {
                let Some(o) = v.as_object() else { return false };
                let score_ok = o
                    .get("overall_score")
                    .and_then(|x| x.as_f64())
                    .is_some_and(|s| (0.0..=100.0).contains(&s));
                let conf_ok = o
                    .get("confidence")
                    .and_then(|x| x.as_f64())
                    .is_some_and(|c| (0.0..=1.0).contains(&c));
                let relevant_ok = o.get("relevant").and_then(|x| x.as_bool()).is_some();
                let answerable_ok = o.get("answerable").and_then(|x| x.as_bool()).is_some();
                let issues_ok = match o.get("issues").and_then(|x| x.as_array()) {
                    Some(xs) => {
                        if xs.len() > 12 {
                            return false;
                        }
                        xs.iter().all(|x| {
                            x.as_str().is_some_and(|s| {
                                let s = s.trim();
                                !s.is_empty() && s.len() <= 64 && ISSUE_VOCAB.contains(&s)
                            })
                        })
                    }
                    None => false,
                };
                // Optional: allow judges to “muse” in a bounded, aggregatable shape.
                // We keep it structured (dimension + note) to support meta aggregation.
                let musings_ok = match o.get("musings") {
                    None => true,
                    Some(v) => match v.as_array() {
                        Some(xs) => {
                            if xs.len() > 12 {
                                return false;
                            }
                            xs.iter().all(|it| {
                                let Some(m) = it.as_object() else {
                                    return false;
                                };
                                let dim_ok = m
                                    .get("dimension")
                                    .and_then(|x| x.as_str())
                                    .is_some_and(|s| {
                                        let s = s.trim();
                                        !s.is_empty() && s.len() <= 32
                                    });
                                let note_ok =
                                    m.get("note").and_then(|x| x.as_str()).is_some_and(|s| {
                                        let s = s.trim();
                                        !s.is_empty() && s.len() <= 200
                                    });
                                dim_ok && note_ok
                            })
                        }
                        None => false,
                    },
                };
                let answerable = o
                    .get("answerable")
                    .and_then(|x| x.as_bool())
                    .unwrap_or(false);
                let overall_score = o
                    .get("overall_score")
                    .and_then(|x| x.as_f64())
                    .unwrap_or(0.0);
                let should_muse = !answerable || overall_score < 85.0;
                let musings_present = o
                    .get("musings")
                    .and_then(|x| x.as_array())
                    .is_some_and(|xs| !xs.is_empty());
                let musings_required_ok = !should_muse || musings_present;

                score_ok
                    && conf_ok
                    && relevant_ok
                    && answerable_ok
                    && issues_ok
                    && musings_ok
                    && musings_required_ok
            }

            fn validate_judge_overall(v: &serde_json::Value) -> bool {
                let Some(o) = v.as_object() else { return false };
                let oa_ok = o
                    .get("overall_assessment")
                    .and_then(|x| x.as_str())
                    .is_some_and(|s| !s.trim().is_empty());
                // top_failure_modes is a controlled vocabulary so we can aggregate it.
                let tfm_ok = match o.get("top_failure_modes").and_then(|x| x.as_array()) {
                    Some(xs) => {
                        if xs.len() > 12 {
                            return false;
                        }
                        xs.iter().all(|x| {
                            x.as_str().is_some_and(|s| {
                                let s = s.trim();
                                !s.is_empty() && s.len() <= 64 && ISSUE_VOCAB.contains(&s)
                            })
                        })
                    }
                    None => false,
                };
                let rd_ok = o
                    .get("recommended_defaults")
                    .map(|x| x.is_object())
                    .unwrap_or(false);
                let conf_ok = o
                    .get("confidence")
                    .and_then(|x| x.as_f64())
                    .is_some_and(|c| (0.0..=1.0).contains(&c));
                oa_ok && tfm_ok && rd_ok && conf_ok
            }

            fn validate_meta_summary(v: &serde_json::Value) -> bool {
                let Some(o) = v.as_object() else { return false };
                let oa_ok = o
                    .get("overall_assessment")
                    .and_then(|x| x.as_str())
                    .is_some_and(|s| {
                        let s = s.trim();
                        !s.is_empty() && s.len() <= 500
                    });
                let agree_ok = o
                    .get("cross_judge_agreement")
                    .and_then(|x| x.as_str())
                    .is_some_and(|s| {
                        let s = s.trim();
                        !s.is_empty() && s.len() <= 200
                    });
                // Keep systemic failures aggregatable, same controlled vocab as per-judge issues.
                let fails_ok = match o.get("top_systemic_failures").and_then(|x| x.as_array()) {
                    Some(xs) => {
                        if xs.len() > 12 {
                            return false;
                        }
                        xs.iter().all(|x| {
                            x.as_str().is_some_and(|s| {
                                let s = s.trim();
                                !s.is_empty() && s.len() <= 64 && ISSUE_VOCAB.contains(&s)
                            })
                        })
                    }
                    None => false,
                };
                let fixes_ok = match o.get("top_3_fixes").and_then(|x| x.as_array()) {
                    Some(xs) => {
                        if xs.len() > 3 {
                            return false;
                        }
                        let mut seen = std::collections::BTreeSet::new();
                        xs.iter().all(|x| {
                            x.as_str().is_some_and(|s| {
                                let s = s.trim();
                                !s.is_empty() && s.len() <= 96 && seen.insert(s.to_string())
                            })
                        })
                    }
                    None => false,
                };
                let next_ok = match o
                    .get("recommended_next_experiments")
                    .and_then(|x| x.as_array())
                {
                    Some(xs) => {
                        if xs.len() > 12 {
                            return false;
                        }
                        xs.iter().all(|x| {
                            x.as_str().is_some_and(|s| {
                                let s = s.trim();
                                !s.is_empty() && s.len() <= 96
                            })
                        })
                    }
                    None => false,
                };
                let dims_ok = match o.get("top_dimensions").and_then(|x| x.as_array()) {
                    Some(xs) => {
                        if xs.len() > 12 {
                            return false;
                        }
                        let mut seen = std::collections::BTreeSet::new();
                        xs.iter().all(|x| {
                            x.as_str().is_some_and(|s| {
                                let s = s.trim();
                                !s.is_empty() && s.len() <= 32 && seen.insert(s.to_string())
                            })
                        })
                    }
                    None => false,
                };
                let muses_ok = match o.get("top_musings").and_then(|x| x.as_array()) {
                    Some(xs) => {
                        if xs.len() > 12 {
                            return false;
                        }
                        xs.iter().all(|x| {
                            x.as_str().is_some_and(|s| {
                                let s = s.trim();
                                !s.is_empty() && s.len() <= 96
                            })
                        })
                    }
                    None => false,
                };
                oa_ok && agree_ok && fails_ok && fixes_ok && next_ok && dims_ok && muses_ok
            }

            fn validate_task_solve(v: &serde_json::Value) -> bool {
                let Some(o) = v.as_object() else { return false };
                let best = o
                    .get("best_trial_id")
                    .and_then(|x| x.as_str())
                    .is_some_and(|s| !s.trim().is_empty());
                let es = o
                    .get("evidence_sufficient")
                    .and_then(|x| x.as_bool())
                    .is_some();
                let conf = o
                    .get("confidence")
                    .and_then(|x| x.as_f64())
                    .is_some_and(|c| (0.0..=1.0).contains(&c));
                let cites = o.get("citations").map(|x| x.is_array()).unwrap_or(false);
                let has_answer = o
                    .get("answer")
                    .and_then(|x| x.as_str())
                    .is_some_and(|s| !s.trim().is_empty());
                let has_abstain = o
                    .get("abstain_reason")
                    .and_then(|x| x.as_str())
                    .is_some_and(|s| !s.trim().is_empty());
                best && es && conf && cites && (has_answer || has_abstain)
            }

            let base_url = args.base_url.trim_end_matches('/').to_string();
            let e2e = eval::load_e2e_queries_v1(&args.queries_json)?;
            let qrels_by_qid: std::collections::BTreeMap<String, Vec<String>> =
                if let Some(p) = args.qrels.as_ref() {
                    let qrels = eval::load_e2e_qrels_v1(p)?;
                    qrels
                        .qrels
                        .into_iter()
                        .map(|q| (q.query_id, q.expected_url_substrings))
                        .collect()
                } else {
                    std::collections::BTreeMap::new()
                };

            let mut domain_tags_filter: Vec<String> = Vec::new();
            if let Some(s) = args.domain_tags.as_ref() {
                domain_tags_filter = parse_csv(s);
            }

            let svc = mcp::WebpipeMcp::new().map_err(|e| anyhow::anyhow!(e.to_string()))?;

            let max_results = args.max_results.clamp(1, 20);
            let max_urls = args.max_urls.clamp(1, 10);
            let top_chunks = args.top_chunks.clamp(1, 50);
            let max_chunk_chars = args.max_chunk_chars.clamp(20, 5_000);

            let trials: Vec<TrialCfg> = match args.trial_set.to_ascii_lowercase().as_str() {
                "wide" => vec![
                    TrialCfg {
                        id: "non_agentic",
                        agentic: false,
                        agentic_selector: "lexical",
                        url_selection_mode: "preserve",
                        fetch_backend: args.fetch_backend.clone(),
                    },
                    TrialCfg {
                        id: "agentic_lexical",
                        agentic: true,
                        agentic_selector: "lexical",
                        url_selection_mode: "query_rank",
                        fetch_backend: args.fetch_backend.clone(),
                    },
                    TrialCfg {
                        id: "agentic_auto",
                        agentic: true,
                        agentic_selector: "auto",
                        url_selection_mode: "query_rank",
                        fetch_backend: args.fetch_backend.clone(),
                    },
                ],
                _ => vec![
                    TrialCfg {
                        id: "non_agentic",
                        agentic: false,
                        agentic_selector: "lexical",
                        url_selection_mode: "preserve",
                        fetch_backend: args.fetch_backend.clone(),
                    },
                    TrialCfg {
                        id: "agentic_lexical",
                        agentic: true,
                        agentic_selector: "lexical",
                        url_selection_mode: "query_rank",
                        fetch_backend: args.fetch_backend.clone(),
                    },
                ],
            };

            // Optional presets override file (cheap prompt iteration without recompiling).
            let mut preset_overrides: std::collections::BTreeMap<String, JudgePreset> =
                std::collections::BTreeMap::new();
            if let Some(p) = args.judge_presets_json.as_ref() {
                let raw = std::fs::read_to_string(p)?;
                let v: serde_json::Value = serde_json::from_str(&raw)?;
                let ok = v.get("schema_version").and_then(|x| x.as_u64()) == Some(1)
                    && v.get("kind").and_then(|x| x.as_str()) == Some("webpipe_judge_presets");
                if !ok {
                    anyhow::bail!("judge_presets_json: unexpected kind/schema_version");
                }
                if let Some(ps) = v.get("presets").and_then(|x| x.as_array()) {
                    for pr in ps {
                        let id = pr.get("id").and_then(|x| x.as_str()).unwrap_or("").trim();
                        let system = pr
                            .get("system")
                            .and_then(|x| x.as_str())
                            .unwrap_or("")
                            .trim();
                        if id.is_empty() || system.is_empty() {
                            continue;
                        }
                        let domain_tags = pr
                            .get("domain_tags")
                            .and_then(|x| x.as_array())
                            .map(|a| {
                                a.iter()
                                    .filter_map(|x| x.as_str())
                                    .map(|s| s.trim().to_string())
                                    .filter(|s| !s.is_empty())
                                    .collect::<Vec<String>>()
                            })
                            .unwrap_or_default();
                        preset_overrides.insert(
                            id.to_ascii_lowercase(),
                            JudgePreset {
                                id: id.to_string(),
                                domain_tags,
                                system: system.to_string(),
                            },
                        );
                    }
                }
            }

            // Select judge presets (built-ins, optionally overridden by judge_presets_json).
            let mut preset_by_id = builtin_judge_presets();
            for (k, v) in preset_overrides {
                preset_by_id.insert(k, v);
            }

            // Optional curriculum: use a prior run artifact to filter/sort query selection.
            let curriculum_mode = args.curriculum.to_ascii_lowercase();
            let curriculum: std::collections::BTreeMap<String, CurriculumStats> =
                if curriculum_mode != "none" {
                    if let Some(p) = args.curriculum_from.as_ref() {
                        load_curriculum(p.as_path()).unwrap_or_default()
                    } else {
                        std::collections::BTreeMap::new()
                    }
                } else {
                    std::collections::BTreeMap::new()
                };

            let judge_ids = parse_csv(&args.judges);
            let mut judges: Vec<JudgePreset> = Vec::new();
            for id in judge_ids {
                let jid = id.to_ascii_lowercase();
                let Some(p) = preset_by_id.get(&jid).cloned() else {
                    anyhow::bail!(
                        "unknown judge preset: {jid} (known: ux, writing, rendered_page, math, redteam, docs, academic, news)"
                    );
                };
                judges.push(p);
            }
            if judges.is_empty() {
                anyhow::bail!("no judges selected");
            }

            let mut judge_reports: Vec<serde_json::Value> = Vec::new();
            // Deliberately no shared “global notes” channel across judges/queries.

            for preset in judges {
                // Filter queries by domain tags (CLI filter wins; else preset tags).
                let mut selected: Vec<&eval::E2eQueryV1> = Vec::new();
                for q in e2e.queries.iter() {
                    let q_tags: Vec<&str> = q.tags.iter().map(|s| s.as_str()).collect();
                    let want = if !domain_tags_filter.is_empty() {
                        domain_tags_filter
                            .iter()
                            .any(|t| q_tags.iter().any(|qt| qt.eq_ignore_ascii_case(t)))
                    } else {
                        preset
                            .domain_tags
                            .iter()
                            .any(|t| q_tags.iter().any(|qt| qt.eq_ignore_ascii_case(t.as_str())))
                    };
                    if want {
                        selected.push(q);
                    }
                }
                if selected.is_empty() {
                    // Fallback: judge still runs on all queries if no tags match.
                    selected = e2e.queries.iter().collect();
                }

                // Curriculum filter: unseen/failed subsets.
                if curriculum_mode == "unseen" && !curriculum.is_empty() {
                    selected.retain(|q| !curriculum.contains_key(&q.query_id));
                } else if curriculum_mode == "failed" && !curriculum.is_empty() {
                    selected.retain(|q| curriculum.get(&q.query_id).is_some_and(|st| !st.worked));
                }

                // Deterministic ordering by (hash(seed+judge_id, query_id), query_id).
                let judge_seed = hash64(args.seed, preset.id.as_str());
                selected.sort_by(|a, b| {
                    let ha = hash64(judge_seed, &a.query_id);
                    let hb = hash64(judge_seed, &b.query_id);
                    let base = ha.cmp(&hb).then_with(|| a.query_id.cmp(&b.query_id));
                    // Curriculum ordering: use prior best_score when requested; keep deterministic tie-break.
                    if curriculum_mode == "harder" && !curriculum.is_empty() {
                        let sa = curriculum
                            .get(&a.query_id)
                            .map(|s| s.best_score)
                            .unwrap_or(0.0);
                        let sb = curriculum
                            .get(&b.query_id)
                            .map(|s| s.best_score)
                            .unwrap_or(0.0);
                        sa.partial_cmp(&sb)
                            .unwrap_or(std::cmp::Ordering::Equal)
                            .then(base)
                    } else if curriculum_mode == "easier" && !curriculum.is_empty() {
                        let sa = curriculum
                            .get(&a.query_id)
                            .map(|s| s.best_score)
                            .unwrap_or(0.0);
                        let sb = curriculum
                            .get(&b.query_id)
                            .map(|s| s.best_score)
                            .unwrap_or(0.0);
                        sb.partial_cmp(&sa)
                            .unwrap_or(std::cmp::Ordering::Equal)
                            .then(base)
                    } else {
                        base
                    }
                });
                selected.truncate(args.max_queries.clamp(1, 10_000));

                let mut per_query: Vec<serde_json::Value> = Vec::new();
                let mut totals = serde_json::json!({
                    "queries": 0u64,
                    "trials": 0u64,
                    "llm_failed": 0u64,
                    "llm_parse_failed": 0u64,
                    "llm_ok": 0u64,
                    "hit_expected_url_any_trial": 0u64
                });

                // Judge prompts should be *stateless* to avoid cross-query “note bleed”.

                let task_solve_enabled = args.task_solve;
                let task_solve_max_trials = args.task_solve_max_trials.clamp(1, 5);
                let task_solve_max_evidence_chars =
                    args.task_solve_max_evidence_chars.clamp(200, 5_000);

                let task_solver_system = r#"You are an evidence-only task solver.

You will receive a user query and several tool trials, each with:
- trial_id
- observed_urls
- warnings
- quality (optional)
- evidence_text (bounded)

Rules:
- Use ONLY the evidence_text and observed_urls. Do not use outside knowledge.
- If evidence is insufficient, abstain and explain what is missing (briefly).
- Output ONE JSON object only (no markdown, no prose) with schema:
{
  "best_trial_id": string,
  "answer": string,              // required if evidence_sufficient=true
  "abstain_reason": string,      // required if evidence_sufficient=false
  "evidence_sufficient": boolean,
  "confidence": number,          // 0..1
  "citations": [
    { "url": string, "quote": string }
  ]
}"#;

                let mut solver_history: Vec<webpipe_local::openai_compat::ChatMessage> = vec![
                    webpipe_local::openai_compat::ChatMessage::system(task_solver_system),
                    webpipe_local::openai_compat::ChatMessage::assistant(
                        "Follow the schema exactly. If you abstain, still pick best_trial_id based on evidence quality.",
                    ),
                ];

                for q in selected {
                    let urls: Vec<String> = q
                        .url_paths
                        .iter()
                        .map(|p| {
                            let p = p.trim();
                            if p.starts_with("http://") || p.starts_with("https://") {
                                p.to_string()
                            } else {
                                format!("{base_url}/{}", p.trim_start_matches('/'))
                            }
                        })
                        .collect();

                    let expected = qrels_by_qid.get(&q.query_id).cloned().unwrap_or_default();

                    let mut trial_rows: Vec<serde_json::Value> = Vec::new();
                    let mut hit_any = false;
                    let mut solver_trials: Vec<serde_json::Value> = Vec::new();
                    let mut solver_best: Option<(f64, String)> = None;

                    for t in &trials {
                        let t0 = std::time::Instant::now();
                        // IMPORTANT: even in-process, go through the same “exposed tool” shape:
                        // JSON args → serde decode → tool method → CallToolResult JSON string.
                        let args_json = serde_json::json!({
                            "query": q.query,
                            "urls": urls,
                            "provider": args.provider,
                            "auto_mode": args.auto_mode,
                            "selection_mode": args.selection_mode,
                            "fetch_backend": t.fetch_backend,
                            "no_network": false,
                            "max_results": max_results,
                            "max_urls": max_urls,
                            "timeout_ms": args.timeout_ms,
                            "max_bytes": args.max_bytes,
                            "top_chunks": top_chunks,
                            "max_chunk_chars": max_chunk_chars,
                            "include_links": false,
                            "include_text": false,
                            "agentic": t.agentic,
                            "agentic_selector": t.agentic_selector,
                            "url_selection_mode": t.url_selection_mode,
                            "cache_read": true,
                            "cache_write": true
                        });
                        // `from_value` consumes the JSON; keep a copy for optional transcript logging.
                        let tool_args: mcp::WebSearchExtractArgs =
                            serde_json::from_value(args_json.clone())?;
                        let res = svc
                            .web_search_extract(Parameters(Some(tool_args)))
                            .await
                            .map_err(|e| anyhow::anyhow!(e.to_string()))?;

                        let res_json: serde_json::Value = if let Some(v) =
                            res.structured_content.clone()
                        {
                            v
                        } else {
                            // Back-compat: older tool results echoed JSON as the first `content` text item.
                            // Newer tools may include Markdown first, so scan for any parseable JSON text.
                            let mut parsed: Option<serde_json::Value> = None;
                            for c in &res.content {
                                let s = c.as_text().map(|t| t.text.as_str()).unwrap_or("");
                                let s = s.trim();
                                if s.is_empty() {
                                    continue;
                                }
                                if let Ok(v) = serde_json::from_str::<serde_json::Value>(s) {
                                    parsed = Some(v);
                                    break;
                                }
                            }
                            parsed.ok_or_else(|| anyhow::anyhow!("web_search_extract tool result had no structured_content and no JSON text payload"))?
                        };
                        if transcript_enabled {
                            transcript_seq = transcript_seq.wrapping_add(1);
                            // IMPORTANT: keep transcript tool results parseable.
                            // Full `res_json` can be large; `json_trunc` will truncate the serialized JSON
                            // (making it unparseable). Instead, log:
                            // - args (small)
                            // - a compact, stable `result_summary` object
                            // - warning/quality signals used by downstream judging
                            let results_count = res_json
                                .get("results")
                                .and_then(|v| v.as_array())
                                .map(|a| a.len())
                                .unwrap_or(0);
                            let top_chunks_count = res_json
                                .get("top_chunks")
                                .and_then(|v| v.as_array())
                                .map(|a| a.len())
                                .unwrap_or(0);
                            let provider_eff = res_json
                                .get("provider")
                                .and_then(|v| v.as_str())
                                .unwrap_or("");
                            let fetch_backend_eff = res_json
                                .get("fetch_backend")
                                .and_then(|v| v.as_str())
                                .unwrap_or("");
                            let ok = res_json
                                .get("ok")
                                .and_then(|v| v.as_bool())
                                .unwrap_or(false);

                            let line = serde_json::json!({
                                "schema_version": 1,
                                "kind": "webpipe_eval_transcript_event",
                                "generated_at_epoch_s": now,
                                "seq": transcript_seq,
                                "run_kind": "eval_judge_swarm",
                                "stage": "tool",
                                "call_id": format!("tool:{}:{}:{}", preset.id, q.query_id, t.id),
                                "tool": {
                                    "name": "web_search_extract",
                                    "args": json_trunc(&args_json, transcript_max_chars),
                                    "result_summary": {
                                        "ok": ok,
                                        "provider": provider_eff,
                                        "fetch_backend": fetch_backend_eff,
                                        "results_count": results_count,
                                        "top_chunks_count": top_chunks_count
                                    },
                                    // Keep these: they are small but high-signal.
                                    "warnings": res_json.get("warnings").cloned().unwrap_or(serde_json::Value::Null),
                                    "quality": res_json.get("quality").cloned().unwrap_or(serde_json::Value::Null),
                                    "elapsed_ms": t0.elapsed().as_millis()
                                }
                            });
                            let _ = append_jsonl(transcript_path.as_path(), &line);
                        }

                        // Compact judge text: join top_chunks.text, then truncate.
                        let mut joined = String::new();
                        if let Some(top) = res_json.get("top_chunks").and_then(|v| v.as_array()) {
                            for (idx, c) in top.iter().enumerate() {
                                if let Some(tx) = c.get("text").and_then(|v| v.as_str()) {
                                    if idx > 0 {
                                        joined.push('\n');
                                        joined.push('\n');
                                    }
                                    joined.push_str(tx);
                                }
                            }
                        }
                        let (judge_text, judge_text_truncated) = take_chars(&joined, 2500);
                        let (solver_evidence_text, _) =
                            take_chars(&judge_text, task_solve_max_evidence_chars);

                        // Observed URLs.
                        let mut observed_urls: Vec<String> = Vec::new();
                        if let Some(rs) = res_json.get("results").and_then(|v| v.as_array()) {
                            for r in rs {
                                if let Some(u) = r.get("url").and_then(|v| v.as_str()) {
                                    observed_urls.push(u.to_string());
                                }
                            }
                        }

                        let matched = expected
                            .iter()
                            .find(|needle| {
                                observed_urls.iter().any(|u| u.contains(needle.as_str()))
                            })
                            .cloned();
                        let hit_expected_url = matched.is_some();
                        if hit_expected_url {
                            hit_any = true;
                        }

                        let quality = res_json
                            .get("quality")
                            .cloned()
                            .unwrap_or(serde_json::Value::Null);
                        let warnings: Vec<String> = res_json
                            .get("warnings")
                            .and_then(|v| v.as_array())
                            .map(|a| {
                                a.iter()
                                    .filter_map(|x| x.as_str().map(|s| s.to_string()))
                                    .collect()
                            })
                            .unwrap_or_default();

                        // Ask the judge to score this trial (stateless).
                        // Include boundedness/truncation metadata explicitly.
                        let user_prompt = format!(
                            "Query:\n{}\n\nTrial:\n{}\n\nObserved URLs:\n{}\n\nWarnings:\n{}\n\nQuality (if present):\n{}\n\nEvidence (judge_text; chars={} truncated={}):\n{}\n",
                            q.query.trim(),
                            t.id,
                            observed_urls.join("\n"),
                            warnings.join(", "),
                            serde_json::to_string_pretty(&quality).unwrap_or_default(),
                            judge_text.chars().count(),
                            judge_text_truncated,
                            judge_text.trim()
                        );

                        let messages = vec![
                            webpipe_local::openai_compat::ChatMessage::system(
                                preset.system.as_str(),
                            ),
                            webpipe_local::openai_compat::ChatMessage::user(&user_prompt),
                        ];

                        let mut llm_error: Option<String> = None;
                        let llm_t0 = std::time::Instant::now();
                        let mut resp_text = match llm_call(
                            llm_backend.as_str(),
                            openai_like.as_ref(),
                            ollama.as_ref(),
                            openrouter_caps.as_ref(),
                            &chat_options,
                            messages.clone(),
                            llm_timeout_ms,
                            temperature,
                            top_p,
                            json_mode,
                        )
                        .await
                        {
                            Ok(s) => s,
                            Err(e) => {
                                llm_error = Some(e.to_string());
                                String::new()
                            }
                        };

                        let mut parsed = extract_json_object(&resp_text);
                        // Hard-remove any free-form “notes” channel if the model emits it anyway.
                        if let Some(o) = parsed.as_mut().and_then(|v| v.as_object_mut()) {
                            let _ = o.remove("notes");
                        }
                        if transcript_enabled {
                            let evidence_chars = judge_text.chars().count();
                            let observed_url_count = observed_urls.len();
                            let warnings_count = warnings.len();
                            transcript_seq = transcript_seq.wrapping_add(1);
                            let line = serde_json::json!({
                                "schema_version": 1,
                                "kind": "webpipe_eval_transcript_event",
                                "generated_at_epoch_s": now,
                                "seq": transcript_seq,
                                "run_kind": "eval_judge_swarm",
                                "stage": "judge",
                                "call_id": format!("judge:{}:{}:{}", preset.id, q.query_id, t.id),
                                "attempt": 1,
                                "context": {
                                    "observed_url_count": observed_url_count,
                                    "warnings_count": warnings_count,
                                    "evidence_chars": evidence_chars,
                                    "evidence_truncated": judge_text_truncated,
                                },
                                "llm": {
                                    "backend": llm_backend,
                                    "model_effective": llm_model_effective,
                                    "json_mode": json_mode,
                                    "timeout_ms": llm_timeout_ms
                                },
                                "prompt": {
                                    "system": tx_trunc(preset.system.as_str(), transcript_max_chars),
                                    "user": tx_trunc(&user_prompt, transcript_max_chars)
                                },
                                "response": {
                                    "raw": tx_trunc(&resp_text, transcript_max_chars),
                                    "parsed": parsed,
                                    "error": llm_error
                                },
                                "timing_ms": llm_t0.elapsed().as_millis()
                            });
                            let _ = append_jsonl(transcript_path.as_path(), &line);
                        }
                        if parsed.as_ref().is_some_and(validate_scorecard) {
                            // ok
                        } else if retry_on_parse_fail
                            && openai_like.is_some()
                            && llm_error.is_none()
                        {
                            let (prev, _) = take_chars(&resp_text, 900);
                            let retry_prompt = format!(
                                "Your previous output was not valid JSON matching the schema.\nReturn ONLY one JSON object matching the schema.\n\nPrevious output:\n{}\n\n(Repeat)\n{}",
                                prev.trim(),
                                user_prompt
                            );
                            let llm_t1 = std::time::Instant::now();
                            resp_text = llm_call(
                                llm_backend.as_str(),
                                openai_like.as_ref(),
                                ollama.as_ref(),
                                openrouter_caps.as_ref(),
                                &chat_options,
                                vec![
                                    webpipe_local::openai_compat::ChatMessage::system(
                                        preset.system.as_str(),
                                    ),
                                    webpipe_local::openai_compat::ChatMessage::user(&retry_prompt),
                                ],
                                llm_timeout_ms,
                                temperature,
                                top_p,
                                json_mode,
                            )
                            .await
                            .unwrap_or_default();
                            parsed = extract_json_object(&resp_text);
                            if let Some(o) = parsed.as_mut().and_then(|v| v.as_object_mut()) {
                                let _ = o.remove("notes");
                            }
                            parsed = parsed.filter(validate_scorecard);
                            if transcript_enabled {
                                let evidence_chars = judge_text.chars().count();
                                let observed_url_count = observed_urls.len();
                                let warnings_count = warnings.len();
                                transcript_seq = transcript_seq.wrapping_add(1);
                                let line = serde_json::json!({
                                    "schema_version": 1,
                                    "kind": "webpipe_eval_transcript_event",
                                    "generated_at_epoch_s": now,
                                    "seq": transcript_seq,
                                    "run_kind": "eval_judge_swarm",
                                    "stage": "judge",
                                    "call_id": format!("judge:{}:{}:{}", preset.id, q.query_id, t.id),
                                    "attempt": 2,
                                    "context": {
                                        "observed_url_count": observed_url_count,
                                        "warnings_count": warnings_count,
                                        "evidence_chars": evidence_chars,
                                        "evidence_truncated": judge_text_truncated,
                                    },
                                    "llm": {
                                        "backend": llm_backend,
                                        "model_effective": llm_model_effective,
                                        "json_mode": json_mode,
                                        "timeout_ms": llm_timeout_ms
                                    },
                                    "prompt": {
                                        "system": tx_trunc(preset.system.as_str(), transcript_max_chars),
                                        "user": tx_trunc(&retry_prompt, transcript_max_chars)
                                    },
                                    "response": {
                                        "raw": tx_trunc(&resp_text, transcript_max_chars),
                                        "parsed": parsed
                                    },
                                    "timing_ms": llm_t1.elapsed().as_millis()
                                });
                                let _ = append_jsonl(transcript_path.as_path(), &line);
                            }
                        }

                        totals["trials"] =
                            serde_json::json!(totals["trials"].as_u64().unwrap_or(0) + 1);
                        if llm_error.is_some() {
                            totals["llm_failed"] = serde_json::json!(
                                totals
                                    .get("llm_failed")
                                    .and_then(|v| v.as_u64())
                                    .unwrap_or(0)
                                    + 1
                            );
                        } else if parsed.is_none() {
                            totals["llm_parse_failed"] = serde_json::json!(
                                totals["llm_parse_failed"].as_u64().unwrap_or(0) + 1
                            );
                        } else {
                            totals["llm_ok"] =
                                serde_json::json!(totals["llm_ok"].as_u64().unwrap_or(0) + 1);
                        }

                        trial_rows.push(serde_json::json!({
                            "trial_id": t.id,
                            "elapsed_ms": t0.elapsed().as_millis(),
                            "agentic": t.agentic,
                            "agentic_selector": t.agentic_selector,
                            "fetch_backend": t.fetch_backend,
                            "url_selection_mode": t.url_selection_mode,
                            "ok": res_json.get("ok").and_then(|v| v.as_bool()).unwrap_or(false),
                            "observed_urls": observed_urls,
                            "expected_url_substrings": expected,
                            "hit_expected_url": hit_expected_url,
                            "matched_substring": matched,
                            "warnings": warnings,
                            "quality": quality,
                            "judge_text_truncated": judge_text_truncated,
                            "judge": parsed,
                            "judge_raw": if llm_error.is_some() { llm_error.unwrap_or_default() } else if parsed.is_none() { resp_text } else { String::new() }
                        }));

                        if task_solve_enabled {
                            let score = trial_rows
                                .last()
                                .and_then(|r| r.get("judge"))
                                .and_then(|j| j.get("overall_score"))
                                .and_then(|x| x.as_f64())
                                .unwrap_or(0.0);
                            solver_trials.push(serde_json::json!({
                                "trial_id": t.id,
                                "score": score,
                                "observed_urls": trial_rows.last().and_then(|r| r.get("observed_urls")).cloned().unwrap_or(serde_json::Value::Null),
                                "warnings": trial_rows.last().and_then(|r| r.get("warnings")).cloned().unwrap_or(serde_json::Value::Null),
                                "quality": trial_rows.last().and_then(|r| r.get("quality")).cloned().unwrap_or(serde_json::Value::Null),
                                "evidence_text": solver_evidence_text
                            }));
                            if solver_best
                                .as_ref()
                                .map(|(s, _)| score > *s)
                                .unwrap_or(true)
                            {
                                solver_best = Some((score, t.id.to_string()));
                            }
                        }

                        if let Some(j) = trial_rows
                            .last()
                            .and_then(|r| r.get("judge"))
                            .and_then(|v| v.as_object())
                        {
                            let score = j
                                .get("overall_score")
                                .and_then(|x| x.as_f64())
                                .unwrap_or(0.0);
                            let relevant =
                                j.get("relevant").and_then(|x| x.as_bool()).unwrap_or(false);
                            let answerable = j
                                .get("answerable")
                                .and_then(|x| x.as_bool())
                                .unwrap_or(false);
                            let _ = (score, relevant, answerable);
                        }
                    }

                    // Optional: evidence-only task solver (picks best trial + answers or abstains).
                    let mut task_solve_raw = String::new();
                    let mut task_solve: Option<serde_json::Value> = None;
                    if task_solve_enabled {
                        let mut st = solver_trials.clone();
                        st.sort_by(|a, b| {
                            let sa = a.get("score").and_then(|x| x.as_f64()).unwrap_or(0.0);
                            let sb = b.get("score").and_then(|x| x.as_f64()).unwrap_or(0.0);
                            sb.partial_cmp(&sa).unwrap_or(std::cmp::Ordering::Equal)
                        });
                        st.truncate(task_solve_max_trials);

                        let best_hint = solver_best
                            .as_ref()
                            .map(|(_, id)| id.as_str())
                            .unwrap_or("unknown");
                        let user_prompt = format!(
                            "User query:\n{}\n\nBest-trial hint (from per-trial judge scores): {}\n\nTrials (bounded):\n{}\n",
                            q.query.trim(),
                            best_hint,
                            serde_json::to_string_pretty(&st).unwrap_or_default()
                        );
                        solver_history.push(webpipe_local::openai_compat::ChatMessage::user(
                            &user_prompt,
                        ));
                        let solve_t0 = std::time::Instant::now();
                        task_solve_raw = llm_call(
                            llm_backend.as_str(),
                            openai_like.as_ref(),
                            ollama.as_ref(),
                            openrouter_caps.as_ref(),
                            &chat_options,
                            solver_history.clone(),
                            llm_timeout_ms,
                            temperature,
                            top_p,
                            json_mode,
                        )
                        .await
                        .unwrap_or_default();
                        task_solve =
                            extract_json_object(&task_solve_raw).filter(validate_task_solve);
                        if transcript_enabled {
                            transcript_seq = transcript_seq.wrapping_add(1);
                            let line = serde_json::json!({
                                "schema_version": 1,
                                "kind": "webpipe_eval_transcript_event",
                                "generated_at_epoch_s": now,
                                "seq": transcript_seq,
                                "run_kind": "eval_judge_swarm",
                                "stage": "task_solve",
                                "call_id": format!("solve:{}:{}", preset.id, q.query_id),
                                "attempt": 1,
                                "llm": {
                                    "backend": llm_backend,
                                    "model_effective": llm_model_effective,
                                    "json_mode": json_mode,
                                    "timeout_ms": llm_timeout_ms
                                },
                                "prompt": {
                                    "system": tx_trunc(task_solver_system, transcript_max_chars),
                                    "user": tx_trunc(&user_prompt, transcript_max_chars)
                                },
                                "response": {
                                    "raw": tx_trunc(&task_solve_raw, transcript_max_chars),
                                    "parsed": task_solve
                                },
                                "timing_ms": solve_t0.elapsed().as_millis()
                            });
                            let _ = append_jsonl(transcript_path.as_path(), &line);
                        }
                        if task_solve.is_none() && retry_on_parse_fail && openai_like.is_some() {
                            let (prev, _) = take_chars(&task_solve_raw, 900);
                            let retry = format!(
                                "Return ONLY one JSON object matching the schema.\nPrevious output:\n{}\n\n(Repeat)\n{}",
                                prev.trim(),
                                user_prompt
                            );
                            solver_history
                                .push(webpipe_local::openai_compat::ChatMessage::user(&retry));
                            let solve_t1 = std::time::Instant::now();
                            task_solve_raw = llm_call(
                                llm_backend.as_str(),
                                openai_like.as_ref(),
                                ollama.as_ref(),
                                openrouter_caps.as_ref(),
                                &chat_options,
                                solver_history.clone(),
                                llm_timeout_ms,
                                temperature,
                                top_p,
                                json_mode,
                            )
                            .await
                            .unwrap_or_default();
                            task_solve =
                                extract_json_object(&task_solve_raw).filter(validate_task_solve);
                            if transcript_enabled {
                                transcript_seq = transcript_seq.wrapping_add(1);
                                let line = serde_json::json!({
                                    "schema_version": 1,
                                    "kind": "webpipe_eval_transcript_event",
                                    "generated_at_epoch_s": now,
                                    "seq": transcript_seq,
                                    "run_kind": "eval_judge_swarm",
                                    "stage": "task_solve",
                                    "call_id": format!("solve:{}:{}:retry", preset.id, q.query_id),
                                    "attempt": 2,
                                    "llm": {
                                        "backend": llm_backend,
                                        "model_effective": llm_model_effective,
                                        "json_mode": json_mode,
                                        "timeout_ms": llm_timeout_ms
                                    },
                                    "prompt": {
                                        "system": tx_trunc(task_solver_system, transcript_max_chars),
                                        "user": tx_trunc(&retry, transcript_max_chars)
                                    },
                                    "response": {
                                        "raw": tx_trunc(&task_solve_raw, transcript_max_chars),
                                        "parsed": task_solve
                                    },
                                    "timing_ms": solve_t1.elapsed().as_millis()
                                });
                                let _ = append_jsonl(transcript_path.as_path(), &line);
                            }
                        }
                        if !task_solve_raw.trim().is_empty() {
                            let (resp2, _) = take_chars(&task_solve_raw, 1800);
                            solver_history.push(
                                webpipe_local::openai_compat::ChatMessage::assistant(resp2.trim()),
                            );
                        }
                    }

                    totals["queries"] =
                        serde_json::json!(totals["queries"].as_u64().unwrap_or(0) + 1);
                    if hit_any {
                        totals["hit_expected_url_any_trial"] = serde_json::json!(
                            totals["hit_expected_url_any_trial"].as_u64().unwrap_or(0) + 1
                        );
                    }

                    per_query.push(serde_json::json!({
                        "query_id": q.query_id,
                        "query": q.query,
                        "tags": q.tags,
                        "urls": urls,
                        "trials": trial_rows,
                        "hit_expected_url_any_trial": hit_any,
                        "task_solve": task_solve,
                        "task_solve_raw": if task_solve.is_none() { task_solve_raw } else { String::new() }
                    }));
                }

                // Per-judge overall assessment (one more "turn").
                let judge_summary_prompt = {
                    let mut compact: Vec<serde_json::Value> = Vec::new();
                    for pq in &per_query {
                        let qid = pq
                            .get("query_id")
                            .cloned()
                            .unwrap_or(serde_json::Value::Null);
                        let query = pq.get("query").cloned().unwrap_or(serde_json::Value::Null);
                        let hit_any = pq
                            .get("hit_expected_url_any_trial")
                            .cloned()
                            .unwrap_or(serde_json::Value::Null);
                        let mut trial_scores: Vec<serde_json::Value> = Vec::new();
                        if let Some(ts) = pq.get("trials").and_then(|v| v.as_array()) {
                            for t in ts {
                                let trial_id = t.get("trial_id").cloned().unwrap_or_default();
                                let ok = t.get("ok").cloned().unwrap_or_default();
                                let hit = t.get("hit_expected_url").cloned().unwrap_or_default();
                                let score = t
                                    .get("judge")
                                    .and_then(|j| j.get("overall_score"))
                                    .cloned()
                                    .unwrap_or(serde_json::Value::Null);
                                let issues = t
                                    .get("judge")
                                    .and_then(|j| j.get("issues"))
                                    .cloned()
                                    .unwrap_or(serde_json::Value::Null);
                                trial_scores.push(serde_json::json!({
                                    "trial_id": trial_id,
                                    "ok": ok,
                                    "hit_expected_url": hit,
                                    "overall_score": score,
                                    "issues": issues
                                }));
                            }
                        }
                        compact.push(serde_json::json!({
                            "query_id": qid,
                            "query": query,
                            "hit_expected_url_any_trial": hit_any,
                            "trials": trial_scores
                        }));
                    }
                    format!(
                        "Summarize the judge's findings across queries.\nReturn ONE JSON object ONLY with schema:\n{{\"overall_assessment\": string, \"top_failure_modes\": string[], \"recommended_defaults\": object, \"confidence\": number}}\n\nData:\n{}\n",
                        serde_json::to_string_pretty(&compact).unwrap_or_default()
                    )
                };
                let judge_overall_system = r#"You are a strict evaluator. Output strict JSON only.

Return ONE JSON object ONLY with exactly these keys:
{
  "overall_assessment": string,
  "top_failure_modes": string[],
  "recommended_defaults": object,
  "confidence": number
}

Rules:
- "top_failure_modes" MUST be an array of strings from this controlled vocabulary (use [] if none):
  ["missing_key_facts","too_short","boilerplate","low_signal","off_topic","truncated","did_not_hit_expected_url"]
- "recommended_defaults" MUST be an object (use {} if none).
- Do not include keys like "overall_score", "relevant", "answerable", "issues", "notes"."#;
                let judge_overall_t0 = std::time::Instant::now();
                let mut judge_overall_raw = llm_call(
                    llm_backend.as_str(),
                    openai_like.as_ref(),
                    ollama.as_ref(),
                    openrouter_caps.as_ref(),
                    &chat_options,
                    vec![
                        webpipe_local::openai_compat::ChatMessage::system(judge_overall_system),
                        webpipe_local::openai_compat::ChatMessage::user(&judge_summary_prompt),
                    ],
                    llm_timeout_ms,
                    temperature,
                    top_p,
                    json_mode,
                )
                .await
                .unwrap_or_default();
                let mut judge_overall =
                    extract_json_object(&judge_overall_raw).filter(validate_judge_overall);
                if transcript_enabled {
                    transcript_seq = transcript_seq.wrapping_add(1);
                    let line = serde_json::json!({
                        "schema_version": 1,
                        "kind": "webpipe_eval_transcript_event",
                        "generated_at_epoch_s": now,
                        "seq": transcript_seq,
                        "run_kind": "eval_judge_swarm",
                        "stage": "judge_overall",
                        "call_id": format!("judge_overall:{}:attempt1", preset.id),
                        "llm": {
                            "backend": llm_backend,
                            "model_effective": llm_model_effective,
                            "json_mode": json_mode,
                            "timeout_ms": llm_timeout_ms
                        },
                        "prompt": {
                            "system": tx_trunc(judge_overall_system, transcript_max_chars),
                            "user": tx_trunc(&judge_summary_prompt, transcript_max_chars)
                        },
                        "response": {
                            "raw": tx_trunc(&judge_overall_raw, transcript_max_chars),
                            "parsed": judge_overall
                        },
                        "timing_ms": judge_overall_t0.elapsed().as_millis()
                    });
                    let _ = append_jsonl(transcript_path.as_path(), &line);
                }
                if judge_overall.is_none() && retry_on_parse_fail && openai_like.is_some() {
                    let (prev, _) = take_chars(&judge_overall_raw, 900);
                    let retry = format!(
                        "Return ONLY one JSON object matching the schema.\nPrevious output:\n{}\n",
                        prev.trim()
                    );
                    let judge_overall_t1 = std::time::Instant::now();
                    judge_overall_raw = llm_call(
                        llm_backend.as_str(),
                        openai_like.as_ref(),
                        ollama.as_ref(),
                        openrouter_caps.as_ref(),
                        &chat_options,
                        vec![
                            webpipe_local::openai_compat::ChatMessage::system(judge_overall_system),
                            webpipe_local::openai_compat::ChatMessage::user(&retry),
                        ],
                        llm_timeout_ms,
                        temperature,
                        top_p,
                        json_mode,
                    )
                    .await
                    .unwrap_or_default();
                    judge_overall =
                        extract_json_object(&judge_overall_raw).filter(validate_judge_overall);
                    if transcript_enabled {
                        transcript_seq = transcript_seq.wrapping_add(1);
                        let line = serde_json::json!({
                            "schema_version": 1,
                            "kind": "webpipe_eval_transcript_event",
                            "generated_at_epoch_s": now,
                            "seq": transcript_seq,
                            "run_kind": "eval_judge_swarm",
                            "stage": "judge_overall",
                            "call_id": format!("judge_overall:{}:attempt2", preset.id),
                            "llm": {
                                "backend": llm_backend,
                                "model_effective": llm_model_effective,
                                "json_mode": json_mode,
                                "timeout_ms": llm_timeout_ms
                            },
                            "prompt": {
                                "system": tx_trunc(judge_overall_system, transcript_max_chars),
                                "user": tx_trunc(&retry, transcript_max_chars)
                            },
                            "response": {
                                "raw": tx_trunc(&judge_overall_raw, transcript_max_chars),
                                "parsed": judge_overall
                            },
                            "timing_ms": judge_overall_t1.elapsed().as_millis()
                        });
                        let _ = append_jsonl(transcript_path.as_path(), &line);
                    }
                }

                judge_reports.push(serde_json::json!({
                    "schema_version": 1,
                    "kind": "webpipe_eval_judge_agent_report",
                    "generated_at_epoch_s": now,
                    "judge_id": preset.id,
                    "inputs": {
                        "queries_json": args.queries_json,
                        "qrels": args.qrels,
                        "base_url": args.base_url,
                        "provider": args.provider,
                        "auto_mode": args.auto_mode,
                        "selection_mode": args.selection_mode,
                        "fetch_backend": args.fetch_backend,
                        "trial_set": args.trial_set,
                        "judge_presets_json": args.judge_presets_json,
                        "max_queries": args.max_queries,
                        "seed": args.seed,
                        "llm_backend": llm_backend,
                        "llm_model": args.llm_model,
                        "llm_model_effective": llm_model_effective,
                        "llm_timeout_ms": llm_timeout_ms,
                        "json_mode": json_mode,
                        "retry_on_parse_fail": retry_on_parse_fail
                    },
                    "totals": totals,
                    "overall": judge_overall,
                    "overall_raw": if judge_overall.is_none() { judge_overall_raw } else { String::new() },
                    "per_query": per_query
                }));
            }

            // Meta-judge (deterministic aggregation + optional LLM synthesis).
            let meta_summary = {
                let mut compact: Vec<serde_json::Value> = Vec::new();
                for jr in &judge_reports {
                    compact.push(serde_json::json!({
                        "judge_id": jr.get("judge_id").cloned().unwrap_or(serde_json::Value::Null),
                        "totals": jr.get("totals").cloned().unwrap_or(serde_json::Value::Null),
                        "overall": jr.get("overall").cloned().unwrap_or(serde_json::Value::Null),
                        "per_query": jr.get("per_query").cloned().unwrap_or(serde_json::Value::Null),
                    }));
                }
                let meta_schema_reminder = r#"Return ONE JSON object ONLY with exactly these keys:
{
  "overall_assessment": string,
  "cross_judge_agreement": string,
  "top_systemic_failures": string[],
  "top_3_fixes": string[],
  "recommended_next_experiments": string[],
  "top_dimensions": string[],
  "top_musings": string[]
}

Rules:
- "top_systemic_failures", "top_3_fixes", and "recommended_next_experiments" MUST be arrays (use [] if none).
- "top_systemic_failures" MUST use ONLY this controlled vocabulary (use [] if none):
  ["missing_key_facts","too_short","boilerplate","low_signal","off_topic","truncated","did_not_hit_expected_url"]
- "top_dimensions" MUST be an array of <=12 short strings (<=32 chars); deduplicate.
- "top_musings" MUST be an array of <=12 short strings (<=96 chars); focus on cross-judge themes from musings/good/bad/fixes.
- For "top_3_fixes", deduplicate and prioritize concrete, tool-shaped fixes (max 3).
- Each string in "top_3_fixes" and "recommended_next_experiments" MUST be <= 96 chars; avoid paragraphs.
- Do not include keys like "overall_score", "issues", "confidence"."#;
                format!(
                        "You are a meta-judge. Aggregate multiple judge reports into one evaluation summary.\n{}\n\nJudge reports (compact):\n{}\n",
                        meta_schema_reminder,
                    serde_json::to_string_pretty(&compact).unwrap_or_default()
                )
            };
            let meta_system = r#"You are a strict meta-judge. Output strict JSON only and follow the schema exactly."#;
            let meta_t0 = std::time::Instant::now();
            let mut meta_raw = llm_call(
                llm_backend.as_str(),
                openai_like.as_ref(),
                ollama.as_ref(),
                openrouter_caps.as_ref(),
                &chat_options,
                vec![
                    webpipe_local::openai_compat::ChatMessage::system(meta_system),
                    webpipe_local::openai_compat::ChatMessage::user(&meta_summary),
                ],
                llm_timeout_ms,
                temperature,
                top_p,
                json_mode,
            )
            .await
            .unwrap_or_default();
            let mut meta = extract_json_object(&meta_raw).filter(validate_meta_summary);
            if transcript_enabled {
                transcript_seq = transcript_seq.wrapping_add(1);
                let line = serde_json::json!({
                    "schema_version": 1,
                    "kind": "webpipe_eval_transcript_event",
                    "generated_at_epoch_s": now,
                    "seq": transcript_seq,
                    "run_kind": "eval_judge_swarm",
                    "stage": "meta",
                    "call_id": "meta:attempt1",
                    "llm": {
                        "backend": llm_backend,
                        "model_effective": llm_model_effective,
                        "json_mode": json_mode,
                        "timeout_ms": llm_timeout_ms
                    },
                    "prompt": {
                        "system": tx_trunc(meta_system, transcript_max_chars),
                        "user": tx_trunc(&meta_summary, transcript_max_chars)
                    },
                    "response": {
                        "raw": tx_trunc(&meta_raw, transcript_max_chars),
                        "parsed": meta
                    },
                    "timing_ms": meta_t0.elapsed().as_millis()
                });
                let _ = append_jsonl(transcript_path.as_path(), &line);
            }
            if meta.is_none() && retry_on_parse_fail && openai_like.is_some() {
                let (prev, _) = take_chars(&meta_raw, 900);
                let retry = format!(
                    "Your previous output did not match the required schema.\n\nReturn ONLY one JSON object. Keep EXACTLY these keys and types:\n{{\n  \"overall_assessment\": string,\n  \"cross_judge_agreement\": string,\n  \"top_systemic_failures\": string[],\n  \"top_3_fixes\": string[],\n  \"recommended_next_experiments\": string[],\n  \"top_dimensions\": string[],\n  \"top_musings\": string[]\n}}\n\nCopy this template and fill it in (keep arrays as arrays; use [] if none):\n{{\"overall_assessment\":\"...\",\"cross_judge_agreement\":\"...\",\"top_systemic_failures\":[\"...\"],\"top_3_fixes\":[\"...\"],\"recommended_next_experiments\":[\"...\"],\"top_dimensions\":[\"...\"],\"top_musings\":[\"...\"]}}\n\nPrevious output:\n{}\n\nSource material:\n{}",
                    prev.trim(),
                    meta_summary
                );
                let meta_t1 = std::time::Instant::now();
                meta_raw = llm_call(
                    llm_backend.as_str(),
                    openai_like.as_ref(),
                    ollama.as_ref(),
                    openrouter_caps.as_ref(),
                    &chat_options,
                    vec![
                        webpipe_local::openai_compat::ChatMessage::system(meta_system),
                        webpipe_local::openai_compat::ChatMessage::user(&retry),
                    ],
                    llm_timeout_ms,
                    temperature,
                    top_p,
                    json_mode,
                )
                .await
                .unwrap_or_default();
                meta = extract_json_object(&meta_raw).filter(validate_meta_summary);
                if transcript_enabled {
                    transcript_seq = transcript_seq.wrapping_add(1);
                    let line = serde_json::json!({
                        "schema_version": 1,
                        "kind": "webpipe_eval_transcript_event",
                        "generated_at_epoch_s": now,
                        "seq": transcript_seq,
                        "run_kind": "eval_judge_swarm",
                        "stage": "meta",
                        "call_id": "meta:attempt2",
                        "llm": {
                            "backend": llm_backend,
                            "model_effective": llm_model_effective,
                            "json_mode": json_mode,
                            "timeout_ms": llm_timeout_ms
                        },
                        "prompt": {
                            "system": tx_trunc(meta_system, transcript_max_chars),
                            "user": tx_trunc(&retry, transcript_max_chars)
                        },
                        "response": {
                            "raw": tx_trunc(&meta_raw, transcript_max_chars),
                            "parsed": meta
                        },
                        "timing_ms": meta_t1.elapsed().as_millis()
                    });
                    let _ = append_jsonl(transcript_path.as_path(), &line);
                }
            }

            let payload = serde_json::json!({
                "schema_version": 1,
                "kind": "webpipe_eval_judge_swarm",
                "generated_at_epoch_s": now,
                "inputs": {
                    "queries_json": args.queries_json,
                    "qrels": args.qrels,
                    "base_url": args.base_url,
                    "provider": args.provider,
                    "auto_mode": args.auto_mode,
                    "selection_mode": args.selection_mode,
                    "fetch_backend": args.fetch_backend,
                    "trial_set": args.trial_set,
                    "judges": args.judges,
                    "judge_presets_json": args.judge_presets_json,
                    "domain_tags": args.domain_tags,
                    "max_queries": args.max_queries,
                    "seed": args.seed,
                    "llm_backend": llm_backend,
                    "llm_model": args.llm_model,
                    "llm_model_effective": llm_model_effective,
                    "llm_timeout_ms": llm_timeout_ms,
                    "json_mode": json_mode,
                    "retry_on_parse_fail": retry_on_parse_fail,
                    "task_solve": args.task_solve,
                    "task_solve_max_trials": args.task_solve_max_trials,
                    "task_solve_max_evidence_chars": args.task_solve_max_evidence_chars,
                    "transcript": transcript_enabled,
                    "transcript_jsonl": if transcript_enabled { Some(transcript_path.clone()) } else { None },
                    "transcript_max_chars": transcript_max_chars
                },
                "meta": meta,
                "meta_raw": if meta.is_none() { meta_raw } else { String::new() },
                "judge_reports": judge_reports
            });
            std::fs::write(&out, serde_json::to_string_pretty(&payload)? + "\n")?;
            println!("{}", out.display());
        }
        #[cfg(feature = "eval")]
        Commands::EvalGenerateDomainPack(args) => {
            let now = args.now_epoch_s.unwrap_or_else(|| {
                std::time::SystemTime::now()
                    .duration_since(std::time::UNIX_EPOCH)
                    .unwrap_or_default()
                    .as_secs()
            });
            let out = args.out.unwrap_or_else(|| {
                std::path::PathBuf::from(format!(
                    ".generated/webpipe-e2e-queries-domain-pack-{now}.json"
                ))
            });
            let domains = args
                .domains
                .split(',')
                .map(|s| s.trim().to_string())
                .filter(|s| !s.is_empty())
                .collect::<Vec<_>>();

            let p = eval::generate_domain_pack(eval::DomainPackSpec {
                domains,
                per_domain: args.per_domain,
                seed: args.seed,
                out,
                notes: args.notes,
            })?;
            println!("{}", p.display());
        }
        #[cfg(all(feature = "eval", feature = "stdio"))]
        Commands::EvalCriticLoop(args) => {
            use rmcp::{
                model::CallToolRequestParam,
                service::ServiceExt,
                transport::{ConfigureCommandExt, TokioChildProcess},
            };

            let now = args.now_epoch_s.unwrap_or_else(|| {
                std::time::SystemTime::now()
                    .duration_since(std::time::UNIX_EPOCH)
                    .unwrap_or_default()
                    .as_secs()
            });
            let out = args.out.unwrap_or_else(|| {
                std::path::PathBuf::from(format!(".generated/webpipe-eval-critic-loop-{now}.json"))
            });
            let transcript_path = args.transcript_jsonl.clone().unwrap_or_else(|| {
                std::path::PathBuf::from(format!("{}.transcript.jsonl", out.display()))
            });

            std::fs::create_dir_all(
                out.parent()
                    .unwrap_or_else(|| std::path::Path::new(".generated")),
            )?;
            if let Some(p) = transcript_path.parent() {
                std::fs::create_dir_all(p)?;
            }

            fn append_jsonl(
                path: &std::path::Path,
                line: &serde_json::Value,
            ) -> anyhow::Result<()> {
                use std::io::Write;
                let mut f = std::fs::OpenOptions::new()
                    .create(true)
                    .append(true)
                    .open(path)?;
                writeln!(f, "{}", serde_json::to_string(line).unwrap_or_default())?;
                Ok(())
            }
            fn take_chars(s: &str, max_chars: usize) -> (String, bool) {
                if max_chars == 0 {
                    return (String::new(), true);
                }
                let mut out = String::new();
                for (n, ch) in s.chars().enumerate() {
                    if n >= max_chars {
                        return (out, true);
                    }
                    out.push(ch);
                }
                (out, false)
            }
            fn tx_trunc(s: &str, max_chars: usize) -> serde_json::Value {
                let (t, truncated) = take_chars(s, max_chars);
                serde_json::json!({ "text": t, "truncated": truncated })
            }
            fn json_trunc(v: &serde_json::Value, max_chars: usize) -> serde_json::Value {
                let s = serde_json::to_string(v).unwrap_or_default();
                tx_trunc(&s, max_chars)
            }

            let urls: Vec<String> = args
                .urls
                .into_iter()
                .map(|s| s.trim().to_string())
                .filter(|s| !s.is_empty())
                .collect();
            if urls.is_empty() {
                anyhow::bail!("no URLs provided (use --url ... --url ...)");
            }
            let max_urls = std::cmp::min(10usize, urls.len());

            // Spawn MCP stdio server as a child and talk to it as a client.
            let bin = std::env::current_exe()?;
            let service = ()
                .serve(TokioChildProcess::new(
                    tokio::process::Command::new(bin).configure(|cmd| {
                        cmd.args(["mcp-stdio"]);
                        cmd.env("WEBPIPE_DOTENV", "0");
                        // Avoid accidentally using real keys on dev machines.
                        cmd.env_remove("WEBPIPE_BRAVE_API_KEY");
                        cmd.env_remove("BRAVE_SEARCH_API_KEY");
                        cmd.env_remove("WEBPIPE_TAVILY_API_KEY");
                        cmd.env_remove("TAVILY_API_KEY");
                        cmd.env_remove("WEBPIPE_FIRECRAWL_API_KEY");
                        cmd.env_remove("FIRECRAWL_API_KEY");
                    }),
                )?)
                .await?;

            // Tool call (bounded).
            let args_json = serde_json::json!({
                "query": args.query,
                "urls": urls,
                "url_selection_mode": "preserve",
                "fetch_backend": args.fetch_backend,
                "no_network": args.no_network,
                "agentic": false,
                "max_urls": max_urls,
                "top_chunks": 5,
                "max_chunk_chars": 350,
                "include_text": false,
                "include_links": false,
                "cache_read": false,
                "cache_write": false
            });

            let t0 = std::time::Instant::now();
            let resp = service
                .call_tool(CallToolRequestParam {
                    name: "search_evidence".into(),
                    arguments: Some(args_json.as_object().cloned().unwrap_or_default()),
                })
                .await?;
            let elapsed_ms = t0.elapsed().as_millis() as u64;

            let structured = resp.structured_content.clone();
            let structured_ok = structured.as_ref().is_some_and(|v| v.is_object());
            let payload = structured.clone().unwrap_or(serde_json::Value::Null);

            let mut issues: Vec<&str> = Vec::new();
            let mut push_issue = |s: &'static str| {
                if !CRITIC_ISSUE_VOCAB.contains(&s) {
                    return;
                }
                if !issues.contains(&s) {
                    issues.push(s);
                }
            };

            if !structured_ok {
                push_issue("missing_structured_content");
            }

            // Cursor-facing Markdown contract checks.
            let md0 = resp
                .content
                .first()
                .and_then(|c| c.as_text())
                .map(|t| t.text.clone())
                .unwrap_or_default();
            let markdown_present = !md0.trim().is_empty();
            if !markdown_present {
                push_issue("missing_markdown");
            } else {
                if serde_json::from_str::<serde_json::Value>(&md0).is_ok() {
                    push_issue("markdown_is_json");
                }
                if !md0.contains("## Request") {
                    push_issue("markdown_missing_request_section");
                }
                if !md0.contains("## Summary") {
                    push_issue("markdown_missing_summary_section");
                }
            }

            // Deterministic critic over structured payload (no model calls).
            let ok = payload.get("ok").and_then(|v| v.as_bool()).unwrap_or(false);
            if !ok {
                push_issue("tool_failed");
            }
            let kind = payload.get("kind").and_then(|v| v.as_str()).unwrap_or("");
            if kind.is_empty() {
                push_issue("missing_or_bad_kind");
            } else if kind != "web_search_extract" {
                push_issue("unexpected_kind");
            }
            let results_count = payload
                .get("results")
                .and_then(|v| v.as_array())
                .map(|a| a.len())
                .unwrap_or(0);
            let observed_url_count = payload
                .get("results")
                .and_then(|v| v.as_array())
                .map(|a| {
                    let mut n = 0usize;
                    for it in a {
                        if it
                            .get("url")
                            .and_then(|u| u.as_str())
                            .is_some_and(|s| !s.trim().is_empty())
                        {
                            n += 1;
                        }
                    }
                    n
                })
                .unwrap_or(0);
            if observed_url_count == 0 {
                push_issue("missing_urls");
            }
            let top_chunks_count = payload
                .get("top_chunks")
                .and_then(|v| v.as_array())
                .map(|a| a.len())
                .unwrap_or(0);
            if top_chunks_count == 0 {
                push_issue("empty_top_chunks");
            }
            let warnings_count = payload
                .get("warnings")
                .and_then(|v| v.as_array())
                .map(|a| a.len())
                .unwrap_or(0);
            if warnings_count > 0 {
                push_issue("warnings_present");
            }
            let has_low_signal = payload
                .get("quality")
                .and_then(|q| q.get("signals"))
                .and_then(|s| s.get("has_low_signal"))
                .and_then(|b| b.as_bool())
                .unwrap_or(false);
            if has_low_signal {
                push_issue("low_signal");
            }

            let critic = serde_json::json!({
                "schema_version": 1,
                "kind": "webpipe_critic_v1",
                "issues": issues,
                "ok": ok,
                "structured_ok": structured_ok,
                "markdown_ok": markdown_present,
                "markdown_len_chars": md0.chars().count(),
                "results_count": results_count,
                "observed_url_count": observed_url_count,
                "top_chunks_count": top_chunks_count,
                "warnings_count": warnings_count,
                "has_low_signal": has_low_signal
            });

            // Transcript events (tool + critic).
            let transcript_max_chars = args.transcript_max_chars.clamp(200, 200_000);
            let mut seq: u64 = 0;
            seq = seq.wrapping_add(1);
            let tool_line = serde_json::json!({
                "schema_version": 1,
                "kind": "webpipe_eval_transcript_event",
                "generated_at_epoch_s": now,
                "seq": seq,
                "run_kind": "eval_critic_loop",
                "stage": "tool",
                "call_id": "tool:web_search_extract",
                "tool": {
                    "name": "web_search_extract",
                    "args": json_trunc(&args_json, transcript_max_chars),
                    "result_summary": {
                        "ok": ok,
                        "results_count": results_count,
                        "observed_url_count": observed_url_count,
                        "top_chunks_count": top_chunks_count,
                        "warnings_count": warnings_count,
                        "markdown_ok": markdown_present,
                        "structured_ok": structured_ok
                    },
                    "elapsed_ms": elapsed_ms
                }
            });
            let _ = append_jsonl(transcript_path.as_path(), &tool_line);
            seq = seq.wrapping_add(1);
            let critic_line = serde_json::json!({
                "schema_version": 1,
                "kind": "webpipe_eval_transcript_event",
                "generated_at_epoch_s": now,
                "seq": seq,
                "run_kind": "eval_critic_loop",
                "stage": "critic",
                "call_id": "critic:deterministic",
                "prompt": { "user": tx_trunc("deterministic_critic_v1", transcript_max_chars) },
                "response": { "parsed": critic }
            });
            let _ = append_jsonl(transcript_path.as_path(), &critic_line);

            service.cancel().await?;

            let report = serde_json::json!({
                "schema_version": 1,
                "kind": "webpipe_eval_critic_loop",
                "generated_at_epoch_s": now,
                "inputs": {
                    "query": args_json.get("query").cloned().unwrap_or(serde_json::Value::Null),
                    "urls": args_json.get("urls").cloned().unwrap_or(serde_json::Value::Null),
                    "fetch_backend": args_json.get("fetch_backend").cloned().unwrap_or(serde_json::Value::Null),
                    "no_network": args_json.get("no_network").cloned().unwrap_or(serde_json::Value::Null)
                },
                "outputs": {
                    "out": out.display().to_string(),
                    "transcript_jsonl": transcript_path.display().to_string()
                },
                "tool_result_summary": {
                    "ok": ok,
                    "results_count": results_count,
                    "observed_url_count": observed_url_count,
                    "top_chunks_count": top_chunks_count
                },
                "critic": critic
            });
            std::fs::write(&out, serde_json::to_string_pretty(&report)? + "\n")?;
            println!("{}", out.display());
        }
        #[cfg(all(feature = "eval", feature = "stdio"))]
        Commands::EvalCriticRun(args) => {
            use rmcp::{
                model::CallToolRequestParam,
                service::ServiceExt,
                transport::{ConfigureCommandExt, TokioChildProcess},
            };

            let now = args.now_epoch_s.unwrap_or_else(|| {
                std::time::SystemTime::now()
                    .duration_since(std::time::UNIX_EPOCH)
                    .unwrap_or_default()
                    .as_secs()
            });
            let out = args.out.unwrap_or_else(|| {
                std::path::PathBuf::from(format!(".generated/webpipe-eval-critic-run-{now}.json"))
            });
            let transcript_path = args.transcript_jsonl.clone().unwrap_or_else(|| {
                std::path::PathBuf::from(format!("{}.transcript.jsonl", out.display()))
            });
            std::fs::create_dir_all(
                out.parent()
                    .unwrap_or_else(|| std::path::Path::new(".generated")),
            )?;
            if let Some(p) = transcript_path.parent() {
                std::fs::create_dir_all(p)?;
            }

            fn append_jsonl(
                path: &std::path::Path,
                line: &serde_json::Value,
            ) -> anyhow::Result<()> {
                use std::io::Write;
                let mut f = std::fs::OpenOptions::new()
                    .create(true)
                    .append(true)
                    .open(path)?;
                writeln!(f, "{}", serde_json::to_string(line).unwrap_or_default())?;
                Ok(())
            }
            fn take_chars(s: &str, max_chars: usize) -> (String, bool) {
                if max_chars == 0 {
                    return (String::new(), true);
                }
                let mut out = String::new();
                for (n, ch) in s.chars().enumerate() {
                    if n >= max_chars {
                        return (out, true);
                    }
                    out.push(ch);
                }
                (out, false)
            }
            fn tx_trunc(s: &str, max_chars: usize) -> serde_json::Value {
                let (t, truncated) = take_chars(s, max_chars);
                serde_json::json!({ "text": t, "truncated": truncated })
            }
            fn json_trunc(v: &serde_json::Value, max_chars: usize) -> serde_json::Value {
                let s = serde_json::to_string(v).unwrap_or_default();
                tx_trunc(&s, max_chars)
            }

            let base_url = args.base_url.trim_end_matches('/').to_string();
            if base_url.trim().is_empty() {
                anyhow::bail!("base_url must be non-empty");
            }
            let query_pack_hash = std::fs::read(&args.queries_json)
                .ok()
                .map(|b| blake3_hex_bytes(&b));
            let git_sha = best_effort_git_sha();
            let e2e = eval::load_e2e_queries_v1(&args.queries_json)?;
            let max_queries = args.max_queries.clamp(1, 200);
            let queries = e2e
                .queries
                .into_iter()
                .take(max_queries)
                .collect::<Vec<_>>();
            if queries.is_empty() {
                anyhow::bail!("no queries in dataset");
            }

            // Spawn MCP stdio server once and reuse it for all tool calls.
            let bin = std::env::current_exe()?;
            let service = ()
                .serve(TokioChildProcess::new(
                    tokio::process::Command::new(bin).configure(|cmd| {
                        cmd.args(["mcp-stdio"]);
                        cmd.env("WEBPIPE_DOTENV", "0");
                        // Keep output stable: Markdown-first only, no raw JSON text.
                        cmd.env("WEBPIPE_MCP_INCLUDE_JSON_TEXT", "0");
                        cmd.env("WEBPIPE_MCP_MARKDOWN_CHUNK_EXCERPTS", "0");
                        // Avoid accidentally using real keys on dev machines.
                        cmd.env_remove("WEBPIPE_BRAVE_API_KEY");
                        cmd.env_remove("BRAVE_SEARCH_API_KEY");
                        cmd.env_remove("WEBPIPE_TAVILY_API_KEY");
                        cmd.env_remove("TAVILY_API_KEY");
                        cmd.env_remove("WEBPIPE_FIRECRAWL_API_KEY");
                        cmd.env_remove("FIRECRAWL_API_KEY");
                    }),
                )?)
                .await?;

            let transcript_max_chars = args.transcript_max_chars.clamp(200, 200_000);
            let mut seq: u64 = 0;

            let mut totals_ok: u64 = 0;
            let mut totals_queries: u64 = 0;
            let mut issue_counts: std::collections::BTreeMap<String, u64> =
                std::collections::BTreeMap::new();
            let mut per_query: Vec<serde_json::Value> = Vec::new();

            // Transcript: run manifest (first line).
            seq = seq.wrapping_add(1);
            let manifest_line = serde_json::json!({
                "schema_version": 1,
                "kind": "webpipe_eval_transcript_event",
                "generated_at_epoch_s": now,
                "seq": seq,
                "run_kind": "eval_critic_run",
                "stage": "manifest",
                "call_id": "manifest",
                "manifest": {
                    "git": { "sha": git_sha },
                    "queries_json_hash": query_pack_hash,
                    "llm": { "backend": serde_json::Value::Null, "model": serde_json::Value::Null },
                }
            });
            let _ = append_jsonl(transcript_path.as_path(), &manifest_line);

            for q in &queries {
                totals_queries = totals_queries.saturating_add(1);
                let urls: Vec<String> = q
                    .url_paths
                    .iter()
                    .filter_map(|p| {
                        let s = p.trim();
                        if s.is_empty() {
                            return None;
                        }
                        if s.starts_with("http://") || s.starts_with("https://") {
                            return Some(s.to_string());
                        }
                        let sp = s.trim_start_matches('/');
                        Some(format!("{base_url}/{sp}"))
                    })
                    .collect();

                let args_json = serde_json::json!({
                    "query": q.query,
                    "urls": urls,
                    "url_selection_mode": "preserve",
                    "fetch_backend": args.fetch_backend,
                    "no_network": args.no_network,
                    "agentic": false,
                    "max_urls": 5,
                    "top_chunks": 5,
                    "max_chunk_chars": 350,
                    "include_text": false,
                    "include_links": false,
                    "cache_read": false,
                    "cache_write": false
                });

                let t0 = std::time::Instant::now();
                // Use the stable Cursor-facing tool name. (The underlying payload `kind` remains
                // `web_search_extract`, so the critic checks stay aligned.)
                let resp = service
                    .call_tool(CallToolRequestParam {
                        name: "search_evidence".into(),
                        arguments: Some(args_json.as_object().cloned().unwrap_or_default()),
                    })
                    .await?;
                let elapsed_ms = t0.elapsed().as_millis() as u64;

                // Critic checks (same contract focus as eval-critic-loop).
                let structured = resp.structured_content.clone();
                let structured_ok = structured.as_ref().is_some_and(|v| v.is_object());
                let payload = structured.clone().unwrap_or(serde_json::Value::Null);

                let mut issues: Vec<&str> = Vec::new();
                let mut push_issue = |s: &'static str| {
                    if !CRITIC_ISSUE_VOCAB.contains(&s) {
                        return;
                    }
                    if !issues.contains(&s) {
                        issues.push(s);
                    }
                };
                if !structured_ok {
                    push_issue("missing_structured_content");
                }

                let md0 = resp
                    .content
                    .first()
                    .and_then(|c| c.as_text())
                    .map(|t| t.text.clone())
                    .unwrap_or_default();
                let markdown_present = !md0.trim().is_empty();
                if !markdown_present {
                    push_issue("missing_markdown");
                } else {
                    if serde_json::from_str::<serde_json::Value>(&md0).is_ok() {
                        push_issue("markdown_is_json");
                    }
                    if !md0.contains("## Request") {
                        push_issue("markdown_missing_request_section");
                    }
                    if !md0.contains("## Summary") {
                        push_issue("markdown_missing_summary_section");
                    }
                }

                let ok = payload.get("ok").and_then(|v| v.as_bool()).unwrap_or(false);
                if !ok {
                    push_issue("tool_failed");
                }
                let kind = payload.get("kind").and_then(|v| v.as_str()).unwrap_or("");
                if kind.is_empty() {
                    push_issue("missing_or_bad_kind");
                } else if kind != "web_search_extract" {
                    push_issue("unexpected_kind");
                }
                let results_count = payload
                    .get("results")
                    .and_then(|v| v.as_array())
                    .map(|a| a.len())
                    .unwrap_or(0);
                let observed_url_count = payload
                    .get("results")
                    .and_then(|v| v.as_array())
                    .map(|a| {
                        let mut n = 0usize;
                        for it in a {
                            if it
                                .get("url")
                                .and_then(|u| u.as_str())
                                .is_some_and(|s| !s.trim().is_empty())
                            {
                                n += 1;
                            }
                        }
                        n
                    })
                    .unwrap_or(0);
                if observed_url_count == 0 {
                    push_issue("missing_urls");
                }
                let top_chunks_count = payload
                    .get("top_chunks")
                    .and_then(|v| v.as_array())
                    .map(|a| a.len())
                    .unwrap_or(0);
                if top_chunks_count == 0 {
                    push_issue("empty_top_chunks");
                }
                let warnings_count = payload
                    .get("warnings")
                    .and_then(|v| v.as_array())
                    .map(|a| a.len())
                    .unwrap_or(0);
                if warnings_count > 0 {
                    push_issue("warnings_present");
                }
                let has_low_signal = payload
                    .get("quality")
                    .and_then(|q| q.get("signals"))
                    .and_then(|s| s.get("has_low_signal"))
                    .and_then(|b| b.as_bool())
                    .unwrap_or(false);
                if has_low_signal {
                    push_issue("low_signal");
                }

                let critic = serde_json::json!({
                    "schema_version": 1,
                    "kind": "webpipe_critic_v1",
                    "issues": issues,
                    "ok": ok,
                    "structured_ok": structured_ok,
                    "markdown_ok": markdown_present,
                    "markdown_len_chars": md0.chars().count(),
                    "results_count": results_count,
                    "observed_url_count": observed_url_count,
                    "top_chunks_count": top_chunks_count,
                    "warnings_count": warnings_count,
                    "has_low_signal": has_low_signal
                });

                // Update aggregates.
                if ok {
                    totals_ok = totals_ok.saturating_add(1);
                }
                if let Some(arr) = critic.get("issues").and_then(|v| v.as_array()) {
                    for it in arr {
                        let s = it.as_str().unwrap_or("").to_string();
                        if s.is_empty() {
                            continue;
                        }
                        *issue_counts.entry(s).or_insert(0) += 1;
                    }
                }

                // Transcript: tool event.
                seq = seq.wrapping_add(1);
                let tool_line = serde_json::json!({
                    "schema_version": 1,
                    "kind": "webpipe_eval_transcript_event",
                    "generated_at_epoch_s": now,
                    "seq": seq,
                    "run_kind": "eval_critic_run",
                    "stage": "tool",
                    "call_id": format!("tool:web_search_extract:{}", q.query_id),
                    "tool": {
                        "name": "web_search_extract",
                        "args": json_trunc(&args_json, transcript_max_chars),
                        "result_summary": {
                            "ok": ok,
                            "results_count": results_count,
                            "observed_url_count": observed_url_count,
                            "top_chunks_count": top_chunks_count,
                            "warnings_count": warnings_count,
                            "markdown_ok": markdown_present,
                            "structured_ok": structured_ok
                        },
                        "elapsed_ms": elapsed_ms
                    }
                });
                let _ = append_jsonl(transcript_path.as_path(), &tool_line);

                // Transcript: critic event.
                seq = seq.wrapping_add(1);
                let critic_line = serde_json::json!({
                    "schema_version": 1,
                    "kind": "webpipe_eval_transcript_event",
                    "generated_at_epoch_s": now,
                    "seq": seq,
                    "run_kind": "eval_critic_run",
                    "stage": "critic",
                    "call_id": format!("critic:deterministic:{}", q.query_id),
                    "prompt": { "user": tx_trunc("deterministic_critic_v1", transcript_max_chars) },
                    "response": { "parsed": critic }
                });
                let _ = append_jsonl(transcript_path.as_path(), &critic_line);

                per_query.push(serde_json::json!({
                    "query_id": q.query_id,
                    "query": q.query,
                    "urls_count": urls.len(),
                    "tool_elapsed_ms": elapsed_ms,
                    "critic": critic
                }));
            }

            service.cancel().await?;

            let payload = serde_json::json!({
                "schema_version": 1,
                "kind": "webpipe_eval_critic_run",
                "generated_at_epoch_s": now,
                "manifest": {
                    "git": { "sha": git_sha },
                    "queries_json_hash": query_pack_hash
                },
                "inputs": {
                    "queries_json": args.queries_json,
                    "base_url": args.base_url,
                    "fetch_backend": args.fetch_backend,
                    "no_network": args.no_network,
                    "max_queries": max_queries
                },
                "outputs": {
                    "out": out.display().to_string(),
                    "transcript_jsonl": transcript_path.display().to_string()
                },
                "totals": {
                    "queries": totals_queries,
                    "ok": totals_ok
                },
                "issue_counts": issue_counts,
                "per_query": per_query
            });

            std::fs::write(&out, serde_json::to_string_pretty(&payload)? + "\n")?;
            println!("{}", out.display());
        }
        Commands::Doctor(args) => {
            fn has_env(k: &str) -> bool {
                std::env::var(k).ok().is_some_and(|v| !v.trim().is_empty())
            }

            let t0 = std::time::Instant::now();

            // Env presence (booleans only; never print values).
            let brave_configured =
                has_env("WEBPIPE_BRAVE_API_KEY") || has_env("BRAVE_SEARCH_API_KEY");
            let tavily_configured = has_env("WEBPIPE_TAVILY_API_KEY") || has_env("TAVILY_API_KEY");
            let searxng_configured =
                has_env("WEBPIPE_SEARXNG_ENDPOINT") || has_env("WEBPIPE_SEARXNG_ENDPOINTS");
            let firecrawl_configured =
                has_env("WEBPIPE_FIRECRAWL_API_KEY") || has_env("FIRECRAWL_API_KEY");
            let perplexity_configured =
                has_env("WEBPIPE_PERPLEXITY_API_KEY") || has_env("PERPLEXITY_API_KEY");

            // Cache dir is relevant both for the MCP server and eval harnesses.
            let cache_dir = std::env::var("WEBPIPE_CACHE_DIR")
                .ok()
                .filter(|s| !s.trim().is_empty())
                .map(std::path::PathBuf::from)
                .unwrap_or_else(mcp::default_cache_dir);

            let mut checks: Vec<serde_json::Value> = Vec::new();

            // Check: cache dir is creatable + writable.
            let cache_ok = (|| -> anyhow::Result<()> {
                std::fs::create_dir_all(&cache_dir)?;
                let probe = cache_dir.join(format!(
                    "webpipe-doctor-{}.probe",
                    std::time::SystemTime::now()
                        .duration_since(std::time::UNIX_EPOCH)
                        .unwrap_or_default()
                        .as_millis()
                ));
                std::fs::write(&probe, b"ok")?;
                let _ = std::fs::remove_file(&probe);
                Ok(())
            })()
            .is_ok();
            checks.push(serde_json::json!({
                "name": "cache_dir_writable",
                "ok": cache_ok,
                "message": if cache_ok { "cache dir is writable" } else { "cache dir is not writable" },
                "hint": if cache_ok { "" } else { "Set WEBPIPE_CACHE_DIR to a writable directory." },
            }));

            // Check: stdio MCP handshake (optional).
            let mut stdio_ok: Option<bool> = None;
            let mut stdio_tool_count: Option<usize> = None;
            let mut stdio_error: Option<serde_json::Value> = None;
            let mut stdio_elapsed_ms: Option<u128> = None;

            #[cfg(feature = "stdio")]
            if args.check_stdio {
                use rmcp::service::ServiceExt;
                use rmcp::transport::{ConfigureCommandExt, TokioChildProcess};
                use tokio::process::Command;

                let exe =
                    std::env::current_exe().unwrap_or_else(|_| std::path::PathBuf::from("webpipe"));
                let child = TokioChildProcess::new(Command::new(exe).configure(|cmd| {
                    cmd.args(["mcp-stdio"]);
                    cmd.env("WEBPIPE_CACHE_DIR", &cache_dir);
                    // Make this probe hermetic: don't auto-load a nearby `.env` and "reintroduce"
                    // keys we just removed.
                    cmd.env("WEBPIPE_DOTENV", "0");
                    cmd.env_remove("WEBPIPE_ENV_FILE");
                    // Avoid accidentally inheriting provider keys for this probe.
                    cmd.env_remove("WEBPIPE_BRAVE_API_KEY");
                    cmd.env_remove("BRAVE_SEARCH_API_KEY");
                    cmd.env_remove("WEBPIPE_TAVILY_API_KEY");
                    cmd.env_remove("TAVILY_API_KEY");
                    cmd.env_remove("WEBPIPE_SEARXNG_ENDPOINT");
                    cmd.env_remove("WEBPIPE_FIRECRAWL_API_KEY");
                    cmd.env_remove("FIRECRAWL_API_KEY");
                    cmd.env_remove("WEBPIPE_PERPLEXITY_API_KEY");
                    cmd.env_remove("PERPLEXITY_API_KEY");
                    // Keep stderr quiet-ish for this probe unless explicitly enabled.
                    cmd.env("RUST_LOG", "error");
                }))?;

                let service = ().serve(child).await?;
                let check_t0 = std::time::Instant::now();
                let res = tokio::time::timeout(
                    std::time::Duration::from_millis(args.timeout_ms),
                    service.list_tools(Default::default()),
                )
                .await;
                stdio_elapsed_ms = Some(check_t0.elapsed().as_millis());

                match res {
                    Ok(Ok(tools)) => {
                        stdio_tool_count = Some(tools.tools.len());
                        // Cursor needs per-tool JSON Schema (`inputSchema`) to render structured calls.
                        // rmcp provides JSON Schema objects; an *empty* schema is the real failure mode.
                        let mut missing_schema: Vec<String> = Vec::new();
                        for t in tools.tools.iter() {
                            let schema = t.input_schema.as_ref();
                            let looks_missing = schema.is_empty();
                            if looks_missing {
                                missing_schema.push(t.name.to_string());
                            }
                        }
                        if missing_schema.is_empty() {
                            stdio_ok = Some(true);
                        } else {
                            stdio_ok = Some(false);
                            let sample = tools.tools.first().map(|t| {
                                let keys: Vec<String> =
                                    t.input_schema.keys().take(30).cloned().collect();
                                serde_json::json!({
                                    "name": t.name,
                                    "input_schema_keys": keys
                                })
                            });
                            stdio_error = Some(serde_json::json!({
                                "code": "tools_missing_input_schema",
                                "message": format!("{} tools missing inputSchema", missing_schema.len()),
                                "hint": "Cursor uses tool inputSchema for structured calls. Ensure the MCP server tool list includes JSON Schema inputSchema for each tool.",
                                "tools": missing_schema,
                                "sample": sample
                            }));
                        }
                    }
                    Ok(Err(e)) => {
                        stdio_ok = Some(false);
                        let msg = e.to_string();
                        let hint = if msg.contains("ConnectionClosed")
                            || msg.contains("initialized request")
                            || msg.contains("TransportClosed")
                        {
                            "The child process closed the stdio transport early. Common causes: stdout contamination (printing logs to stdout), wrong args (not running mcp-stdio), or a crash on startup. Reinstall `webpipe` and check it prints nothing to stdout in mcp-stdio mode."
                        } else {
                            "Stdio MCP handshake failed. Reinstall `webpipe` and verify your Cursor mcp.json points at the correct command and uses args: [\"mcp-stdio\"]."
                        };
                        stdio_error = Some(serde_json::json!({
                            "code": "handshake_failed",
                            "message": msg,
                            "hint": hint
                        }));
                    }
                    Err(_elapsed) => {
                        stdio_ok = Some(false);
                        stdio_error = Some(serde_json::json!({
                            "code": "timeout",
                            "message": format!("stdio handshake timed out after {}ms", args.timeout_ms),
                            "hint": "The child did not respond to list_tools in time. Check for a stuck startup (deadlock, slow disk, or a prompt). Also verify WEBPIPE_CACHE_DIR is writable."
                        }));
                    }
                }

                let _ = service.cancel().await;
            }

            #[cfg(not(feature = "stdio"))]
            if args.check_stdio {
                stdio_ok = Some(false);
            }

            checks.push(serde_json::json!({
                "name": "mcp_stdio_handshake",
                "ok": if args.check_stdio { stdio_ok.unwrap_or(false) } else { true },
                "skipped": !args.check_stdio,
                "message": if !args.check_stdio {
                    "stdio MCP handshake skipped"
                } else if stdio_ok.unwrap_or(false) {
                    "stdio MCP handshake succeeded"
                } else {
                    "stdio MCP handshake failed"
                },
                "hint": if !args.check_stdio || stdio_ok.unwrap_or(false) {
                    ""
                } else if cfg!(feature = "stdio") {
                    "Check that Cursor is pointing at the correct `webpipe` binary, and that `WEBPIPE_CACHE_DIR` is writable. If needed, reinstall: `cargo install --path <repo>/webpipe/crates/webpipe-mcp --bin webpipe --force`."
                } else {
                    "`mcp-stdio` requires building with feature `stdio`."
                },
                "tool_count": stdio_tool_count,
                "elapsed_ms": stdio_elapsed_ms,
                "error": stdio_error,
            }));

            let ok = checks.iter().all(|c| c["ok"].as_bool().unwrap_or(false));
            let payload = serde_json::json!({
                "schema_version": 2,
                "kind": "doctor",
                "ok": ok,
                "name": "webpipe",
                "version": env!("CARGO_PKG_VERSION"),
                "platform": {
                    "os": std::env::consts::OS,
                    "arch": std::env::consts::ARCH,
                },
                "features": {
                    "stdio": cfg!(feature = "stdio"),
                    "semantic": true,
                    "embeddings_openai": false,
                    "embeddings_tei": false,
                    "embeddings_openrouter": has_env("WEBPIPE_OPENROUTER_API_KEY")
                        || has_env("OPENROUTER_API_KEY"),
                },
                "elapsed_ms": t0.elapsed().as_millis(),
                "configured": {
                    "providers": {
                        "brave": brave_configured,
                        "tavily": tavily_configured,
                        "searxng": searxng_configured,
                    },
                    "remote_fetch": {
                        "firecrawl": firecrawl_configured,
                    },
                    "llm": {
                        "perplexity": perplexity_configured,
                        "openrouter": has_env("WEBPIPE_OPENROUTER_API_KEY") || has_env("OPENROUTER_API_KEY"),
                        "openrouter_model": if has_env("WEBPIPE_OPENROUTER_API_KEY") || has_env("OPENROUTER_API_KEY") {
                            Some(mcp::WebpipeMcp::openrouter_model_from_env())
                        } else {
                            None
                        },
                        "openai": has_env("WEBPIPE_OPENAI_API_KEY") || has_env("OPENAI_API_KEY"),
                        "openai_model": if has_env("WEBPIPE_OPENAI_API_KEY") || has_env("OPENAI_API_KEY") {
                            Some(mcp::WebpipeMcp::openai_model_from_env())
                        } else {
                            None
                        },
                        "groq": has_env("WEBPIPE_GROQ_API_KEY") || has_env("GROQ_API_KEY"),
                        "groq_model": if has_env("WEBPIPE_GROQ_API_KEY") || has_env("GROQ_API_KEY") {
                            Some(mcp::WebpipeMcp::groq_model_from_env())
                        } else {
                            None
                        },
                    },
                    "cache_dir": cache_dir.to_string_lossy().to_string(),
                },
                "checks": checks,
            });
            match args.output.to_ascii_lowercase().as_str() {
                "text" => {
                    println!("webpipe {} (ok={})", env!("CARGO_PKG_VERSION"), ok);
                    println!(
                        "cache_dir: {}",
                        payload["configured"]["cache_dir"].as_str().unwrap_or("")
                    );
                    println!(
                        "providers: brave={} tavily={}",
                        payload["configured"]["providers"]["brave"]
                            .as_bool()
                            .unwrap_or(false),
                        payload["configured"]["providers"]["tavily"]
                            .as_bool()
                            .unwrap_or(false),
                    );
                    println!(
                        "remote_fetch: firecrawl={}",
                        payload["configured"]["remote_fetch"]["firecrawl"]
                            .as_bool()
                            .unwrap_or(false),
                    );
                    println!(
                        "llm: openrouter={} openai={} groq={} perplexity={}",
                        payload["configured"]["llm"]["openrouter"]
                            .as_bool()
                            .unwrap_or(false),
                        payload["configured"]["llm"]["openai"]
                            .as_bool()
                            .unwrap_or(false),
                        payload["configured"]["llm"]["groq"]
                            .as_bool()
                            .unwrap_or(false),
                        payload["configured"]["llm"]["perplexity"]
                            .as_bool()
                            .unwrap_or(false),
                    );
                    println!("checks:");
                    if let Some(arr) = payload["checks"].as_array() {
                        for c in arr {
                            let name = c["name"].as_str().unwrap_or("?");
                            let ok = c["ok"].as_bool().unwrap_or(false);
                            let skipped = c["skipped"].as_bool().unwrap_or(false);
                            if skipped {
                                println!("- {}: skipped", name);
                            } else {
                                println!("- {}: {}", name, if ok { "ok" } else { "fail" });
                            }
                        }
                    }
                }
                "json" => println!("{payload}"),
                _ => println!("{}", serde_json::to_string_pretty(&payload)?),
            }
        }
        #[cfg(feature = "stdio")]
        Commands::McpListTools(args) => {
            use rmcp::model::PaginatedRequestParam;
            use rmcp::service::ServiceExt;
            use rmcp::transport::{ConfigureCommandExt, TokioChildProcess};
            use tokio::process::Command;

            let bin =
                std::env::current_exe().unwrap_or_else(|_| std::path::PathBuf::from("webpipe"));
            let child = TokioChildProcess::new(Command::new(bin).configure(|cmd| {
                cmd.args(["mcp-stdio"]);
                // Keep stdout reserved for MCP frames (child must not print logs to stdout).
                cmd.env("RUST_LOG", "error");
            }))?;
            let service = ().serve(child).await?;

            let tools = tokio::time::timeout(
                std::time::Duration::from_millis(args.timeout_ms),
                service.list_tools(Some(PaginatedRequestParam::default())),
            )
            .await
            .map_err(|_| {
                anyhow::anyhow!("mcp list_tools timed out after {}ms", args.timeout_ms)
            })??;

            let mut out_tools: Vec<serde_json::Value> = Vec::new();
            for t in tools.tools.iter() {
                let mut o = serde_json::json!({
                    "name": t.name,
                    "description": t.description,
                });
                if args.include_schema {
                    o["input_schema"] = serde_json::Value::Object((*t.input_schema).clone());
                } else {
                    let keys: Vec<String> = t.input_schema.keys().take(50).cloned().collect();
                    o["input_schema_keys"] = serde_json::Value::Array(
                        keys.into_iter().map(serde_json::Value::String).collect(),
                    );
                }
                out_tools.push(o);
            }

            let payload = serde_json::json!({
                "schema_version": 1,
                "kind": "mcp_list_tools",
                "tool_count": out_tools.len(),
                "tools": out_tools,
            });

            match args.output.to_ascii_lowercase().as_str() {
                "json" => println!("{payload}"),
                _ => println!("{}", serde_json::to_string_pretty(&payload)?),
            }

            let _ = service.cancel().await;
        }
        #[cfg(feature = "stdio")]
        Commands::McpCall(args) => {
            use rmcp::model::CallToolRequestParam;
            use rmcp::service::ServiceExt;
            use rmcp::transport::{ConfigureCommandExt, TokioChildProcess};
            use tokio::process::Command;

            let bin =
                std::env::current_exe().unwrap_or_else(|_| std::path::PathBuf::from("webpipe"));
            let child = TokioChildProcess::new(Command::new(bin).configure(|cmd| {
                cmd.args(["mcp-stdio"]);
                // Keep stdout reserved for MCP frames (child must not print logs to stdout).
                cmd.env("RUST_LOG", "error");
            }))?;

            let service = ().serve(child).await?;

            let mut arg_objs: Vec<serde_json::Map<String, serde_json::Value>> = Vec::new();
            match (&args.args_json, &args.args_json_file) {
                (Some(s), None) => {
                    let args_v: serde_json::Value = serde_json::from_str(s)
                        .map_err(|e| anyhow::anyhow!("invalid --args-json: {e}"))?;
                    let Some(obj) = args_v.as_object().cloned() else {
                        anyhow::bail!("--args-json must be a JSON object (top-level {{...}})");
                    };
                    arg_objs.push(obj);
                }
                (None, Some(p)) => {
                    let bytes = std::fs::read(p).map_err(|e| {
                        anyhow::anyhow!("failed to read --args-json-file {}: {e}", p.display())
                    })?;
                    let s = String::from_utf8(bytes)
                        .map_err(|e| anyhow::anyhow!("--args-json-file is not valid UTF-8: {e}"))?;
                    let v: serde_json::Value = serde_json::from_str(&s)
                        .map_err(|e| anyhow::anyhow!("invalid JSON in --args-json-file: {e}"))?;
                    match v {
                        serde_json::Value::Object(o) => arg_objs.push(o),
                        serde_json::Value::Array(arr) => {
                            for (i, it) in arr.into_iter().enumerate() {
                                let Some(o) = it.as_object().cloned() else {
                                    anyhow::bail!(
                                        "--args-json-file array entries must be objects; entry {i} was not an object"
                                    );
                                };
                                arg_objs.push(o);
                            }
                        }
                        _ => anyhow::bail!(
                            "--args-json-file must contain a JSON object or array of objects"
                        ),
                    }
                }
                (None, None) => {
                    // Default to empty args for tools that need no arguments (e.g. webpipe_meta).
                    arg_objs.push(serde_json::Map::new());
                }
                _ => anyhow::bail!("Pass exactly one of --args-json or --args-json-file"),
            }

            let reps = args.repeat.clamp(1, 1000);
            let mut call_durations_ms: Vec<u128> = Vec::new();
            let mut tool_elapsed_ms: Vec<Option<u64>> = Vec::new();

            for rep in 0..reps {
                for (i, obj) in arg_objs.iter().cloned().enumerate() {
                    let call_t0 = std::time::Instant::now();
                    let resp = tokio::time::timeout(
                        std::time::Duration::from_millis(args.timeout_ms),
                        service.call_tool(CallToolRequestParam {
                            name: args.tool.clone().into(),
                            arguments: Some(obj),
                        }),
                    )
                    .await
                    .map_err(|_| {
                        anyhow::anyhow!("mcp call timed out after {}ms", args.timeout_ms)
                    })??;
                    let dur = call_t0.elapsed().as_millis();
                    call_durations_ms.push(dur);

                    let tool_ms = resp
                        .structured_content
                        .as_ref()
                        .and_then(|sc| sc.get("elapsed_ms"))
                        .and_then(|v| v.as_u64());
                    tool_elapsed_ms.push(tool_ms);

                    let should_print = args.print_each || (rep == 0 && i == 0);
                    if should_print {
                        // Print Markdown content first (scan-first for humans).
                        let mut printed_any = false;
                        for c in &resp.content {
                            if let Some(t) = c.as_text() {
                                let s = t.text.trim_end();
                                if !s.is_empty() {
                                    println!("{s}");
                                    printed_any = true;
                                }
                            }
                        }
                        if !printed_any {
                            println!("(no markdown content returned)");
                        }

                        if args.print_structured {
                            if let Some(sc) = resp.structured_content.clone() {
                                println!("\n## structured_content\n");
                                println!("{}", serde_json::to_string_pretty(&sc)?);
                            }
                        }
                    }
                }
            }

            // Timing summary (printed once, even if print_each=false).
            if reps > 1 || arg_objs.len() > 1 {
                println!("\n## Timing\n");
                println!(
                    "- calls: {}; repeats: {}; per-call timeout_ms: {}",
                    call_durations_ms.len(),
                    reps,
                    args.timeout_ms
                );
                let sum: u128 = call_durations_ms.iter().sum();
                let min = call_durations_ms.iter().min().copied().unwrap_or(0);
                let max = call_durations_ms.iter().max().copied().unwrap_or(0);
                let mean = if call_durations_ms.is_empty() {
                    0
                } else {
                    sum / (call_durations_ms.len() as u128)
                };
                println!("- client_call_ms: min={min}, mean={mean}, max={max}");
                let tool_vals: Vec<u64> = tool_elapsed_ms.iter().copied().flatten().collect();
                if !tool_vals.is_empty() {
                    let sum2: u64 = tool_vals.iter().sum();
                    let min2 = tool_vals.iter().min().copied().unwrap_or(0);
                    let max2 = tool_vals.iter().max().copied().unwrap_or(0);
                    let mean2 = sum2 / (tool_vals.len() as u64);
                    println!("- tool_elapsed_ms: min={min2}, mean={mean2}, max={max2} (from structured_content.elapsed_ms)");
                }
            }

            let _ = service.cancel().await;
        }
        #[cfg(feature = "stdio")]
        Commands::McpSweep(args) => {
            use rmcp::model::{CallToolRequestParam, PaginatedRequestParam};
            use rmcp::service::ServiceExt;
            use rmcp::transport::{ConfigureCommandExt, TokioChildProcess};
            use tokio::process::Command;

            let bin =
                std::env::current_exe().unwrap_or_else(|_| std::path::PathBuf::from("webpipe"));
            let child = TokioChildProcess::new(Command::new(bin).configure(|cmd| {
                cmd.args(["mcp-stdio"]);
                // Keep stdout reserved for MCP frames (child must not print logs to stdout).
                cmd.env("RUST_LOG", "error");
            }))?;
            let service = ().serve(child).await?;

            // Discover tool list as the client sees it.
            let tools = tokio::time::timeout(
                std::time::Duration::from_millis(args.timeout_ms),
                service.list_tools(Some(PaginatedRequestParam::default())),
            )
            .await
            .map_err(|_| {
                anyhow::anyhow!("mcp list_tools timed out after {}ms", args.timeout_ms)
            })??;

            let mut runs: Vec<serde_json::Value> = Vec::new();

            // Run each MCP-visible tool with a bounded, tool-agnostic example payload.
            // (This is a smoke test, not a benchmark.)
            for t in &tools.tools {
                let name = t.name.to_string();
                let args_json = match name.as_str() {
                    "webpipe_meta" => serde_json::json!({}),
                    "webpipe_usage" => serde_json::json!({}),
                    "arxiv_search" => serde_json::json!({
                        "query": "transformer attention",
                        "max_results": 3
                    }),
                    "arxiv_enrich" => serde_json::json!({
                        "id_or_url": "1706.03762",
                        "include_discussions": false,
                        "timeout_ms": 20_000
                    }),
                    "paper_search" => serde_json::json!({
                        "query": "transformer attention",
                        "backends": ["semantic_scholar", "openalex"],
                        "limit": 5,
                        "timeout_ms": 20_000,
                        "include_abstract": false
                    }),
                    "web_fetch" => serde_json::json!({
                        "url": "https://example.com",
                        "include_text": false,
                        "timeout_ms": 15_000,
                        "max_bytes": 200_000
                    }),
                    "web_extract" => serde_json::json!({
                        "url": "https://example.com",
                        "include_text": false,
                        "include_structure": true,
                        "top_chunks": 3,
                        "max_chars": args.max_chars,
                        "max_chunk_chars": 400,
                        "timeout_ms": 20_000
                    }),
                    "search_evidence" => serde_json::json!({
                        "query": "what is a red-black tree",
                        "provider": "auto",
                        "auto_mode": "fallback",
                        "exploration": "balanced",
                        "agentic": true,
                        "max_results": 6,
                        "max_urls": args.max_urls,
                        "max_chars": args.max_chars,
                        "top_chunks": 6,
                        "max_chunk_chars": 500,
                        "include_structure": true,
                        "include_text": false,
                        "include_links": false,
                        "compact": false,
                        "timeout_ms": 20_000
                    }),
                    _ => serde_json::json!({}),
                };

                let t0 = std::time::Instant::now();
                let mut ok = true;
                let mut err: Option<String> = None;
                let mut markdown_present = false;
                let mut structured_present = false;
                let mut kind: Option<String> = None;

                let resp = async {
                    let Some(obj) = args_json.as_object().cloned() else {
                        anyhow::bail!("internal: args must be a JSON object");
                    };
                    let resp = tokio::time::timeout(
                        std::time::Duration::from_millis(args.timeout_ms),
                        service.call_tool(CallToolRequestParam {
                            name: name.clone().into(),
                            arguments: Some(obj),
                        }),
                    )
                    .await
                    .map_err(|_| {
                        anyhow::anyhow!("mcp call timed out after {}ms", args.timeout_ms)
                    })??;
                    Ok::<_, anyhow::Error>(resp)
                }
                .await;

                match resp {
                    Ok(resp) => {
                        markdown_present = resp
                            .content
                            .first()
                            .and_then(|c| c.as_text())
                            .is_some_and(|t| !t.text.trim().is_empty());
                        structured_present = resp.structured_content.is_some();
                        if let Some(sc) = resp.structured_content.as_ref() {
                            kind = sc
                                .get("kind")
                                .and_then(|v| v.as_str())
                                .map(|s| s.to_string());
                            ok = sc.get("ok").and_then(|v| v.as_bool()).unwrap_or(true);
                        }
                    }
                    Err(e) => {
                        ok = false;
                        err = Some(e.to_string());
                    }
                }

                runs.push(serde_json::json!({
                    "tool": name,
                    "ok": ok,
                    "elapsed_ms": t0.elapsed().as_millis(),
                    "markdown_present": markdown_present,
                    "structured_present": structured_present,
                    "kind": kind,
                    "args": args_json,
                    "error": err,
                }));
            }

            // Finally, query usage within the same long-lived child server so counts are meaningful.
            let usage = async {
                let resp = tokio::time::timeout(
                    std::time::Duration::from_millis(args.timeout_ms),
                    service.call_tool(CallToolRequestParam {
                        name: "webpipe_usage".into(),
                        arguments: Some(
                            serde_json::json!({})
                                .as_object()
                                .cloned()
                                .unwrap_or_default(),
                        ),
                    }),
                )
                .await
                .ok()?
                .ok()?;
                resp.structured_content
            }
            .await;

            let payload = serde_json::json!({
                "schema_version": 1,
                "kind": "mcp_sweep",
                "ok": runs.iter().all(|r| r.get("ok").and_then(|v| v.as_bool()).unwrap_or(false)),
                "tool_count": tools.tools.len(),
                "runs": runs,
                "usage": usage,
            });

            match args.output.to_ascii_lowercase().as_str() {
                "json" => println!("{payload}"),
                _ => println!("{}", serde_json::to_string_pretty(&payload)?),
            }

            let _ = service.cancel().await;
        }
        #[cfg(feature = "eval")]
        Commands::EvalTranscriptSummarize(args) => {
            let now = args.now_epoch_s.unwrap_or_else(|| {
                std::time::SystemTime::now()
                    .duration_since(std::time::UNIX_EPOCH)
                    .unwrap_or_default()
                    .as_secs()
            });
            let out = args.out.unwrap_or_else(|| {
                std::path::PathBuf::from(format!(
                    ".generated/webpipe-eval-transcript-summary-{now}.json"
                ))
            });
            if let Some(p) = out.parent() {
                std::fs::create_dir_all(p)?;
            }

            let run_kind_filter = args
                .run_kind
                .as_deref()
                .map(|s| s.trim())
                .filter(|s| !s.is_empty());
            let stage_filter = args
                .stage
                .as_deref()
                .map(|s| s.trim())
                .filter(|s| !s.is_empty());
            let top_k = args.top_k.clamp(1, 200);

            #[derive(Clone, Debug)]
            struct SlowEvent {
                elapsed_ms: u64,
                call_id: String,
                run_kind: String,
                stage: String,
                kind: String,
                tool_name: Option<String>,
                llm_backend: Option<String>,
            }

            fn extract_elapsed_ms(v: &serde_json::Value) -> Option<u64> {
                if let Some(ms) = v.get("timing_ms").and_then(|x| x.as_u64()) {
                    return Some(ms);
                }
                v.get("tool")
                    .and_then(|t| t.get("elapsed_ms"))
                    .and_then(|x| x.as_u64())
            }

            use std::io::BufRead;
            let f = std::fs::File::open(&args.transcript_jsonl)?;
            let mut reader = std::io::BufReader::new(f);
            let mut line = String::new();

            let mut total_lines: u64 = 0;
            let mut parsed_lines: u64 = 0;
            let mut parse_failed: u64 = 0;
            let mut included: u64 = 0;

            let mut counts_by_stage: std::collections::BTreeMap<String, u64> =
                std::collections::BTreeMap::new();
            let mut counts_by_run_kind: std::collections::BTreeMap<String, u64> =
                std::collections::BTreeMap::new();
            let mut critic_issue_counts: std::collections::BTreeMap<String, u64> =
                std::collections::BTreeMap::new();
            let mut critic_issue_unknown: u64 = 0;
            let mut judge_musing_dimension_counts: std::collections::BTreeMap<String, u64> =
                std::collections::BTreeMap::new();
            let mut judge_musing_dimension_unknown: u64 = 0;
            let mut slow: Vec<SlowEvent> = Vec::new();

            loop {
                line.clear();
                let n = reader.read_line(&mut line)?;
                if n == 0 {
                    break;
                }
                total_lines = total_lines.saturating_add(1);
                let s = line.trim();
                if s.is_empty() {
                    continue;
                }
                let v: serde_json::Value = match serde_json::from_str(s) {
                    Ok(v) => {
                        parsed_lines = parsed_lines.saturating_add(1);
                        v
                    }
                    Err(_) => {
                        parse_failed = parse_failed.saturating_add(1);
                        continue;
                    }
                };

                let run_kind = v
                    .get("run_kind")
                    .and_then(|x| x.as_str())
                    .unwrap_or("")
                    .trim()
                    .to_string();
                let stage = v
                    .get("stage")
                    .and_then(|x| x.as_str())
                    .unwrap_or("")
                    .trim()
                    .to_string();
                if let Some(fk) = run_kind_filter {
                    if run_kind != fk {
                        continue;
                    }
                }
                if let Some(fs) = stage_filter {
                    if stage != fs {
                        continue;
                    }
                }

                included = included.saturating_add(1);
                *counts_by_run_kind.entry(run_kind.clone()).or_insert(0) += 1;
                *counts_by_stage.entry(stage.clone()).or_insert(0) += 1;

                // Optional: aggregate deterministic critic issues when present.
                if stage == "critic" {
                    if let Some(issues) = v
                        .get("response")
                        .and_then(|r| r.get("parsed"))
                        .and_then(|p| p.get("issues"))
                        .and_then(|x| x.as_array())
                    {
                        for it in issues {
                            let s = it.as_str().unwrap_or("").trim();
                            if s.is_empty() || s.len() > 96 {
                                critic_issue_unknown = critic_issue_unknown.saturating_add(1);
                                continue;
                            }
                            if CRITIC_ISSUE_VOCAB.contains(&s) {
                                *critic_issue_counts.entry(s.to_string()).or_insert(0) += 1;
                            } else {
                                critic_issue_unknown = critic_issue_unknown.saturating_add(1);
                            }
                        }
                    }
                }

                // Optional: aggregate judge musing dimensions (privacy-safe; structured).
                if stage == "judge" {
                    if let Some(ms) = v
                        .get("response")
                        .and_then(|r| r.get("parsed"))
                        .and_then(|p| p.get("musings"))
                        .and_then(|x| x.as_array())
                    {
                        for it in ms {
                            let dim = it
                                .get("dimension")
                                .and_then(|x| x.as_str())
                                .unwrap_or("")
                                .trim();
                            if dim.is_empty() || dim.len() > 32 {
                                judge_musing_dimension_unknown =
                                    judge_musing_dimension_unknown.saturating_add(1);
                                continue;
                            }
                            *judge_musing_dimension_counts
                                .entry(dim.to_string())
                                .or_insert(0) += 1;
                        }
                    }
                }

                let elapsed_ms = extract_elapsed_ms(&v).unwrap_or(0);
                let call_id = v
                    .get("call_id")
                    .and_then(|x| x.as_str())
                    .unwrap_or("")
                    .to_string();
                let kind = v
                    .get("kind")
                    .and_then(|x| x.as_str())
                    .unwrap_or("")
                    .to_string();
                let tool_name = v
                    .get("tool")
                    .and_then(|t| t.get("name"))
                    .and_then(|x| x.as_str())
                    .map(|s| s.to_string());
                let llm_backend = v
                    .get("llm")
                    .and_then(|t| t.get("backend"))
                    .and_then(|x| x.as_str())
                    .map(|s| s.to_string());

                slow.push(SlowEvent {
                    elapsed_ms,
                    call_id,
                    run_kind,
                    stage,
                    kind,
                    tool_name,
                    llm_backend,
                });
            }

            slow.sort_by_key(|e| std::cmp::Reverse(e.elapsed_ms));
            slow.truncate(top_k);
            let slow_json: Vec<serde_json::Value> = slow
                .into_iter()
                .map(|e| {
                    serde_json::json!({
                        "elapsed_ms": e.elapsed_ms,
                        "call_id": e.call_id,
                        "run_kind": e.run_kind,
                        "stage": e.stage,
                        "kind": e.kind,
                        "tool_name": e.tool_name,
                        "llm_backend": e.llm_backend
                    })
                })
                .collect();

            let payload = serde_json::json!({
                "schema_version": 1,
                "kind": "webpipe_eval_transcript_summary",
                "generated_at_epoch_s": now,
                "ok": parse_failed == 0,
                "inputs": {
                    "transcript_jsonl": args.transcript_jsonl,
                    "run_kind": args.run_kind,
                    "stage": args.stage,
                    "top_k": top_k
                },
                "totals": {
                    // "lines/parsed/parse_failed/included" are the low-level scanning counters.
                    "lines": total_lines,
                    "parsed": parsed_lines,
                    "parse_failed": parse_failed,
                    "included": included,
                    // Back-compat aliases (contract tests): treat "events" as "included".
                    "events": included,
                    "parse_failed_events": parse_failed,
                    "by_stage": counts_by_stage,
                    "by_run_kind": counts_by_run_kind
                },
                "critic": {
                    "issue_counts": critic_issue_counts,
                    "unknown_issue_count": critic_issue_unknown
                },
                "judge_musings": {
                    "dimension_counts": judge_musing_dimension_counts,
                    "unknown_dimension_count": judge_musing_dimension_unknown
                },
                "slowest": slow_json
            });
            std::fs::write(&out, serde_json::to_string_pretty(&payload)? + "\n")?;
            println!("{}", out.display());
        }
        #[cfg(all(feature = "eval", feature = "vlm"))]
        Commands::EvalVlmSummarize(args) => {
            let now = args.now_epoch_s.unwrap_or_else(|| {
                std::time::SystemTime::now()
                    .duration_since(std::time::UNIX_EPOCH)
                    .unwrap_or_default()
                    .as_secs()
            });
            let out = args.out.unwrap_or_else(|| {
                std::path::PathBuf::from(format!(".generated/webpipe-eval-vlm-summary-{now}.json"))
            });
            if let Some(p) = out.parent() {
                std::fs::create_dir_all(p)?;
            }

            #[derive(Clone, Debug)]
            struct FixCount {
                fix: String,
                count: u64,
            }

            let input_paths: Vec<String> = args
                .inputs
                .iter()
                .map(|p| p.display().to_string())
                .collect();

            let mut per_input = Vec::new();
            let mut runs: u64 = 0;
            let mut parsed_ok: u64 = 0;

            let mut fixes: std::collections::BTreeMap<String, u64> =
                std::collections::BTreeMap::new();
            let mut issues_by_area: std::collections::BTreeMap<String, u64> =
                std::collections::BTreeMap::new();
            let mut issues_by_area_norm: std::collections::BTreeMap<String, u64> =
                std::collections::BTreeMap::new();
            let mut themes: std::collections::BTreeMap<String, u64> =
                std::collections::BTreeMap::new();
            let mut theme_fix_counts: std::collections::BTreeMap<
                String,
                std::collections::BTreeMap<String, u64>,
            > = std::collections::BTreeMap::new();

            fn norm_area(s: &str) -> String {
                let t = s.trim().to_ascii_lowercase();
                // Normalize a few frequent near-duplicates (keeps reports consistent across models).
                if t.contains("navigation") {
                    return "navigation".to_string();
                }
                if t.contains("typography") {
                    return "typography".to_string();
                }
                if t.contains("spacing") || t.contains("layout") {
                    return "layout_spacing".to_string();
                }
                if t.contains("hierarchy") {
                    return "visual_hierarchy".to_string();
                }
                if t.contains("information") {
                    return "information_architecture".to_string();
                }
                t.replace(['/', ' '], "_")
            }

            fn theme_for_fix(fix: &str) -> Option<&'static str> {
                let t = fix.trim().to_ascii_lowercase();
                if t.is_empty() {
                    return None;
                }
                if (t.contains("table of contents") || t.contains("contents") || t.contains("toc"))
                    && (t.contains("sidebar")
                        || t.contains("two-column")
                        || t.contains("two column")
                        || t.contains("sticky"))
                {
                    return Some("toc_sidebar");
                }
                if t.contains("margin")
                    || t.contains("padding")
                    || t.contains("gap")
                    || t.contains("whitespace")
                    || t.contains("spacing")
                {
                    return Some("tighten_spacing");
                }
                if t.contains("legend")
                    || t.contains("glossary")
                    || (t.contains("define") && t.contains("parameter"))
                {
                    return Some("parameter_legend");
                }
                if t.contains("contrast") || t.contains("darken") || t.contains("legibility") {
                    return Some("metadata_contrast");
                }
                if t.contains("underline") || t.contains("hover") || t.contains("interactive") {
                    return Some("toc_link_affordance");
                }
                None
            }

            for path in &args.inputs {
                let raw = std::fs::read_to_string(path)?;
                let v: serde_json::Value = serde_json::from_str(&raw)?;
                let kind = v.get("kind").and_then(|x| x.as_str()).unwrap_or("");
                let schema_version = v
                    .get("schema_version")
                    .and_then(|x| x.as_u64())
                    .unwrap_or(0);
                if kind != "webpipe_vlm_openrouter" || schema_version != 1 {
                    anyhow::bail!(
                        "unexpected input artifact kind/schema_version for {}",
                        path.display()
                    );
                }

                runs = runs.saturating_add(1);
                let ok = v.get("ok").and_then(|x| x.as_bool()).unwrap_or(false);
                let parsed_ok_i = v
                    .get("parsed_ok")
                    .and_then(|x| x.as_bool())
                    .unwrap_or(false);
                if parsed_ok_i {
                    parsed_ok = parsed_ok.saturating_add(1);
                }

                let img = v
                    .get("inputs")
                    .and_then(|x| x.get("image_path"))
                    .and_then(|x| x.as_str())
                    .unwrap_or("")
                    .to_string();

                let mut score: Option<f64> = None;
                let mut top_3_fixes: Vec<String> = Vec::new();
                let mut issues: Vec<serde_json::Value> = Vec::new();

                if let Some(p) = v.get("parsed") {
                    score = p.get("score_0_10").and_then(|x| x.as_f64());
                    if let Some(arr) = p.get("top_3_fixes").and_then(|x| x.as_array()) {
                        for s in arr.iter().filter_map(|x| x.as_str()) {
                            let s = s.trim();
                            if s.is_empty() {
                                continue;
                            }
                            top_3_fixes.push(s.to_string());
                            *fixes.entry(s.to_string()).or_insert(0) += 1;
                            if let Some(theme) = theme_for_fix(s) {
                                let t = theme.to_string();
                                *themes.entry(t.clone()).or_insert(0) += 1;
                                *theme_fix_counts
                                    .entry(t)
                                    .or_default()
                                    .entry(s.to_string())
                                    .or_insert(0) += 1;
                            }
                        }
                    }
                    if let Some(arr) = p.get("issues").and_then(|x| x.as_array()) {
                        for it in arr {
                            issues.push(it.clone());
                            if let Some(area) = it.get("area").and_then(|x| x.as_str()) {
                                let a = area.trim();
                                if !a.is_empty() {
                                    *issues_by_area.entry(a.to_string()).or_insert(0) += 1;
                                    *issues_by_area_norm.entry(norm_area(a)).or_insert(0) += 1;
                                }
                            }
                        }
                    }
                }

                per_input.push(serde_json::json!({
                    "input_path": path.display().to_string(),
                    "image_path": img,
                    "ok": ok,
                    "parsed_ok": parsed_ok_i,
                    "score_0_10": score,
                    "top_3_fixes": top_3_fixes,
                    "issues": issues
                }));
            }

            let mut fixes_vec: Vec<FixCount> = fixes
                .into_iter()
                .map(|(fix, count)| FixCount { fix, count })
                .collect();
            fixes_vec.sort_by(|a, b| b.count.cmp(&a.count).then_with(|| a.fix.cmp(&b.fix)));

            let top_3_fixes: Vec<String> =
                fixes_vec.iter().take(3).map(|fc| fc.fix.clone()).collect();

            let mut themes_vec = themes.clone().into_iter().collect::<Vec<_>>();
            themes_vec.sort_by(|a, b| b.1.cmp(&a.1).then_with(|| a.0.cmp(&b.0)));
            let top_3_themes: Vec<String> =
                themes_vec.into_iter().take(3).map(|(k, _)| k).collect();
            let mut top_3_fixes_by_theme: Vec<String> = Vec::new();
            for theme in &top_3_themes {
                if let Some(m) = theme_fix_counts.get(theme) {
                    let mut v = m.iter().map(|(k, c)| (k.clone(), *c)).collect::<Vec<_>>();
                    v.sort_by(|a, b| b.1.cmp(&a.1).then_with(|| a.0.cmp(&b.0)));
                    if let Some((fix, _)) = v.into_iter().next() {
                        top_3_fixes_by_theme.push(fix);
                    }
                }
            }

            let payload = serde_json::json!({
                "schema_version": 1,
                "kind": "webpipe_eval_vlm_summary",
                "generated_at_epoch_s": now,
                "inputs": {
                    "count": input_paths.len(),
                    "paths": input_paths,
                },
                "totals": {
                    "runs": runs,
                    "parsed_ok": parsed_ok,
                },
                "top_3_fixes": top_3_fixes,
                "fix_counts": fixes_vec.iter().take(50).map(|fc| serde_json::json!({"fix": fc.fix, "count": fc.count})).collect::<Vec<_>>(),
                "issues_by_area": issues_by_area,
                "issues_by_area_norm": issues_by_area_norm,
                "themes": themes,
                "top_3_themes": top_3_themes,
                "top_3_fixes_by_theme": top_3_fixes_by_theme,
                "per_input": per_input,
            });

            std::fs::write(&out, serde_json::to_string_pretty(&payload)? + "\n")?;
            println!("{}", out.display());
        }
        #[cfg(all(feature = "eval", feature = "vlm"))]
        Commands::EvalVlmRun(args) => {
            let t0 = std::time::Instant::now();
            let transcript_seq0: u64 = 0;
            fn env(key: &str) -> Option<String> {
                std::env::var(key)
                    .ok()
                    .map(|s| s.trim().to_string())
                    .filter(|s| !s.is_empty())
            }
            fn is_image_like(p: &std::path::Path) -> bool {
                matches!(
                    p.extension()
                        .and_then(|s| s.to_str())
                        .unwrap_or("")
                        .to_ascii_lowercase()
                        .as_str(),
                    "png" | "jpg" | "jpeg" | "webp" | "gif"
                )
            }

            let now = args.now_epoch_s.unwrap_or_else(|| {
                std::time::SystemTime::now()
                    .duration_since(std::time::UNIX_EPOCH)
                    .unwrap_or_default()
                    .as_secs()
            });

            // Goal profile registry (built-ins). Keep this small and curated.
            #[derive(Clone, Debug)]
            struct GoalProfile {
                id: &'static str,
                description: &'static str,
                goals: &'static [&'static str],
            }
            static GOAL_PROFILES: &[GoalProfile] = &[
                GoalProfile {
                    id: "sinprimes_story_v1",
                    description: "sinprimes story page: reading flow, plot legibility, low-risk CSS refinements, with P0/P1-first severity.",
                    goals: &[
                        "Fix P0/P1 issues before any purely cosmetic changes.",
                        "Prefer small, local CSS/HTML tweaks over layout rewrites.",
                        "Keep navigation/TOC orientation strong on both desktop and mobile slices.",
                        "Make plots readable at a glance (ticks/labels/legends) without needing zoom.",
                        "Prevent anchor jumps from clipping headings under fixed UI (progress bar).",
                        "Keep tables scanable (alignment, striping, header separation) without increasing visual noise.",
                        "Treat math typesetting issues as P0 only when they break meaning/legibility (overflow, baseline collisions).",
                    ],
                },
            ];
            fn goal_profile_by_id(id: &str) -> Option<&'static GoalProfile> {
                let t = id.trim();
                if t.is_empty() {
                    return None;
                }
                GOAL_PROFILES.iter().find(|gp| gp.id == t)
            }

            if args.list_goal_profiles {
                let payload = serde_json::json!({
                    "schema_version": 1,
                    "kind": "webpipe_eval_vlm_goal_profiles",
                    "profiles": GOAL_PROFILES.iter().map(|gp| serde_json::json!({
                        "id": gp.id,
                        "description": gp.description,
                        "goals": gp.goals,
                    })).collect::<Vec<_>>(),
                });
                println!("{}", serde_json::to_string_pretty(&payload)?);
                return Ok(());
            }

            // Collect images from explicit args and/or directory scan.
            let mut images: Vec<std::path::PathBuf> = args.images.clone();
            if let Some(dir) = args.images_dir.as_ref() {
                let max_images = args.max_images.clamp(0, 500);
                if max_images > 0 {
                    let mut from_dir: Vec<std::path::PathBuf> = Vec::new();
                    for ent in std::fs::read_dir(dir)? {
                        let ent = ent?;
                        let p = ent.path();
                        if p.is_file() && is_image_like(&p) {
                            from_dir.push(p);
                        }
                    }
                    from_dir.sort_by_key(|a| a.display().to_string());
                    from_dir.truncate(max_images);
                    images.extend(from_dir);
                }
            }
            images.sort_by_key(|a| a.display().to_string());
            images.dedup();
            if images.is_empty() {
                anyhow::bail!("no images provided (use --image ... or --images-dir ...)");
            }

            let out_dir = args.out_dir.unwrap_or_else(|| {
                std::path::PathBuf::from(format!(".generated/webpipe-eval-vlm-run-{now}"))
            });
            std::fs::create_dir_all(&out_dir)?;
            let out_summary = args
                .out_summary
                .unwrap_or_else(|| out_dir.join("summary.json"));
            // Transcript defaults: keep eval runs auditable by default, and put the JSONL next
            // to the other run artifacts so downstream tooling can discover it.
            let transcript_jsonl_path = args
                .transcript_jsonl
                .clone()
                .unwrap_or_else(|| out_dir.join("eval-vlm-run.transcript.jsonl"));
            if let Some(p) = transcript_jsonl_path.parent() {
                std::fs::create_dir_all(p)?;
            }
            let _ = std::fs::OpenOptions::new()
                .create(true)
                .append(true)
                .open(&transcript_jsonl_path)?;

            let api_key = env("WEBPIPE_OPENROUTER_API_KEY")
                .or_else(|| env("OPENROUTER_API_KEY"))
                .ok_or_else(|| {
                    anyhow::anyhow!("missing OPENROUTER_API_KEY (or WEBPIPE_OPENROUTER_API_KEY)")
                })?;
            let model = args
                .model
                .or_else(|| env("WEBPIPE_OPENROUTER_MODEL"))
                .unwrap_or_else(|| "google/gemini-3-flash-preview".to_string());
            let trials = args.trials.clamp(1, 20);
            let temperature = args.temperature.clamp(0.0, 2.0);
            let base = env("WEBPIPE_OPENROUTER_BASE_URL")
                .unwrap_or_else(|| "https://openrouter.ai/api".to_string());
            let url = format!("{}/v1/chat/completions", base.trim_end_matches('/'));

            let prompt = args.prompt.unwrap_or_else(|| {
                "Return ONE JSON object only (no markdown, no prose).\n\nYou are critiquing a *rendered technical page screenshot* (sometimes full-page and very tall). Use BOTH the screenshot and any provided Render Context (computed CSS, TOC state, console/network errors).\n\nSchema (required keys):\n- overall: string (2–4 sentences; include 1 sentence on strengths, 1 on biggest risks)\n- score_0_10: number\n- verdict: \"good\"|\"mixed\"|\"bad\"\n- strengths: string[] (3–6 concrete positives)\n- issues: array of {\n    area: \"navigation\"|\"layout_spacing\"|\"typography\"|\"visual_hierarchy\"|\"plots_figures\"|\"tables_data\"|\"math_typesetting\"|\"writing_clarity\"|\"accessibility\"|\"technical_runtime\",\n    severity: \"P0\"|\"P1\"|\"P2\",\n    region: \"top\"|\"middle\"|\"bottom\"|\"global\",\n    problem: string,\n    evidence: string,\n    fix: string\n  }\n- top_3_fixes: string[] (ranked, highest leverage)\n\nRules:\n- Evidence must be concrete and locally grounded (quote a label, mention a specific UI element, or cite a render-context field like computed_styles.* / console.errors count).\n- Fixes must be actionable (specific CSS/layout/typography changes), not generic.\n- Keep each issues[].{problem,evidence,fix} <= 200 chars.\n- Keep each top_3_fixes entry <= 96 chars.\n- Do NOT recommend changes already present per render context.\n  - If computed_styles.html.scroll-padding-top is nonzero, do NOT suggest adding/changing scroll-padding-top unless you have evidence headings are still clipped.\n  - If computed_styles.h2.scroll-margin-top is nonzero, do NOT suggest adding/changing scroll-margin-top unless you cite a clipped heading.\n  - If computed_styles.katex_display.overflow-x is \"auto\" or \"scroll\", do NOT suggest adding overflow-x: auto for display math.\n- If this screenshot is a partial viewport (not full-page), say so in overall and avoid claims about unseen sections."
                    .to_string()
            });
            let mut prompt_effective = prompt.clone();
            if args.context_dir.is_some() {
                // A short marker in the user prompt so the model knows extra context follows.
                prompt_effective.push_str(
                    "\n\n(Additional render context is provided below; consider it alongside the screenshot.)\n",
                );
            }
            let (_prompt_tx, _) = vlm_truncate_chars(&prompt_effective, args.transcript_max_chars);

            // Optional, stable judge goals (priority ordered).
            fn load_goals_file(p: &std::path::Path) -> anyhow::Result<Vec<String>> {
                let raw = std::fs::read_to_string(p)?;
                let mut out: Vec<String> = Vec::new();
                for line in raw.lines() {
                    let t = line.trim();
                    if t.is_empty() || t.starts_with('#') {
                        continue;
                    }
                    out.push(t.to_string());
                }
                Ok(out)
            }
            fn dedup_keep_first(v: Vec<String>) -> Vec<String> {
                let mut seen: std::collections::BTreeMap<String, ()> =
                    std::collections::BTreeMap::new();
                let mut out: Vec<String> = Vec::new();
                for s in v {
                    let t = s.trim();
                    if t.is_empty() {
                        continue;
                    }
                    if seen.contains_key(t) {
                        continue;
                    }
                    seen.insert(t.to_string(), ());
                    out.push(t.to_string());
                }
                out
            }
            fn format_goals_block(goals: &[String]) -> String {
                if goals.is_empty() {
                    return String::new();
                }
                let mut s = String::new();
                s.push_str("## Goals (priority order)\n");
                for g in goals {
                    let gg = g.trim();
                    if gg.is_empty() {
                        continue;
                    }
                    s.push_str("- ");
                    s.push_str(gg);
                    s.push('\n');
                }
                s.push_str(
                    "\nRules for goals:\n- Use these goals to set severity (P0/P1/P2) and to choose top_3_fixes.\n- If goals conflict, say which you prioritized and why.\n",
                );
                s
            }

            let mut goals0: Vec<String> = Vec::new();
            // 1) explicit goals first (highest priority; argv order)
            goals0.extend(args.goals.iter().cloned());
            // 2) goals file next
            if let Some(p) = args.goals_file.as_ref() {
                goals0.extend(load_goals_file(p.as_path())?);
            }
            // 3) goal profiles last (baseline)
            let mut goal_profiles_used: Vec<String> = Vec::new();
            for gp in &args.goal_profiles {
                let id = gp.trim();
                if id.is_empty() {
                    continue;
                }
                let prof = goal_profile_by_id(id).ok_or_else(|| {
                    anyhow::anyhow!("unknown --goal-profile {id:?} (try --list-goal-profiles)")
                })?;
                goal_profiles_used.push(prof.id.to_string());
                for g in prof.goals {
                    goals0.push((*g).to_string());
                }
            }
            let goals = std::sync::Arc::new(dedup_keep_first(goals0));
            let goal_profiles_used = std::sync::Arc::new(goal_profiles_used);

            // Aggregation state (same spirit as eval-vlm-summarize, but computed during the run).
            let mut per_input = Vec::new();
            let mut runs: u64 = 0;
            let mut parsed_ok: u64 = 0;
            let mut fixes: std::collections::BTreeMap<String, u64> =
                std::collections::BTreeMap::new();
            let mut issues_by_area: std::collections::BTreeMap<String, u64> =
                std::collections::BTreeMap::new();
            let mut issues_by_area_norm: std::collections::BTreeMap<String, u64> =
                std::collections::BTreeMap::new();
            let mut themes: std::collections::BTreeMap<String, u64> =
                std::collections::BTreeMap::new();
            let mut theme_fix_counts: std::collections::BTreeMap<
                String,
                std::collections::BTreeMap<String, u64>,
            > = std::collections::BTreeMap::new();
            let mut by_profile: std::collections::BTreeMap<String, serde_json::Value> =
                std::collections::BTreeMap::new();

            fn norm_area(s: &str) -> String {
                let t = s.trim().to_ascii_lowercase();
                if t.contains("navigation") {
                    return "navigation".to_string();
                }
                if t.contains("typography") {
                    return "typography".to_string();
                }
                if t.contains("spacing") || t.contains("layout") {
                    return "layout_spacing".to_string();
                }
                if t.contains("hierarchy") {
                    return "visual_hierarchy".to_string();
                }
                if t.contains("information") {
                    return "information_architecture".to_string();
                }
                t.replace(['/', ' '], "_")
            }
            fn theme_for_fix(fix: &str) -> Option<&'static str> {
                let t = fix.trim().to_ascii_lowercase();
                if t.is_empty() {
                    return None;
                }
                if (t.contains("table of contents") || t.contains("contents") || t.contains("toc"))
                    && (t.contains("sidebar")
                        || t.contains("two-column")
                        || t.contains("two column")
                        || t.contains("sticky"))
                {
                    return Some("toc_sidebar");
                }
                if t.contains("margin")
                    || t.contains("padding")
                    || t.contains("gap")
                    || t.contains("whitespace")
                    || t.contains("spacing")
                {
                    return Some("tighten_spacing");
                }
                if t.contains("legend")
                    || t.contains("glossary")
                    || (t.contains("define") && t.contains("parameter"))
                {
                    return Some("parameter_legend");
                }
                if t.contains("contrast") || t.contains("darken") || t.contains("legibility") {
                    return Some("metadata_contrast");
                }
                if t.contains("underline") || t.contains("hover") || t.contains("interactive") {
                    return Some("toc_link_affordance");
                }
                None
            }

            fn sanitize_stem(s: &str) -> String {
                // Deterministic, filename-safe-ish stem: keep ASCII alnum plus a few separators.
                // This avoids collisions for same-stem across dirs when combined with the index prefix.
                let mut out = String::with_capacity(s.len());
                for ch in s.chars() {
                    let ok = ch.is_ascii_alphanumeric() || matches!(ch, '-' | '_' | '.');
                    out.push(if ok { ch } else { '_' });
                }
                let t = out.trim_matches('_').to_string();
                if t.is_empty() {
                    "image".to_string()
                } else {
                    t
                }
            }

            fn sanitize_id(s: &str) -> String {
                let mut out = String::new();
                for ch in s.trim().chars() {
                    let ok = ch.is_ascii_alphanumeric() || matches!(ch, '-' | '_' | '.');
                    out.push(if ok { ch } else { '_' });
                }
                let t = out.trim_matches('_').to_string();
                if t.is_empty() {
                    "general".to_string()
                } else {
                    t
                }
            }
            fn profile_prompt_suffix(profile: &str) -> &'static str {
                match profile {
                    "ux_layout" => "Profile focus: UX + layout. Prioritize navigation, spacing, and reading flow.\n",
                    "tables" => "Profile focus: tables + data legibility. Prioritize row/column scanability, numeric alignment, borders/striping.\n",
                    "plots" => "Profile focus: plots + figures. Prioritize axis/legend/title readability, export sizing, and label overlap.\n",
                    "math" => "Profile focus: math typesetting. Prioritize KaTeX baseline, display-math spacing, and overflow.\n",
                    "writing" => "Profile focus: writing clarity. Prioritize structure, definitions, redundancy, and reader orientation.\n",
                    "runtime" => "Profile focus: runtime/console/network. Prioritize JS errors, missing assets, and render warnings.\n",
                    _ => "Profile focus: general.\n",
                }
            }
            fn blake3_hex(s: &str) -> String {
                blake3::hash(s.as_bytes()).to_hex().to_string()
            }

            let profiles: Vec<String> = if args.profiles.is_empty() {
                vec!["general".to_string()]
            } else {
                let mut v = args
                    .profiles
                    .iter()
                    .map(|s| sanitize_id(s))
                    .collect::<Vec<_>>();
                v.sort();
                v.dedup();
                v
            };
            let cache_mode = args.cache.trim().to_ascii_lowercase();
            let max_inflight = args.max_inflight.clamp(1, 32);

            // Fan-out tasks: images × profiles × trials.
            #[derive(Clone, Debug)]
            struct TaskSpec {
                image_index: usize,
                image_path: std::path::PathBuf,
                image_stem: String,
                profile: String,
                trial: u64,
            }
            #[derive(Clone, Debug)]
            struct TaskOut {
                spec: TaskSpec,
                artifact_path: std::path::PathBuf,
                artifact_file: String,
                payload: serde_json::Value,
                _reused: bool,
                _elapsed_ms: u64,
            }

            let context_dir = args.context_dir.clone();
            let context_max_chars = args.context_max_chars;
            let max_chars = args.max_chars;
            let transcript_jsonl = std::sync::Arc::new(Some(transcript_jsonl_path.clone()));
            let transcript_max_chars = args.transcript_max_chars;
            let use_legacy_name = profiles.len() == 1 && profiles[0].as_str() == "general";

            let mut tasks: Vec<TaskSpec> = Vec::new();
            for (idx0, img_path) in images.iter().enumerate() {
                let stem0 = img_path
                    .file_stem()
                    .and_then(|s| s.to_str())
                    .unwrap_or("image");
                let stem = sanitize_stem(stem0);
                for profile in &profiles {
                    for t_idx in 0..trials {
                        tasks.push(TaskSpec {
                            image_index: idx0,
                            image_path: img_path.clone(),
                            image_stem: stem.clone(),
                            profile: profile.clone(),
                            trial: t_idx + 1,
                        });
                    }
                }
            }

            let http = std::sync::Arc::new(reqwest::Client::new());
            let sem = std::sync::Arc::new(tokio::sync::Semaphore::new(max_inflight));
            let tx_seq = std::sync::Arc::new(std::sync::atomic::AtomicU64::new(transcript_seq0));
            let tx_lock = std::sync::Arc::new(tokio::sync::Mutex::new(()));

            let base_prompt = prompt.clone();
            let base_prompt_with_marker = prompt_effective.clone();
            let prompt_tx_base = {
                let (p, _) = vlm_truncate_chars(&base_prompt_with_marker, transcript_max_chars);
                p
            };

            let mut handles: Vec<tokio::task::JoinHandle<anyhow::Result<TaskOut>>> = Vec::new();
            for spec in tasks {
                let http = http.clone();
                let sem = sem.clone();
                let tx_seq = tx_seq.clone();
                let tx_lock = tx_lock.clone();
                let url = url.clone();
                let api_key = api_key.clone();
                let model = model.clone();
                let out_dir = out_dir.clone();
                let cache_mode = cache_mode.clone();
                let context_dir = context_dir.clone();
                let base_prompt = base_prompt.clone();
                let goals = goals.clone();
                let goal_profiles_used = goal_profiles_used.clone();
                let transcript_jsonl = transcript_jsonl.clone();
                let prompt_tx_base = prompt_tx_base.clone();

                handles.push(tokio::spawn(async move {
                    let _permit = sem.acquire().await?;
                    let t0_one = std::time::Instant::now();

                    let mut prompt_effective = base_prompt.clone();
                    prompt_effective.push_str("\n\n");
                    prompt_effective.push_str(profile_prompt_suffix(spec.profile.as_str()));
                    if !goals.is_empty() {
                        prompt_effective.push('\n');
                        prompt_effective.push_str(format_goals_block(goals.as_ref()).as_str());
                    }
                    if context_dir.is_some() {
                        prompt_effective.push_str(
                            "\n(Additional render context is provided below; consider it alongside the screenshot.)\n",
                        );
                    }

                    let ctx_txt = context_dir.as_ref().and_then(|d| {
                        vlm_load_context_for_image(d.as_path(), spec.image_path.as_path(), context_max_chars)
                    });
                    let prompt_trial = if let Some(ctx_txt) = ctx_txt.as_ref() {
                        format!("{prompt_effective}\n\n## Render context (from Playwright)\n{ctx_txt}\n")
                    } else {
                        prompt_effective.clone()
                    };
                    let prompt_hash = blake3_hex(prompt_trial.as_str());
                    let context_hash = ctx_txt
                        .as_ref()
                        .map(|s| blake3_hex(s.as_str()))
                        .unwrap_or_else(|| "none".to_string());

                    let file = if use_legacy_name {
                        format!(
                            "{:03}-{}.t{:02}.vlm_openrouter.json",
                            spec.image_index + 1,
                            spec.image_stem,
                            spec.trial
                        )
                    } else {
                        let prof = sanitize_id(spec.profile.as_str());
                        format!(
                            "{:03}-{}.{}.t{:02}.vlm_openrouter.json",
                            spec.image_index + 1,
                            spec.image_stem,
                            prof,
                            spec.trial
                        )
                    };
                    let out_path = out_dir.join(&file);

                    if cache_mode != "off" && out_path.exists() {
                        if let Ok(raw) = std::fs::read_to_string(&out_path) {
                            if let Ok(v) = serde_json::from_str::<serde_json::Value>(&raw) {
                                let reuse = if cache_mode == "reuse_any" {
                                    true
                                } else {
                                    let im = v.get("inputs").cloned().unwrap_or(serde_json::Value::Null);
                                    let m0 = im.get("model").and_then(|x| x.as_str()).unwrap_or("");
                                    let t0 = im.get("temperature").and_then(|x| x.as_f64()).unwrap_or(-1.0);
                                    let ph0 = im.get("prompt_hash").and_then(|x| x.as_str()).unwrap_or("");
                                    let ch0 = im.get("context_hash").and_then(|x| x.as_str()).unwrap_or("");
                                    m0 == model.as_str()
                                        && (t0 - temperature).abs() < 1e-9
                                        && ph0 == prompt_hash.as_str()
                                        && ch0 == context_hash.as_str()
                                };
                                if reuse {
                                    return Ok(TaskOut {
                                        spec,
                                        artifact_path: out_path,
                                        artifact_file: file,
                                        payload: v,
                                        _reused: true,
                                        _elapsed_ms: t0_one.elapsed().as_millis() as u64,
                                    });
                                }
                            }
                        }
                    }

                    let payload0 = vlm_openrouter_call(
                        http.as_ref(),
                        url.as_str(),
                        api_key.as_str(),
                        model.as_str(),
                        prompt_trial.as_str(),
                        spec.image_path.as_path(),
                        max_chars,
                        temperature,
                        now,
                    )
                    .await;
                    let mut payload = payload0;
                    if let Some(m) = payload.get_mut("inputs").and_then(|x| x.as_object_mut()) {
                        m.insert("profile".to_string(), serde_json::Value::String(spec.profile.clone()));
                        m.insert("prompt_hash".to_string(), serde_json::Value::String(prompt_hash));
                        m.insert("context_hash".to_string(), serde_json::Value::String(context_hash));
                        if !goals.is_empty() {
                            m.insert(
                                "goals".to_string(),
                                serde_json::Value::Array(
                                    goals
                                        .iter()
                                        .map(|s| serde_json::Value::String(s.clone()))
                                        .collect(),
                                ),
                            );
                        }
                        if !goal_profiles_used.is_empty() {
                            m.insert(
                                "goal_profiles".to_string(),
                                serde_json::Value::Array(
                                    goal_profiles_used
                                        .iter()
                                        .map(|s| serde_json::Value::String(s.clone()))
                                        .collect(),
                                ),
                            );
                        }
                    }

                    std::fs::write(&out_path, serde_json::to_string_pretty(&payload)? + "\n")?;

                    // Transcript event (optional, serialized).
                    if let Some(tx_path) = transcript_jsonl.as_ref().as_ref() {
                        let _g = tx_lock.lock().await;
                        if let Some(p) = tx_path.parent() {
                            std::fs::create_dir_all(p)?;
                        }
                        let elapsed_ms = t0_one.elapsed().as_millis() as u64;
                        let seq = tx_seq.fetch_add(1, std::sync::atomic::Ordering::Relaxed) + 1;
                        let parsed_ok_i = payload
                            .get("parsed_ok")
                            .and_then(|x| x.as_bool())
                            .unwrap_or(false);
                        let ok = payload.get("ok").and_then(|x| x.as_bool()).unwrap_or(false);
                        let goals_tx: Vec<String> = goals.as_ref().clone();
                        let goal_profiles_tx: Vec<String> = goal_profiles_used.as_ref().clone();
                        let line = serde_json::json!({
                            "schema_version": 1,
                            "kind": "webpipe_eval_transcript_event",
                            "generated_at_epoch_s": now,
                            "seq": seq,
                            "run_kind": "eval_vlm_run",
                            "stage": "vlm_openrouter",
                            "call_id": format!("vlm_openrouter:{}:{}:p{}:t{}", now, spec.image_path.display(), spec.profile, spec.trial),
                            "timing_ms": elapsed_ms,
                            "llm": {
                                "backend": "openrouter",
                                "model_effective": model,
                                "temperature": temperature
                            },
                            "prompt": { "user": prompt_tx_base, "profile": spec.profile, "goals": goals_tx, "goal_profiles": goal_profiles_tx },
                            "inputs": {
                                "image_path": spec.image_path.display().to_string(),
                                "trial": spec.trial
                            },
                            "response": {
                                "ok": ok,
                                "parsed_ok": parsed_ok_i,
                                "parsed": payload.get("parsed").cloned().unwrap_or(serde_json::Value::Null),
                                "http_status": payload.get("http").and_then(|h| h.get("status")).and_then(|x| x.as_u64()),
                            }
                        });
                        vlm_append_jsonl(tx_path.as_path(), &line)?;
                    }

                    Ok(TaskOut {
                        spec,
                        artifact_path: out_path,
                        artifact_file: file,
                        payload,
                        _reused: false,
                        _elapsed_ms: t0_one.elapsed().as_millis() as u64,
                    })
                }));
            }

            let mut outs: Vec<TaskOut> = Vec::new();
            for h in handles {
                outs.push(h.await??);
            }
            // Keep transcript writes bounded + serialized; no stable meaning for seq outside the JSONL file.

            // Collect per-image.
            outs.sort_by(|a, b| {
                (a.spec.image_index, a.spec.profile.clone(), a.spec.trial).cmp(&(
                    b.spec.image_index,
                    b.spec.profile.clone(),
                    b.spec.trial,
                ))
            });

            let mut by_image: std::collections::BTreeMap<usize, Vec<TaskOut>> =
                std::collections::BTreeMap::new();
            for o in outs {
                by_image.entry(o.spec.image_index).or_default().push(o);
            }

            let mut by_profile_runs: std::collections::BTreeMap<String, u64> =
                std::collections::BTreeMap::new();
            let mut by_profile_parsed_ok: std::collections::BTreeMap<String, u64> =
                std::collections::BTreeMap::new();
            let mut by_profile_fix_counts: std::collections::BTreeMap<
                String,
                std::collections::BTreeMap<String, u64>,
            > = std::collections::BTreeMap::new();
            let mut by_profile_issues_by_area_norm: std::collections::BTreeMap<
                String,
                std::collections::BTreeMap<String, u64>,
            > = std::collections::BTreeMap::new();

            for (idx0, group) in by_image {
                let img_path = images.get(idx0).cloned().unwrap_or_default();
                let mut artifact_paths: Vec<String> = Vec::new();
                let mut artifact_files: Vec<String> = Vec::new();
                let mut trial_scores: Vec<f64> = Vec::new();
                let mut trial_parsed_ok: u64 = 0;

                let mut img_fix_counts: std::collections::BTreeMap<String, u64> =
                    std::collections::BTreeMap::new();
                let mut img_issues: Vec<serde_json::Value> = Vec::new();
                let mut per_profile: std::collections::BTreeMap<String, serde_json::Value> =
                    std::collections::BTreeMap::new();

                for o in group {
                    artifact_paths.push(o.artifact_path.display().to_string());
                    artifact_files.push(o.artifact_file.clone());

                    let parsed_ok_i = o
                        .payload
                        .get("parsed_ok")
                        .and_then(|x| x.as_bool())
                        .unwrap_or(false);
                    runs = runs.saturating_add(1);
                    *by_profile_runs.entry(o.spec.profile.clone()).or_insert(0) += 1;
                    if parsed_ok_i {
                        parsed_ok = parsed_ok.saturating_add(1);
                        trial_parsed_ok = trial_parsed_ok.saturating_add(1);
                        *by_profile_parsed_ok
                            .entry(o.spec.profile.clone())
                            .or_insert(0) += 1;
                    }

                    if let Some(p) = o.payload.get("parsed") {
                        if let Some(s) = p.get("score_0_10").and_then(|x| x.as_f64()) {
                            trial_scores.push(s);
                        }
                        if let Some(arr) = p.get("top_3_fixes").and_then(|x| x.as_array()) {
                            for s in arr.iter().filter_map(|x| x.as_str()) {
                                let s = s.trim();
                                if s.is_empty() {
                                    continue;
                                }
                                *img_fix_counts.entry(s.to_string()).or_insert(0) += 1;
                                *fixes.entry(s.to_string()).or_insert(0) += 1;
                                *by_profile_fix_counts
                                    .entry(o.spec.profile.clone())
                                    .or_default()
                                    .entry(s.to_string())
                                    .or_insert(0) += 1;
                                if let Some(theme) = theme_for_fix(s) {
                                    let t = theme.to_string();
                                    *themes.entry(t.clone()).or_insert(0) += 1;
                                    *theme_fix_counts
                                        .entry(t)
                                        .or_default()
                                        .entry(s.to_string())
                                        .or_insert(0) += 1;
                                }
                            }
                        }
                        if let Some(arr) = p.get("issues").and_then(|x| x.as_array()) {
                            for it in arr {
                                img_issues.push(it.clone());
                                if let Some(area) = it.get("area").and_then(|x| x.as_str()) {
                                    let a = area.trim();
                                    if !a.is_empty() {
                                        *issues_by_area.entry(a.to_string()).or_insert(0) += 1;
                                        let an = norm_area(a);
                                        *issues_by_area_norm.entry(an.clone()).or_insert(0) += 1;
                                        *by_profile_issues_by_area_norm
                                            .entry(o.spec.profile.clone())
                                            .or_default()
                                            .entry(an)
                                            .or_insert(0) += 1;
                                    }
                                }
                            }
                        }
                    }

                    // Per-profile (per image) small bundle.
                    let pp = per_profile
                        .entry(o.spec.profile.clone())
                        .or_insert_with(|| {
                            serde_json::json!({
                                "runs": 0u64,
                                "parsed_ok": 0u64,
                                "artifact_files": Vec::<String>::new(),
                                "artifact_paths": Vec::<String>::new(),
                                "scores": Vec::<f64>::new(),
                                "verdicts": Vec::<String>::new(),
                                "issues": Vec::<serde_json::Value>::new()
                            })
                        });
                    if let Some(obj) = pp.as_object_mut() {
                        let r0 = obj.get("runs").and_then(|x| x.as_u64()).unwrap_or(0) + 1;
                        obj.insert("runs".to_string(), serde_json::Value::from(r0));
                        if parsed_ok_i {
                            let p0 = obj.get("parsed_ok").and_then(|x| x.as_u64()).unwrap_or(0) + 1;
                            obj.insert("parsed_ok".to_string(), serde_json::Value::from(p0));
                        }
                        if let Some(a) =
                            obj.get_mut("artifact_files").and_then(|x| x.as_array_mut())
                        {
                            a.push(serde_json::Value::String(o.artifact_file.clone()));
                        }
                        if let Some(a) =
                            obj.get_mut("artifact_paths").and_then(|x| x.as_array_mut())
                        {
                            a.push(serde_json::Value::String(
                                o.artifact_path.display().to_string(),
                            ));
                        }
                        if let Some(p) = o.payload.get("parsed") {
                            if let Some(s) = p.get("score_0_10").and_then(|x| x.as_f64()) {
                                if let Some(a) =
                                    obj.get_mut("scores").and_then(|x| x.as_array_mut())
                                {
                                    a.push(serde_json::Value::from(s));
                                }
                            }
                            if let Some(v) = p.get("verdict").and_then(|x| x.as_str()) {
                                let vv = v.trim();
                                if !vv.is_empty() {
                                    if let Some(a) =
                                        obj.get_mut("verdicts").and_then(|x| x.as_array_mut())
                                    {
                                        a.push(serde_json::Value::String(vv.to_string()));
                                    }
                                }
                            }
                            if let Some(arr) = p.get("issues").and_then(|x| x.as_array()) {
                                if let Some(a) =
                                    obj.get_mut("issues").and_then(|x| x.as_array_mut())
                                {
                                    for it in arr {
                                        a.push(it.clone());
                                    }
                                }
                            }
                        }
                    }
                }

                let score_median: Option<f64> = if trial_scores.is_empty() {
                    None
                } else {
                    let mut v = trial_scores.clone();
                    v.sort_by(|a, b| a.partial_cmp(b).unwrap_or(std::cmp::Ordering::Equal));
                    Some(v[v.len() / 2])
                };

                let mut img_fix_vec: Vec<(String, u64)> = img_fix_counts.into_iter().collect();
                img_fix_vec.sort_by(|a, b| b.1.cmp(&a.1).then_with(|| a.0.cmp(&b.0)));
                let img_top_3_fixes: Vec<String> =
                    img_fix_vec.iter().take(3).map(|(s, _)| s.clone()).collect();
                let consensus_min = trials.div_ceil(2);
                let consensus_fixes: Vec<String> = img_fix_vec
                    .iter()
                    .filter(|(_s, c)| *c >= consensus_min)
                    .take(5)
                    .map(|(s, _)| s.clone())
                    .collect();

                let artifact_path0 = artifact_paths.first().cloned().unwrap_or_default();
                let artifact_file0 = artifact_files.first().cloned().unwrap_or_default();

                per_input.push(serde_json::json!({
                    "image_index": (idx0 + 1) as u64,
                    "image_path": img_path.display().to_string(),
                    "artifact_path": artifact_path0,
                    "artifact_file": artifact_file0,
                    "artifact_paths": artifact_paths,
                    "artifact_files": artifact_files,
                    "profiles": profiles,
                    "per_profile": per_profile,
                    "trials": {
                        "count": trials,
                        "parsed_ok": trial_parsed_ok,
                        "scores": trial_scores,
                        "score_median": score_median,
                        "consensus_min_votes": consensus_min,
                        "consensus_fixes": consensus_fixes
                    },
                    "ok": trial_parsed_ok > 0,
                    "parsed_ok": trial_parsed_ok > 0,
                    "score_0_10": score_median,
                    "top_3_fixes": img_top_3_fixes,
                    "issues": img_issues
                }));
            }

            // Attach per-profile rollups (run-level).
            for p in &profiles {
                let runs_p = by_profile_runs.get(p).cloned().unwrap_or(0);
                let ok_p = by_profile_parsed_ok.get(p).cloned().unwrap_or(0);
                let fixes_p = by_profile_fix_counts
                    .get(p)
                    .cloned()
                    .unwrap_or_default()
                    .into_iter()
                    .collect::<std::collections::BTreeMap<_, _>>();
                let issues_p = by_profile_issues_by_area_norm
                    .get(p)
                    .cloned()
                    .unwrap_or_default()
                    .into_iter()
                    .collect::<std::collections::BTreeMap<_, _>>();
                by_profile.insert(
                    p.clone(),
                    serde_json::json!({
                        "runs": runs_p,
                        "parsed_ok": ok_p,
                        "parsed_ok_rate": if runs_p == 0 { 0.0 } else { (ok_p as f64) / (runs_p as f64) },
                        "fix_counts": fixes_p,
                        "issues_by_area_norm": issues_p
                    }),
                );
            }

            // Produce meta summary.
            #[derive(Clone, Debug)]
            struct FixCount {
                fix: String,
                count: u64,
            }
            let mut fixes_vec: Vec<FixCount> = fixes
                .into_iter()
                .map(|(fix, count)| FixCount { fix, count })
                .collect();
            fixes_vec.sort_by(|a, b| b.count.cmp(&a.count).then_with(|| a.fix.cmp(&b.fix)));
            let top_3_fixes: Vec<String> =
                fixes_vec.iter().take(3).map(|fc| fc.fix.clone()).collect();

            fn sev_rank(s: &str) -> u8 {
                match s.trim().to_ascii_uppercase().as_str() {
                    "P0" => 0,
                    "P1" => 1,
                    "P2" => 2,
                    _ => 2,
                }
            }
            fn verdict_rank(s: &str) -> u8 {
                match s.trim().to_ascii_lowercase().as_str() {
                    "bad" => 0,
                    "mixed" => 1,
                    "good" => 2,
                    _ => 1,
                }
            }

            // Meta aggregation: severity-first rollup across all parsed issues.
            // This is deterministic and does not add extra model calls.
            let mut meta_scores: Vec<f64> = Vec::new();
            let mut meta_verdict_best: u8 = 2; // good
            let mut sev_counts: std::collections::BTreeMap<String, u64> =
                std::collections::BTreeMap::new();
            let mut fix_counts_p0: std::collections::BTreeMap<String, u64> =
                std::collections::BTreeMap::new();
            let mut fix_counts_p1: std::collections::BTreeMap<String, u64> =
                std::collections::BTreeMap::new();

            for it in per_input.iter() {
                // scores
                if let Some(s) = it
                    .get("trials")
                    .and_then(|t| t.get("scores"))
                    .and_then(|x| x.as_array())
                {
                    for v in s.iter().filter_map(|x| x.as_f64()) {
                        meta_scores.push(v);
                    }
                }
                // Prefer explicit verdict in per-profile parsed payloads if present (more reliable).
                if let Some(pp) = it.get("per_profile").and_then(|x| x.as_object()) {
                    for (_k, v) in pp {
                        if let Some(vs) = v.get("verdicts").and_then(|x| x.as_array()) {
                            for vv in vs.iter().filter_map(|x| x.as_str()) {
                                let vv = vv.trim();
                                if !vv.is_empty() {
                                    meta_verdict_best = meta_verdict_best.min(verdict_rank(vv));
                                }
                            }
                        }
                        if let Some(issues) = v.get("issues").and_then(|x| x.as_array()) {
                            for iss in issues {
                                let sev =
                                    iss.get("severity").and_then(|x| x.as_str()).unwrap_or("P2");
                                let sev_key = sev.trim().to_ascii_uppercase();
                                *sev_counts.entry(sev_key.clone()).or_insert(0) += 1;
                                let fx =
                                    iss.get("fix").and_then(|x| x.as_str()).unwrap_or("").trim();
                                if !fx.is_empty() {
                                    if sev_rank(&sev_key) == 0 {
                                        *fix_counts_p0.entry(fx.to_string()).or_insert(0) += 1;
                                    } else if sev_rank(&sev_key) == 1 {
                                        *fix_counts_p1.entry(fx.to_string()).or_insert(0) += 1;
                                    }
                                }
                            }
                        }
                    }
                } else if let Some(arr) = it.get("issues").and_then(|x| x.as_array()) {
                    // Back-compat: older summaries without per_profile.
                    for iss in arr {
                        let sev = iss.get("severity").and_then(|x| x.as_str()).unwrap_or("P2");
                        let sev_key = sev.trim().to_ascii_uppercase();
                        *sev_counts.entry(sev_key.clone()).or_insert(0) += 1;
                        let v = iss.get("fix").and_then(|x| x.as_str()).unwrap_or("").trim();
                        if !v.is_empty() {
                            if sev_rank(&sev_key) == 0 {
                                *fix_counts_p0.entry(v.to_string()).or_insert(0) += 1;
                            } else if sev_rank(&sev_key) == 1 {
                                *fix_counts_p1.entry(v.to_string()).or_insert(0) += 1;
                            }
                        }
                    }
                }
            }

            meta_scores.sort_by(|a, b| a.partial_cmp(b).unwrap_or(std::cmp::Ordering::Equal));
            let meta_score_median = if meta_scores.is_empty() {
                None
            } else {
                Some(meta_scores[meta_scores.len() / 2])
            };
            let meta_score_min = meta_scores.first().cloned();
            let meta_has_p0 = sev_counts.get("P0").cloned().unwrap_or(0) > 0;
            let meta_has_p1 = sev_counts.get("P1").cloned().unwrap_or(0) > 0;
            let meta_verdict = if meta_has_p0 {
                "bad"
            } else if meta_has_p1 {
                "mixed"
            } else {
                match meta_verdict_best {
                    0 => "bad",
                    1 => "mixed",
                    _ => "good",
                }
            };

            fn top_k_counts(
                map: &std::collections::BTreeMap<String, u64>,
                k: usize,
            ) -> Vec<serde_json::Value> {
                let mut v = map.iter().map(|(s, c)| (s.clone(), *c)).collect::<Vec<_>>();
                v.sort_by(|a, b| b.1.cmp(&a.1).then_with(|| a.0.cmp(&b.0)));
                v.into_iter()
                    .take(k)
                    .map(|(fix, count)| serde_json::json!({"fix": fix, "count": count}))
                    .collect()
            }
            let meta = serde_json::json!({
                "verdict": meta_verdict,
                "score_median_0_10": meta_score_median,
                "score_min_0_10": meta_score_min,
                "severity_counts": sev_counts,
                "top_fix_counts_p0": top_k_counts(&fix_counts_p0, 10),
                "top_fix_counts_p1": top_k_counts(&fix_counts_p1, 10),
                "top_fix_counts_overall": fixes_vec.iter().take(10).map(|fc| serde_json::json!({"fix": fc.fix, "count": fc.count})).collect::<Vec<_>>(),
            });

            let mut themes_vec = themes.clone().into_iter().collect::<Vec<_>>();
            themes_vec.sort_by(|a, b| b.1.cmp(&a.1).then_with(|| a.0.cmp(&b.0)));
            let top_3_themes: Vec<String> =
                themes_vec.into_iter().take(3).map(|(k, _)| k).collect();

            let mut top_3_fixes_by_theme: Vec<String> = Vec::new();
            for theme in &top_3_themes {
                if let Some(m) = theme_fix_counts.get(theme) {
                    let mut v = m.iter().map(|(k, c)| (k.clone(), *c)).collect::<Vec<_>>();
                    v.sort_by(|a, b| b.1.cmp(&a.1).then_with(|| a.0.cmp(&b.0)));
                    if let Some((fix, _)) = v.into_iter().next() {
                        top_3_fixes_by_theme.push(fix);
                    }
                }
            }

            let elapsed_ms = t0.elapsed().as_millis() as u64;
            let summary = serde_json::json!({
                "schema_version": 1,
                "kind": "webpipe_eval_vlm_run",
                "generated_at_epoch_s": now,
                "timing_ms": elapsed_ms,
                "inputs": {
                    "images": images.iter().map(|p| p.display().to_string()).collect::<Vec<_>>(),
                    "images_dir": args.images_dir.as_ref().map(|p| p.display().to_string()),
                    "max_images": args.max_images,
                    "model": model,
                    "goals": goals.as_ref().clone(),
                    "goal_profiles": goal_profiles_used.as_ref().clone(),
                    "goals_file": args.goals_file.as_ref().map(|p| p.display().to_string()),
                    "trials": trials,
                    "temperature": temperature,
                    "profiles": profiles,
                    "max_inflight": max_inflight,
                    "cache": cache_mode,
                },
                "outputs": {
                    "out_dir": out_dir.display().to_string(),
                    "summary_path": out_summary.display().to_string(),
                    "transcript_jsonl": Some(transcript_jsonl_path.display().to_string()),
                },
                "totals": {
                    "runs": runs,
                    "parsed_ok": parsed_ok,
                    "images": images.len(),
                    "trials": trials,
                    "parsed_ok_rate": if runs == 0 { 0.0 } else { (parsed_ok as f64) / (runs as f64) }
                },
                "top_3_fixes": top_3_fixes,
                "top_3_themes": top_3_themes,
                "top_3_fixes_by_theme": top_3_fixes_by_theme,
                "issues_by_area": issues_by_area,
                "issues_by_area_norm": issues_by_area_norm,
                "by_profile": by_profile,
                "meta": meta,
                "themes": themes,
                "fix_counts": fixes_vec.iter().take(50).map(|fc| serde_json::json!({"fix": fc.fix, "count": fc.count})).collect::<Vec<_>>(),
                "per_input": per_input,
            });

            std::fs::write(&out_summary, serde_json::to_string_pretty(&summary)? + "\n")?;
            println!("{}", out_summary.display());
        }
        #[cfg(all(feature = "eval", feature = "vlm"))]
        Commands::EvalVlmBundle(args) => {
            let now = args.now_epoch_s.unwrap_or_else(|| {
                std::time::SystemTime::now()
                    .duration_since(std::time::UNIX_EPOCH)
                    .unwrap_or_default()
                    .as_secs()
            });
            let out = args.out.unwrap_or_else(|| {
                std::path::PathBuf::from(format!(".generated/webpipe-eval-vlm-bundle-{now}.json"))
            });
            if let Some(p) = out.parent() {
                std::fs::create_dir_all(p)?;
            }

            fn read_summary(path: &std::path::Path) -> anyhow::Result<serde_json::Value> {
                let raw = std::fs::read_to_string(path)?;
                let v: serde_json::Value = serde_json::from_str(&raw)?;
                if v.get("kind").and_then(|x| x.as_str()) != Some("webpipe_eval_vlm_run") {
                    anyhow::bail!(
                        "summary.kind must be webpipe_eval_vlm_run: {}",
                        path.display()
                    );
                }
                if v.get("schema_version").and_then(|x| x.as_u64()) != Some(1) {
                    anyhow::bail!("summary.schema_version must be 1: {}", path.display());
                }
                Ok(v)
            }

            let page = read_summary(&args.page_summary)?;
            let plots = read_summary(&args.plots_summary)?;

            // Merge fix_counts from both summaries into a combined top list.
            let mut fix_counts: std::collections::BTreeMap<String, u64> =
                std::collections::BTreeMap::new();
            for src in [&page, &plots] {
                if let Some(arr) = src.get("fix_counts").and_then(|x| x.as_array()) {
                    for it in arr {
                        let fix = it.get("fix").and_then(|x| x.as_str()).unwrap_or("").trim();
                        let c = it.get("count").and_then(|x| x.as_u64()).unwrap_or(0);
                        if !fix.is_empty() && c > 0 {
                            *fix_counts.entry(fix.to_string()).or_insert(0) += c;
                        }
                    }
                }
            }
            let mut fixes_vec: Vec<(String, u64)> = fix_counts.into_iter().collect();
            fixes_vec.sort_by(|a, b| b.1.cmp(&a.1).then_with(|| a.0.cmp(&b.0)));
            let combined_top_10: Vec<serde_json::Value> = fixes_vec
                .iter()
                .take(10)
                .map(|(fix, count)| serde_json::json!({"fix": fix, "count": count}))
                .collect();

            let payload = serde_json::json!({
                "schema_version": 1,
                "kind": "webpipe_eval_vlm_bundle",
                "generated_at_epoch_s": now,
                "inputs": {
                    "page_summary": args.page_summary.display().to_string(),
                    "plots_summary": args.plots_summary.display().to_string()
                },
                "summary": {
                    "combined_top_fix_counts": combined_top_10
                },
                "page": page,
                "plots": plots
            });

            std::fs::write(&out, serde_json::to_string_pretty(&payload)? + "\n")?;
            println!("{}", out.display());
        }
        Commands::Version(args) => {
            let v = serde_json::json!({
                "schema_version": 2,
                "kind": "version",
                "ok": true,
                "name": "webpipe",
                "version": env!("CARGO_PKG_VERSION"),
            });
            match args.output.to_ascii_lowercase().as_str() {
                "text" => println!("webpipe {}", env!("CARGO_PKG_VERSION")),
                _ => println!("{}", v),
            }
        }
        #[cfg(feature = "vlm")]
        Commands::VlmImageToText(args) => {
            let now = args.now_epoch_s.unwrap_or_else(|| {
                std::time::SystemTime::now()
                    .duration_since(std::time::UNIX_EPOCH)
                    .unwrap_or_default()
                    .as_secs()
            });
            let out = args.out.unwrap_or_else(|| {
                std::path::PathBuf::from(format!(".generated/webpipe-vlm-image-to-text-{now}.json"))
            });
            std::fs::create_dir_all(
                out.parent()
                    .unwrap_or_else(|| std::path::Path::new(".generated")),
            )?;

            let img_path = args.image;
            let bytes = std::fs::read(&img_path)?;

            fn infer_mime_from_path(p: &std::path::Path) -> Option<&'static str> {
                let ext = p
                    .extension()
                    .and_then(|s| s.to_str())
                    .unwrap_or("")
                    .trim()
                    .to_ascii_lowercase();
                match ext.as_str() {
                    "png" => Some("image/png"),
                    "jpg" | "jpeg" => Some("image/jpeg"),
                    "webp" => Some("image/webp"),
                    "gif" => Some("image/gif"),
                    _ => None,
                }
            }

            let mime_type = args
                .mime_type
                .as_deref()
                .map(|s| s.trim().to_string())
                .filter(|s| !s.is_empty())
                .or_else(|| infer_mime_from_path(&img_path).map(|s| s.to_string()))
                .unwrap_or_else(|| "application/octet-stream".to_string());

            // Feature-gated: keep network calls opt-in at build time.
            #[cfg(feature = "vision-gemini")]
            let http = reqwest::Client::new();
            #[cfg(feature = "vision-gemini")]
            let result: Result<(String, String), &'static str> =
                webpipe_local::vision_gemini::gemini_image_to_text(http, &bytes, &mime_type).await;
            #[cfg(not(feature = "vision-gemini"))]
            let result: Result<(String, String), &'static str> =
                Err("vision_gemini_feature_disabled");

            let payload = match result {
                Ok((model, text)) => serde_json::json!({
                    "schema_version": 1,
                    "kind": "webpipe_vlm_image_to_text",
                    "generated_at_epoch_s": now,
                    "ok": true,
                    "inputs": {
                        "image_path": img_path,
                        "mime_type": mime_type,
                        "bytes_len": bytes.len()
                    },
                    "model": model,
                    "text": text
                }),
                Err(code) => serde_json::json!({
                    "schema_version": 1,
                    "kind": "webpipe_vlm_image_to_text",
                    "generated_at_epoch_s": now,
                    "ok": false,
                    "inputs": {
                        "image_path": img_path,
                        "mime_type": mime_type,
                        "bytes_len": bytes.len()
                    },
                    "error": { "code": code }
                }),
            };

            std::fs::write(&out, serde_json::to_string_pretty(&payload)? + "\n")?;
            println!("{}", out.display());
        }
        #[cfg(feature = "vlm")]
        Commands::VlmOpenrouter(args) => {
            let t0 = std::time::Instant::now();
            let mut transcript_seq: u64 = 0;
            fn env(key: &str) -> Option<String> {
                std::env::var(key)
                    .ok()
                    .map(|s| s.trim().to_string())
                    .filter(|s| !s.is_empty())
            }

            let now = args.now_epoch_s.unwrap_or_else(|| {
                std::time::SystemTime::now()
                    .duration_since(std::time::UNIX_EPOCH)
                    .unwrap_or_default()
                    .as_secs()
            });
            let out = args.out.unwrap_or_else(|| {
                std::path::PathBuf::from(format!(".generated/webpipe-vlm-openrouter-{now}.json"))
            });
            std::fs::create_dir_all(
                out.parent()
                    .unwrap_or_else(|| std::path::Path::new(".generated")),
            )?;

            let api_key = env("WEBPIPE_OPENROUTER_API_KEY")
                .or_else(|| env("OPENROUTER_API_KEY"))
                .ok_or_else(|| {
                    anyhow::anyhow!("missing OPENROUTER_API_KEY (or WEBPIPE_OPENROUTER_API_KEY)")
                })?;

            let model = args
                .model
                .or_else(|| env("WEBPIPE_OPENROUTER_MODEL"))
                .unwrap_or_else(|| "google/gemini-3-flash-preview".to_string());

            let base = env("WEBPIPE_OPENROUTER_BASE_URL")
                .unwrap_or_else(|| "https://openrouter.ai/api".to_string());
            let url = format!("{}/v1/chat/completions", base.trim_end_matches('/'));

            let img_path = args.image;

            let prompt = args.prompt.unwrap_or_else(|| {
                "Return ONE JSON object only (no markdown, no prose).\n\nYou are critiquing a *rendered technical page screenshot*. Use BOTH the screenshot and any provided Render Context (computed CSS, TOC state, console/network errors).\n\nSchema (required keys):\n- overall: string (2–4 sentences)\n- score_0_10: number\n- verdict: \"good\"|\"mixed\"|\"bad\"\n- strengths: string[] (3–6 concrete positives)\n- issues: array of {\n    area: \"navigation\"|\"layout_spacing\"|\"typography\"|\"visual_hierarchy\"|\"plots_figures\"|\"tables_data\"|\"math_typesetting\"|\"writing_clarity\"|\"accessibility\"|\"technical_runtime\",\n    severity: \"P0\"|\"P1\"|\"P2\",\n    region: \"top\"|\"middle\"|\"bottom\"|\"global\",\n    problem: string,\n    evidence: string,\n    fix: string\n  }\n- top_3_fixes: string[] (ranked)\n\nRules:\n- Evidence must be concrete and locally grounded.\n- Fixes must be actionable, not generic.\n- Do NOT recommend changes already present per render context."
                    .to_string()
            });
            let prompt = if let Some(p) = args.context_json.as_ref() {
                if let Ok(raw) = std::fs::read_to_string(p) {
                    if let Ok(v) = serde_json::from_str::<serde_json::Value>(&raw) {
                        let ctx_txt = vlm_format_context_block(&v, args.context_max_chars);
                        format!("{prompt}\n\n## Render context (from Playwright)\n{ctx_txt}\n")
                    } else {
                        prompt
                    }
                } else {
                    prompt
                }
            } else {
                prompt
            };

            let http = reqwest::Client::new();
            let payload = vlm_openrouter_call(
                &http,
                url.as_str(),
                api_key.as_str(),
                model.as_str(),
                prompt.as_str(),
                img_path.as_path(),
                args.max_chars,
                args.temperature,
                now,
            )
            .await;

            if let Some(tx_path) = args.transcript_jsonl.as_ref() {
                if let Some(p) = tx_path.parent() {
                    let _ = std::fs::create_dir_all(p);
                }
                let (prompt_trunc, _) = vlm_truncate_chars(&prompt, args.transcript_max_chars);
                let elapsed_ms = t0.elapsed().as_millis() as u64;
                transcript_seq = transcript_seq.wrapping_add(1);
                let line = serde_json::json!({
                    "schema_version": 1,
                    "kind": "webpipe_eval_transcript_event",
                    "generated_at_epoch_s": now,
                    "seq": transcript_seq,
                    "run_kind": "vlm_openrouter",
                    "stage": "vlm_openrouter",
                    "call_id": format!("vlm_openrouter:{}:{}", now, img_path.display()),
                    "timing_ms": elapsed_ms,
                    "llm": {
                        "backend": "openrouter",
                        "model_effective": model,
                        "temperature": args.temperature
                    },
                    "prompt": {
                        "user": prompt_trunc
                    },
                    "response": {
                        "parsed_ok": payload.get("parsed_ok").and_then(|x| x.as_bool()).unwrap_or(false),
                        "parsed": payload.get("parsed").cloned().unwrap_or(serde_json::Value::Null),
                        "http_status": payload.get("http").and_then(|h| h.get("status")).and_then(|x| x.as_u64()),
                    }
                });
                let _ = vlm_append_jsonl(tx_path.as_path(), &line);
            }

            std::fs::write(&out, serde_json::to_string_pretty(&payload)? + "\n")?;
            println!("{}", out.display());
        }
        #[cfg(feature = "eval")]
        Commands::EvalQrels(args) => {
            let now = args.now_epoch_s.unwrap_or_else(|| {
                std::time::SystemTime::now()
                    .duration_since(std::time::UNIX_EPOCH)
                    .unwrap_or_default()
                    .as_secs()
            });
            let out = args.out.unwrap_or_else(|| {
                std::path::PathBuf::from(format!(".generated/webpipe-eval-qrels-{now}.json"))
            });

            #[derive(serde::Deserialize)]
            struct Qrel {
                query_id: String,
                url: String,
                grade: u64,
            }
            #[derive(serde::Deserialize)]
            struct QrelsFile {
                schema_version: u64,
                kind: String,
                qrels: Vec<Qrel>,
            }

            let qrels_raw = std::fs::read_to_string(&args.qrels)?;
            let qrels_file: QrelsFile = serde_json::from_str(&qrels_raw)?;
            if qrels_file.schema_version != 1 || qrels_file.kind != "webpipe_seed_qrels" {
                anyhow::bail!("unexpected qrels file kind/schema_version");
            }

            let eval_raw = std::fs::read_to_string(&args.eval_artifact)?;
            let eval_v: serde_json::Value = serde_json::from_str(&eval_raw)?;
            if eval_v.get("kind").and_then(|v| v.as_str()) != Some("eval_search_extract") {
                anyhow::bail!("eval_artifact.kind must be eval_search_extract");
            }

            fn canonicalize_eval_url(u: &str) -> Option<String> {
                let s = u.trim();
                if s.is_empty() {
                    return None;
                }
                // For scoring, treat fragment URLs as the same document.
                // This matches our extraction/link behavior which often drops fragments for stability.
                if let Ok(mut url) = reqwest::Url::parse(s) {
                    url.set_fragment(None);
                    Some(url.to_string())
                } else {
                    None
                }
            }

            let mut by_q: std::collections::BTreeMap<String, Vec<&Qrel>> =
                std::collections::BTreeMap::new();
            for q in &qrels_file.qrels {
                by_q.entry(q.query_id.clone()).or_default().push(q);
            }

            let mut per_query = Vec::new();
            let mut total_expected = 0usize;
            let mut total_found = 0usize;

            let runs = eval_v
                .get("runs")
                .and_then(|v| v.as_array())
                .cloned()
                .unwrap_or_default();
            for run in runs {
                let q = run
                    .get("query")
                    .and_then(|v| v.as_str())
                    .unwrap_or("")
                    .to_string();
                let qid = run
                    .get("query_id")
                    .and_then(|v| v.as_str())
                    .map(|s| s.to_string())
                    .unwrap_or_else(|| q.clone());

                let expected = by_q.get(&qid).cloned().unwrap_or_default();
                if expected.is_empty() {
                    continue;
                }

                let mut got_urls = std::collections::BTreeSet::<String>::new();
                if let Some(per_mode) = run.get("per_selection_mode").and_then(|v| v.as_array()) {
                    for pm in per_mode {
                        if let Some(results) = pm
                            .get("result")
                            .and_then(|r| r.get("results"))
                            .and_then(|v| v.as_array())
                        {
                            for r in results {
                                // Score against both the original URL and the final (redirected) URL.
                                // This makes scoring robust to redirects while keeping the tool output unchanged.
                                if let Some(u) = r.get("url").and_then(|x| x.as_str()) {
                                    if let Some(c) = canonicalize_eval_url(u) {
                                        got_urls.insert(c);
                                    }
                                }
                                if let Some(u) = r.get("final_url").and_then(|x| x.as_str()) {
                                    if let Some(c) = canonicalize_eval_url(u) {
                                        got_urls.insert(c);
                                    }
                                }
                            }
                        }
                    }
                }

                let mut expected_urls = std::collections::BTreeSet::<String>::new();
                for e in &expected {
                    if e.grade > 0 {
                        if let Some(c) = canonicalize_eval_url(&e.url) {
                            expected_urls.insert(c);
                        }
                    }
                }

                let found = expected_urls.intersection(&got_urls).count();
                let exp = expected_urls.len();
                total_expected += exp;
                total_found += found;

                per_query.push(serde_json::json!({
                    "query_id": qid,
                    "expected_relevant_url_count": exp,
                    "found_relevant_url_count": found,
                    "missed_urls": expected_urls.difference(&got_urls).cloned().collect::<Vec<_>>(),
                }));
            }

            let payload = serde_json::json!({
                "schema_version": 1,
                "kind": "eval_qrels",
                "generated_at_epoch_s": now,
                "inputs": {
                    "eval_artifact": args.eval_artifact.display().to_string(),
                    "qrels": args.qrels.display().to_string(),
                },
                "summary": {
                    "expected_relevant_urls_total": total_expected,
                    "found_relevant_urls_total": total_found,
                    "recall": if total_expected == 0 { 0.0 } else { (total_found as f64) / (total_expected as f64) }
                },
                "per_query": per_query
            });

            if let Some(p) = out.parent() {
                std::fs::create_dir_all(p)?;
            }
            std::fs::write(&out, serde_json::to_string_pretty(&payload)? + "\n")?;
            println!("{}", out.display());
        }
        #[cfg(not(feature = "stdio"))]
        Commands::McpStdio => {
            anyhow::bail!("mcp-stdio requires feature `stdio` (rebuild with: --features stdio)");
        }
    }

    Ok(())
}
